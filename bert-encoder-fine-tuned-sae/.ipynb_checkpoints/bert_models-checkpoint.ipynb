{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"JeanKaddour/minipile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MiniPileDataset(Dataset):\n",
    "    def __init__(self, sentences, embeddings):\n",
    "        self.sentences = sentences\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.embeddings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /sailhome/joetey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df_all_sentences = pd.DataFrame(columns=['sentence'])\n",
    "\n",
    "row_index = 0\n",
    "for text in ds['train']['text']:\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        # Use .at to set the value in the DataFrame\n",
    "        df_all_sentences.at[row_index, 'sentence'] = sentence\n",
    "        row_index += 1\n",
    "\n",
    "    if row_index % 100 == 0:\n",
    "        print(row_index)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'files/all_sentences_{timestamp}.csv'\n",
    "df_all_sentences.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0999, -0.0745,  0.1635,  ..., -0.1597, -0.0818, -0.0293],\n",
      "        [ 0.0466,  0.1534, -0.1437,  ..., -0.4857, -0.0143, -0.2541]])\n"
     ]
    }
   ],
   "source": [
    "inputs = bert_tokenizer([\"hello world\", \"the world\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = bert_model(**inputs)\n",
    "batch_embeddings = outputs.last_hidden_state.mean(dim=1).detach()\n",
    "\n",
    "print(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40 sentences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m bert_tokenizer(batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     18\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Write the dataset to a CSV file\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "embeddings = []\n",
    "sentences_passed = []\n",
    "\n",
    "batch_size = 40\n",
    "sentences = df_all_sentences['sentence'].tolist()\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch = sentences[i:i + batch_size]\n",
    "    try:\n",
    "        inputs = bert_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = bert_model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).detach()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        sentences_passed.extend(batch)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processed {i + len(batch)} sentences\")\n",
    "\n",
    "embeddings = torch.stack(embeddings)\n",
    "\n",
    "# Initialize the dataset\n",
    "mini_pile_dataset = MiniPileDataset(sentences, embeddings)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'files/all_sentences_with_embeddings_{timestamp}.pkl'\n",
    "# with open(filename, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['sentence', 'embedding'])\n",
    "#     for sentence, embedding in zip(mini_pile_dataset.sentences, mini_pile_dataset.embeddings):\n",
    "#         writer.writerow([sentence, embedding])\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model to a pickle file\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(mini_pile_dataset, f)\n",
    "\n",
    "print(f\"Embeddings saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MiniPileDataset(sentences, embeddings)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage: load the dataset from the CSV file\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m loaded_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_csv_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfiles/all_sentences_with_embeddings_20240707_081607.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mload_csv_to_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mnext\u001b[39m(reader)  \u001b[38;5;66;03m# Skip the header\u001b[39;00m\n\u001b[1;32m     10\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     12\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, row[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensor([])\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n",
      "File \u001b[0;32m/piech/u/joetey/miniconda3/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "def load_csv_to_dataset(filename):\n",
    "    sentences = []\n",
    "    embeddings = []\n",
    "    with open(filename, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header\n",
    "        count = 0 \n",
    "        for row in reader:\n",
    "            sentence = row[0]\n",
    "            embedding = torch.tensor(list(map(float, row[1].strip('tensor([])').split(','))))\n",
    "            \n",
    "            sentences.append(sentence)\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "            if count % 100000 == 0:\n",
    "                print(count)\n",
    "            count += 1\n",
    "            \n",
    "    return MiniPileDataset(sentences, embeddings)\n",
    "\n",
    "# Example usage: load the dataset from the CSV file\n",
    "loaded_dataset = load_csv_to_dataset('files/all_sentences_with_embeddings_20240707_081607.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to files/all_sentences_with_embeddings_20240707_132959.pkl\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'files/all_sentences_with_embeddings_{timestamp}.pkl'\n",
    "\n",
    "# Save the model to a pickle file\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(loaded_dataset, f)\n",
    "\n",
    "print(f\"Embeddings saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the pickle file\n",
    "import pickle \n",
    "\n",
    "file_name = \"files/all_sentences_with_embeddings_20240707_132959.pkl\"\n",
    "with open(file_name, \"rb\") as f:\n",
    "    loaded_dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Batch Loss: 231131.46875\n",
      "\n",
      "Batch Loss: 230453.390625\n",
      "\n",
      "Batch Loss: 226721.46875\n",
      "\n",
      "Batch Loss: 225636.0625\n",
      "\n",
      "Batch Loss: 223671.8125\n",
      "\n",
      "Batch Loss: 221714.234375\n",
      "\n",
      "Batch Loss: 223210.46875\n",
      "\n",
      "Batch Loss: 219688.203125\n",
      "\n",
      "Batch Loss: 218325.6875\n",
      "\n",
      "Batch Loss: 217084.0\n",
      "\n",
      "Batch Loss: 215298.234375\n",
      "\n",
      "Batch Loss: 213286.828125\n",
      "\n",
      "Batch Loss: 211579.15625\n",
      "\n",
      "Batch Loss: 211219.375\n",
      "\n",
      "Batch Loss: 208751.84375\n",
      "\n",
      "Batch Loss: 207749.140625\n",
      "\n",
      "Batch Loss: 206472.046875\n",
      "\n",
      "Batch Loss: 205780.609375\n",
      "\n",
      "Batch Loss: 204140.46875\n",
      "\n",
      "Batch Loss: 203388.515625\n",
      "\n",
      "Batch Loss: 200344.875\n",
      "\n",
      "Batch Loss: 198488.703125\n",
      "\n",
      "Batch Loss: 198703.546875\n",
      "\n",
      "Batch Loss: 196345.375\n",
      "\n",
      "Batch Loss: 194797.84375\n",
      "\n",
      "Batch Loss: 194113.828125\n",
      "\n",
      "Batch Loss: 192177.65625\n",
      "\n",
      "Batch Loss: 191079.296875\n",
      "\n",
      "Batch Loss: 190836.03125\n",
      "\n",
      "Batch Loss: 188073.609375\n",
      "\n",
      "Batch Loss: 186456.484375\n",
      "\n",
      "Batch Loss: 186022.140625\n",
      "\n",
      "Batch Loss: 184676.65625\n",
      "\n",
      "Batch Loss: 183275.484375\n",
      "\n",
      "Batch Loss: 182720.34375\n",
      "\n",
      "Batch Loss: 179622.5\n",
      "\n",
      "Batch Loss: 179515.453125\n",
      "\n",
      "Batch Loss: 177685.765625\n",
      "\n",
      "Batch Loss: 178015.84375\n",
      "\n",
      "Batch Loss: 176045.453125\n",
      "\n",
      "Batch Loss: 175492.53125\n",
      "\n",
      "Batch Loss: 172657.65625\n",
      "\n",
      "Batch Loss: 171734.96875\n",
      "\n",
      "Batch Loss: 170450.28125\n",
      "\n",
      "Batch Loss: 168897.703125\n",
      "\n",
      "Batch Loss: 168511.484375\n",
      "\n",
      "Batch Loss: 167938.375\n",
      "\n",
      "Batch Loss: 166787.53125\n",
      "\n",
      "Batch Loss: 166636.59375\n",
      "\n",
      "Batch Loss: 163408.78125\n",
      "\n",
      "Batch Loss: 162257.328125\n",
      "\n",
      "Batch Loss: 161642.546875\n",
      "\n",
      "Batch Loss: 161329.546875\n",
      "\n",
      "Batch Loss: 159547.9375\n",
      "\n",
      "Batch Loss: 158350.828125\n",
      "\n",
      "Batch Loss: 157307.3125\n",
      "\n",
      "Batch Loss: 156332.890625\n",
      "\n",
      "Batch Loss: 154088.125\n",
      "\n",
      "Batch Loss: 153229.828125\n",
      "\n",
      "Batch Loss: 151929.203125\n",
      "\n",
      "Batch Loss: 152542.046875\n",
      "\n",
      "Batch Loss: 149697.109375\n",
      "\n",
      "Batch Loss: 149216.296875\n",
      "\n",
      "Batch Loss: 149014.3125\n",
      "\n",
      "Batch Loss: 147486.34375\n",
      "\n",
      "Batch Loss: 147669.6875\n",
      "\n",
      "Batch Loss: 144999.328125\n",
      "\n",
      "Batch Loss: 146116.96875\n",
      "\n",
      "Batch Loss: 143709.75\n",
      "\n",
      "Batch Loss: 142208.890625\n",
      "\n",
      "Batch Loss: 141992.546875\n",
      "\n",
      "Batch Loss: 139622.421875\n",
      "\n",
      "Batch Loss: 139085.46875\n",
      "\n",
      "Batch Loss: 137853.390625\n",
      "\n",
      "Batch Loss: 136903.234375\n",
      "\n",
      "Batch Loss: 135309.78125\n",
      "\n",
      "Batch Loss: 135409.3125\n",
      "\n",
      "Batch Loss: 134328.203125\n",
      "\n",
      "Batch Loss: 133888.03125\n",
      "\n",
      "Batch Loss: 131788.140625\n",
      "\n",
      "Batch Loss: 132151.125\n",
      "\n",
      "Batch Loss: 130253.2109375\n",
      "\n",
      "Batch Loss: 129133.34375\n",
      "\n",
      "Batch Loss: 128629.796875\n",
      "\n",
      "Batch Loss: 127465.484375\n",
      "\n",
      "Batch Loss: 127513.828125\n",
      "\n",
      "Batch Loss: 125652.359375\n",
      "\n",
      "Batch Loss: 124910.1015625\n",
      "\n",
      "Batch Loss: 124340.2109375\n",
      "\n",
      "Batch Loss: 123299.21875\n",
      "\n",
      "Batch Loss: 123248.1328125\n",
      "\n",
      "Batch Loss: 122460.46875\n",
      "\n",
      "Batch Loss: 120738.2734375\n",
      "\n",
      "Batch Loss: 118285.3828125\n",
      "\n",
      "Batch Loss: 116722.9375\n",
      "\n",
      "Batch Loss: 117641.2109375\n",
      "\n",
      "Batch Loss: 116890.8046875\n",
      "\n",
      "Batch Loss: 116938.890625\n",
      "\n",
      "Batch Loss: 114881.5\n",
      "\n",
      "Batch Loss: 114487.1171875\n",
      "\n",
      "Batch Loss: 114601.0078125\n",
      "\n",
      "Batch Loss: 110798.7734375\n",
      "\n",
      "Batch Loss: 112645.4609375\n",
      "\n",
      "Batch Loss: 110505.9375\n",
      "\n",
      "Batch Loss: 111294.984375\n",
      "\n",
      "Batch Loss: 110970.3828125\n",
      "\n",
      "Batch Loss: 110088.21875\n",
      "\n",
      "Batch Loss: 109657.515625\n",
      "\n",
      "Batch Loss: 106497.2421875\n",
      "\n",
      "Batch Loss: 106774.578125\n",
      "\n",
      "Batch Loss: 106968.9609375\n",
      "\n",
      "Batch Loss: 106986.3671875\n",
      "\n",
      "Batch Loss: 104418.4453125\n",
      "\n",
      "Batch Loss: 103086.953125\n",
      "\n",
      "Batch Loss: 104428.7265625\n",
      "\n",
      "Batch Loss: 102544.8671875\n",
      "\n",
      "Batch Loss: 101129.9921875\n",
      "\n",
      "Batch Loss: 100747.3359375\n",
      "\n",
      "Batch Loss: 100417.984375\n",
      "\n",
      "Batch Loss: 98840.4375\n",
      "\n",
      "Batch Loss: 99093.734375\n",
      "\n",
      "Batch Loss: 97914.7265625\n",
      "\n",
      "Batch Loss: 97649.8984375\n",
      "\n",
      "Batch Loss: 97040.4921875\n",
      "\n",
      "Batch Loss: 94672.9921875\n",
      "\n",
      "Batch Loss: 95347.65625\n",
      "\n",
      "Batch Loss: 93874.96875\n",
      "\n",
      "Batch Loss: 93818.53125\n",
      "\n",
      "Batch Loss: 92660.375\n",
      "\n",
      "Batch Loss: 92465.6015625\n",
      "\n",
      "Batch Loss: 91669.6328125\n",
      "\n",
      "Batch Loss: 93028.71875\n",
      "\n",
      "Batch Loss: 89953.1328125\n",
      "\n",
      "Batch Loss: 88114.625\n",
      "\n",
      "Batch Loss: 88260.7421875\n",
      "\n",
      "Batch Loss: 88471.859375\n",
      "\n",
      "Batch Loss: 87943.046875\n",
      "\n",
      "Batch Loss: 87168.46875\n",
      "\n",
      "Batch Loss: 85274.78125\n",
      "\n",
      "Batch Loss: 85977.3359375\n",
      "\n",
      "Batch Loss: 84781.2265625\n",
      "\n",
      "Batch Loss: 84933.171875\n",
      "\n",
      "Batch Loss: 84319.0078125\n",
      "\n",
      "Batch Loss: 82364.09375\n",
      "\n",
      "Batch Loss: 82106.7265625\n",
      "\n",
      "Batch Loss: 82312.546875\n",
      "\n",
      "Batch Loss: 80907.671875\n",
      "\n",
      "Batch Loss: 81567.890625\n",
      "\n",
      "Batch Loss: 79448.3359375\n",
      "\n",
      "Batch Loss: 80980.4140625\n",
      "\n",
      "Batch Loss: 79310.8984375\n",
      "\n",
      "Batch Loss: 78827.7265625\n",
      "\n",
      "Batch Loss: 78638.6484375\n",
      "\n",
      "Batch Loss: 78620.6875\n",
      "\n",
      "Batch Loss: 76303.1015625\n",
      "\n",
      "Batch Loss: 76632.234375\n",
      "\n",
      "Batch Loss: 75817.734375\n",
      "\n",
      "Batch Loss: 75120.84375\n",
      "\n",
      "Batch Loss: 73419.5859375\n",
      "\n",
      "Batch Loss: 75418.484375\n",
      "\n",
      "Batch Loss: 73426.53125\n",
      "\n",
      "Batch Loss: 74085.546875\n",
      "\n",
      "Batch Loss: 71948.171875\n",
      "\n",
      "Batch Loss: 72480.1015625\n",
      "\n",
      "Batch Loss: 72531.59375\n",
      "\n",
      "Batch Loss: 71179.9140625\n",
      "\n",
      "Batch Loss: 72067.5703125\n",
      "\n",
      "Batch Loss: 69845.0234375\n",
      "\n",
      "Batch Loss: 69659.8125\n",
      "\n",
      "Batch Loss: 69133.515625\n",
      "\n",
      "Batch Loss: 68968.2421875\n",
      "\n",
      "Batch Loss: 68147.4765625\n",
      "\n",
      "Batch Loss: 67183.625\n",
      "\n",
      "Batch Loss: 66253.0546875\n",
      "\n",
      "Batch Loss: 67827.9921875\n",
      "\n",
      "Batch Loss: 65828.859375\n",
      "\n",
      "Batch Loss: 65423.08203125\n",
      "\n",
      "Batch Loss: 64755.1640625\n",
      "\n",
      "Batch Loss: 64380.78125\n",
      "\n",
      "Batch Loss: 64714.6875\n",
      "\n",
      "Batch Loss: 62193.68359375\n",
      "\n",
      "Batch Loss: 64219.67578125\n",
      "\n",
      "Batch Loss: 61790.90625\n",
      "\n",
      "Batch Loss: 61312.37109375\n",
      "\n",
      "Batch Loss: 61447.1640625\n",
      "\n",
      "Batch Loss: 62893.796875\n",
      "\n",
      "Batch Loss: 61031.921875\n",
      "\n",
      "Batch Loss: 61270.96875\n",
      "\n",
      "Batch Loss: 60130.1328125\n",
      "\n",
      "Batch Loss: 59592.296875\n",
      "\n",
      "Batch Loss: 58629.0390625\n",
      "\n",
      "Batch Loss: 59748.875\n",
      "\n",
      "Batch Loss: 58915.78125\n",
      "\n",
      "Batch Loss: 58535.5859375\n",
      "\n",
      "Batch Loss: 57959.859375\n",
      "\n",
      "Batch Loss: 56472.72265625\n",
      "\n",
      "Batch Loss: 55704.55859375\n",
      "\n",
      "Batch Loss: 56442.51171875\n",
      "\n",
      "Batch Loss: 55349.515625\n",
      "\n",
      "Batch Loss: 55436.8828125\n",
      "\n",
      "Batch Loss: 54465.01171875\n",
      "\n",
      "Batch Loss: 55755.4296875\n",
      "\n",
      "Batch Loss: 54521.7109375\n",
      "\n",
      "Batch Loss: 52144.1015625\n",
      "\n",
      "Batch Loss: 52053.22265625\n",
      "\n",
      "Batch Loss: 54080.4453125\n",
      "\n",
      "Batch Loss: 51525.6015625\n",
      "\n",
      "Batch Loss: 52665.515625\n",
      "\n",
      "Batch Loss: 51701.46875\n",
      "\n",
      "Batch Loss: 50487.5234375\n",
      "\n",
      "Batch Loss: 51153.76171875\n",
      "\n",
      "Batch Loss: 50002.16015625\n",
      "\n",
      "Batch Loss: 51657.59375\n",
      "\n",
      "Batch Loss: 50947.26171875\n",
      "\n",
      "Batch Loss: 48554.6953125\n",
      "\n",
      "Batch Loss: 50980.35546875\n",
      "\n",
      "Batch Loss: 48970.4375\n",
      "\n",
      "Batch Loss: 49010.65234375\n",
      "\n",
      "Batch Loss: 48443.078125\n",
      "\n",
      "Batch Loss: 48867.40234375\n",
      "\n",
      "Batch Loss: 46720.265625\n",
      "\n",
      "Batch Loss: 45817.9453125\n",
      "\n",
      "Batch Loss: 47185.53125\n",
      "\n",
      "Batch Loss: 47380.984375\n",
      "\n",
      "Batch Loss: 45331.0390625\n",
      "\n",
      "Batch Loss: 44636.2421875\n",
      "\n",
      "Batch Loss: 45401.35546875\n",
      "\n",
      "Batch Loss: 46267.94921875\n",
      "\n",
      "Batch Loss: 45786.1875\n",
      "\n",
      "Batch Loss: 45315.5625\n",
      "\n",
      "Batch Loss: 43861.32421875\n",
      "\n",
      "Batch Loss: 43558.76171875\n",
      "\n",
      "Batch Loss: 43970.765625\n",
      "\n",
      "Batch Loss: 44030.3125\n",
      "\n",
      "Batch Loss: 43258.68359375\n",
      "\n",
      "Batch Loss: 42706.89453125\n",
      "\n",
      "Batch Loss: 42125.4765625\n",
      "\n",
      "Batch Loss: 41582.86328125\n",
      "\n",
      "Batch Loss: 41955.94921875\n",
      "\n",
      "Batch Loss: 41005.3359375\n",
      "\n",
      "Batch Loss: 40798.40625\n",
      "\n",
      "Batch Loss: 40646.04296875\n",
      "\n",
      "Batch Loss: 41075.4921875\n",
      "\n",
      "Batch Loss: 40522.76171875\n",
      "\n",
      "Batch Loss: 38986.39453125\n",
      "\n",
      "Batch Loss: 39890.02734375\n",
      "\n",
      "Batch Loss: 40424.66015625\n",
      "\n",
      "Batch Loss: 39733.4765625\n",
      "\n",
      "Batch Loss: 39941.05859375\n",
      "\n",
      "Batch Loss: 38612.09375\n",
      "\n",
      "Batch Loss: 39807.63671875\n",
      "\n",
      "Batch Loss: 37778.0078125\n",
      "\n",
      "Batch Loss: 38019.8359375\n",
      "\n",
      "Batch Loss: 36682.734375\n",
      "\n",
      "Batch Loss: 37186.14453125\n",
      "\n",
      "Batch Loss: 36641.265625\n",
      "\n",
      "Batch Loss: 37479.59375\n",
      "\n",
      "Batch Loss: 36376.3671875\n",
      "\n",
      "Batch Loss: 37046.12890625\n",
      "\n",
      "Batch Loss: 37498.81640625\n",
      "\n",
      "Batch Loss: 36041.11328125\n",
      "\n",
      "Batch Loss: 34419.953125\n",
      "\n",
      "Batch Loss: 34739.28125\n",
      "\n",
      "Batch Loss: 33902.96875\n",
      "\n",
      "Batch Loss: 36284.6328125\n",
      "\n",
      "Batch Loss: 34645.45703125\n",
      "\n",
      "Batch Loss: 34247.61328125\n",
      "\n",
      "Batch Loss: 33786.30078125\n",
      "\n",
      "Batch Loss: 34299.88671875\n",
      "\n",
      "Batch Loss: 33312.1015625\n",
      "\n",
      "Batch Loss: 33502.48828125\n",
      "\n",
      "Batch Loss: 33011.22265625\n",
      "\n",
      "Batch Loss: 33916.85546875\n",
      "\n",
      "Batch Loss: 33288.6640625\n",
      "\n",
      "Batch Loss: 32685.3359375\n",
      "\n",
      "Batch Loss: 32267.111328125\n",
      "\n",
      "Batch Loss: 34262.0703125\n",
      "\n",
      "Batch Loss: 32294.40625\n",
      "\n",
      "Batch Loss: 30968.13671875\n",
      "\n",
      "Batch Loss: 31893.296875\n",
      "\n",
      "Batch Loss: 32090.880859375\n",
      "\n",
      "Batch Loss: 31332.669921875\n",
      "\n",
      "Batch Loss: 31347.529296875\n",
      "\n",
      "Batch Loss: 31648.224609375\n",
      "\n",
      "Batch Loss: 30192.927734375\n",
      "\n",
      "Batch Loss: 30427.384765625\n",
      "\n",
      "Batch Loss: 28776.56640625\n",
      "\n",
      "Batch Loss: 29380.4765625\n",
      "\n",
      "Batch Loss: 30609.498046875\n",
      "\n",
      "Batch Loss: 29154.57421875\n",
      "\n",
      "Batch Loss: 28490.68359375\n",
      "\n",
      "Batch Loss: 28866.3828125\n",
      "\n",
      "Batch Loss: 28896.08203125\n",
      "\n",
      "Batch Loss: 28431.07421875\n",
      "\n",
      "Batch Loss: 27659.8828125\n",
      "\n",
      "Batch Loss: 28879.8515625\n",
      "\n",
      "Batch Loss: 27879.9296875\n",
      "\n",
      "Batch Loss: 27525.53125\n",
      "\n",
      "Batch Loss: 27679.447265625\n",
      "\n",
      "Batch Loss: 27750.724609375\n",
      "\n",
      "Batch Loss: 26954.583984375\n",
      "\n",
      "Batch Loss: 29004.3515625\n",
      "\n",
      "Batch Loss: 27417.4375\n",
      "\n",
      "Batch Loss: 26033.796875\n",
      "\n",
      "Batch Loss: 26586.478515625\n",
      "\n",
      "Batch Loss: 26314.076171875\n",
      "\n",
      "Batch Loss: 25426.583984375\n",
      "\n",
      "Batch Loss: 26452.244140625\n",
      "\n",
      "Batch Loss: 24943.103515625\n",
      "\n",
      "Batch Loss: 26392.65625\n",
      "\n",
      "Batch Loss: 25059.853515625\n",
      "\n",
      "Batch Loss: 25768.0703125\n",
      "\n",
      "Batch Loss: 24932.013671875\n",
      "\n",
      "Batch Loss: 25178.728515625\n",
      "\n",
      "Batch Loss: 24292.853515625\n",
      "\n",
      "Batch Loss: 25491.439453125\n",
      "\n",
      "Batch Loss: 24651.51171875\n",
      "\n",
      "Batch Loss: 25272.96484375\n",
      "\n",
      "Batch Loss: 24509.0546875\n",
      "\n",
      "Batch Loss: 24545.05859375\n",
      "\n",
      "Batch Loss: 23932.75\n",
      "\n",
      "Batch Loss: 23644.7265625\n",
      "\n",
      "Batch Loss: 23861.7734375\n",
      "\n",
      "Batch Loss: 23321.736328125\n",
      "\n",
      "Batch Loss: 21817.990234375\n",
      "\n",
      "Batch Loss: 22743.09765625\n",
      "\n",
      "Batch Loss: 23268.767578125\n",
      "\n",
      "Batch Loss: 23700.16015625\n",
      "\n",
      "Batch Loss: 22724.787109375\n",
      "\n",
      "Batch Loss: 22674.693359375\n",
      "\n",
      "Batch Loss: 22285.447265625\n",
      "\n",
      "Batch Loss: 22644.40234375\n",
      "\n",
      "Batch Loss: 21524.53125\n",
      "\n",
      "Batch Loss: 21584.50390625\n",
      "\n",
      "Batch Loss: 22040.126953125\n",
      "\n",
      "Batch Loss: 22020.0390625\n",
      "\n",
      "Batch Loss: 20843.234375\n",
      "\n",
      "Batch Loss: 21188.859375\n",
      "\n",
      "Batch Loss: 21294.486328125\n",
      "\n",
      "Batch Loss: 21175.515625\n",
      "\n",
      "Batch Loss: 21032.4765625\n",
      "\n",
      "Batch Loss: 21235.732421875\n",
      "\n",
      "Batch Loss: 21113.58984375\n",
      "\n",
      "Batch Loss: 19775.640625\n",
      "\n",
      "Batch Loss: 20981.810546875\n",
      "\n",
      "Batch Loss: 20518.615234375\n",
      "\n",
      "Batch Loss: 19994.314453125\n",
      "\n",
      "Batch Loss: 20134.39453125\n",
      "\n",
      "Batch Loss: 20931.455078125\n",
      "\n",
      "Batch Loss: 20071.171875\n",
      "\n",
      "Batch Loss: 18885.306640625\n",
      "\n",
      "Batch Loss: 20035.4921875\n",
      "\n",
      "Batch Loss: 19467.48828125\n",
      "\n",
      "Batch Loss: 19084.630859375\n",
      "\n",
      "Batch Loss: 19132.267578125\n",
      "\n",
      "Batch Loss: 19011.71875\n",
      "\n",
      "Batch Loss: 19168.125\n",
      "\n",
      "Batch Loss: 19539.078125\n",
      "\n",
      "Batch Loss: 19145.849609375\n",
      "\n",
      "Batch Loss: 17269.17578125\n",
      "\n",
      "Batch Loss: 17905.55859375\n",
      "\n",
      "Batch Loss: 18024.29296875\n",
      "\n",
      "Batch Loss: 18701.458984375\n",
      "\n",
      "Batch Loss: 17824.677734375\n",
      "\n",
      "Batch Loss: 18438.90625\n",
      "\n",
      "Batch Loss: 17983.119140625\n",
      "\n",
      "Batch Loss: 17509.72265625\n",
      "\n",
      "Batch Loss: 16950.220703125\n",
      "\n",
      "Batch Loss: 17808.017578125\n",
      "\n",
      "Batch Loss: 18103.580078125\n",
      "\n",
      "Batch Loss: 17120.15234375\n",
      "\n",
      "Batch Loss: 17447.400390625\n",
      "\n",
      "Batch Loss: 17650.091796875\n",
      "\n",
      "Batch Loss: 17137.54296875\n",
      "\n",
      "Batch Loss: 16449.158203125\n",
      "\n",
      "Batch Loss: 17401.89453125\n",
      "\n",
      "Batch Loss: 16584.494140625\n",
      "\n",
      "Batch Loss: 16176.8798828125\n",
      "\n",
      "Batch Loss: 16874.25\n",
      "\n",
      "Batch Loss: 16227.8935546875\n",
      "\n",
      "Batch Loss: 16028.8974609375\n",
      "\n",
      "Batch Loss: 16137.2587890625\n",
      "\n",
      "Batch Loss: 16318.255859375\n",
      "\n",
      "Batch Loss: 16297.923828125\n",
      "\n",
      "Batch Loss: 16906.033203125\n",
      "\n",
      "Batch Loss: 15465.1533203125\n",
      "\n",
      "Batch Loss: 15843.2861328125\n",
      "\n",
      "Batch Loss: 15313.1279296875\n",
      "\n",
      "Batch Loss: 15573.35546875\n",
      "\n",
      "Batch Loss: 14898.0302734375\n",
      "\n",
      "Batch Loss: 15027.83203125\n",
      "\n",
      "Batch Loss: 15661.1337890625\n",
      "\n",
      "Batch Loss: 14546.201171875\n",
      "\n",
      "Batch Loss: 15181.4013671875\n",
      "\n",
      "Batch Loss: 15205.1064453125\n",
      "\n",
      "Batch Loss: 14577.3134765625\n",
      "\n",
      "Batch Loss: 14426.076171875\n",
      "\n",
      "Batch Loss: 14697.9765625\n",
      "\n",
      "Batch Loss: 15046.2880859375\n",
      "\n",
      "Batch Loss: 14639.9892578125\n",
      "\n",
      "Batch Loss: 14175.27734375\n",
      "\n",
      "Batch Loss: 14566.548828125\n",
      "\n",
      "Batch Loss: 14349.04296875\n",
      "\n",
      "Batch Loss: 13677.3779296875\n",
      "\n",
      "Batch Loss: 14481.8828125\n",
      "\n",
      "Batch Loss: 13773.8828125\n",
      "\n",
      "Batch Loss: 14453.19140625\n",
      "\n",
      "Batch Loss: 12962.78515625\n",
      "\n",
      "Batch Loss: 13879.3427734375\n",
      "\n",
      "Batch Loss: 14800.580078125\n",
      "\n",
      "Batch Loss: 14267.89453125\n",
      "\n",
      "Batch Loss: 8017.89599609375\n",
      "\n",
      "Epoch 1, Average Loss: 75032.17729705514\n",
      "\n",
      "Batch Loss: 13190.763671875\n",
      "\n",
      "Batch Loss: 12848.267578125\n",
      "\n",
      "Batch Loss: 12844.7724609375\n",
      "\n",
      "Batch Loss: 13237.3076171875\n",
      "\n",
      "Batch Loss: 13258.3525390625\n",
      "\n",
      "Batch Loss: 12772.9501953125\n",
      "\n",
      "Batch Loss: 13066.2978515625\n",
      "\n",
      "Batch Loss: 12695.5478515625\n",
      "\n",
      "Batch Loss: 13210.2705078125\n",
      "\n",
      "Batch Loss: 12722.0478515625\n",
      "\n",
      "Batch Loss: 13086.0712890625\n",
      "\n",
      "Batch Loss: 13698.583984375\n",
      "\n",
      "Batch Loss: 12192.3603515625\n",
      "\n",
      "Batch Loss: 12791.2275390625\n",
      "\n",
      "Batch Loss: 12053.4375\n",
      "\n",
      "Batch Loss: 11904.91796875\n",
      "\n",
      "Batch Loss: 12783.8134765625\n",
      "\n",
      "Batch Loss: 12531.8583984375\n",
      "\n",
      "Batch Loss: 12182.3095703125\n",
      "\n",
      "Batch Loss: 12111.3935546875\n",
      "\n",
      "Batch Loss: 11949.599609375\n",
      "\n",
      "Batch Loss: 12147.388671875\n",
      "\n",
      "Batch Loss: 12520.0205078125\n",
      "\n",
      "Batch Loss: 11834.9111328125\n",
      "\n",
      "Batch Loss: 12176.341796875\n",
      "\n",
      "Batch Loss: 12035.677734375\n",
      "\n",
      "Batch Loss: 11806.3701171875\n",
      "\n",
      "Batch Loss: 11980.751953125\n",
      "\n",
      "Batch Loss: 12655.806640625\n",
      "\n",
      "Batch Loss: 11917.216796875\n",
      "\n",
      "Batch Loss: 11831.341796875\n",
      "\n",
      "Batch Loss: 11982.5078125\n",
      "\n",
      "Batch Loss: 11953.6103515625\n",
      "\n",
      "Batch Loss: 11610.7255859375\n",
      "\n",
      "Batch Loss: 10920.0849609375\n",
      "\n",
      "Batch Loss: 11835.38671875\n",
      "\n",
      "Batch Loss: 11514.740234375\n",
      "\n",
      "Batch Loss: 11042.7578125\n",
      "\n",
      "Batch Loss: 11458.44921875\n",
      "\n",
      "Batch Loss: 10560.2314453125\n",
      "\n",
      "Batch Loss: 11379.8857421875\n",
      "\n",
      "Batch Loss: 10214.107421875\n",
      "\n",
      "Batch Loss: 10886.8408203125\n",
      "\n",
      "Batch Loss: 11237.541015625\n",
      "\n",
      "Batch Loss: 11079.3828125\n",
      "\n",
      "Batch Loss: 11339.43359375\n",
      "\n",
      "Batch Loss: 9822.7373046875\n",
      "\n",
      "Batch Loss: 10308.0244140625\n",
      "\n",
      "Batch Loss: 10075.5537109375\n",
      "\n",
      "Batch Loss: 10275.033203125\n",
      "\n",
      "Batch Loss: 10201.2451171875\n",
      "\n",
      "Batch Loss: 10224.384765625\n",
      "\n",
      "Batch Loss: 10458.6708984375\n",
      "\n",
      "Batch Loss: 10014.40234375\n",
      "\n",
      "Batch Loss: 10381.052734375\n",
      "\n",
      "Batch Loss: 10226.14453125\n",
      "\n",
      "Batch Loss: 10735.66796875\n",
      "\n",
      "Batch Loss: 10296.9677734375\n",
      "\n",
      "Batch Loss: 9359.8017578125\n",
      "\n",
      "Batch Loss: 9758.7685546875\n",
      "\n",
      "Batch Loss: 9778.9755859375\n",
      "\n",
      "Batch Loss: 10107.951171875\n",
      "\n",
      "Batch Loss: 10158.205078125\n",
      "\n",
      "Batch Loss: 8746.1298828125\n",
      "\n",
      "Batch Loss: 9057.8037109375\n",
      "\n",
      "Batch Loss: 9319.2763671875\n",
      "\n",
      "Batch Loss: 9069.8974609375\n",
      "\n",
      "Batch Loss: 10118.2216796875\n",
      "\n",
      "Batch Loss: 9564.6376953125\n",
      "\n",
      "Batch Loss: 9368.5859375\n",
      "\n",
      "Batch Loss: 9038.4267578125\n",
      "\n",
      "Batch Loss: 8578.3271484375\n",
      "\n",
      "Batch Loss: 8850.662109375\n",
      "\n",
      "Batch Loss: 9562.8955078125\n",
      "\n",
      "Batch Loss: 9118.083984375\n",
      "\n",
      "Batch Loss: 8832.474609375\n",
      "\n",
      "Batch Loss: 8922.501953125\n",
      "\n",
      "Batch Loss: 8749.0654296875\n",
      "\n",
      "Batch Loss: 8045.060546875\n",
      "\n",
      "Batch Loss: 8536.794921875\n",
      "\n",
      "Batch Loss: 8467.0380859375\n",
      "\n",
      "Batch Loss: 8840.515625\n",
      "\n",
      "Batch Loss: 8867.763671875\n",
      "\n",
      "Batch Loss: 8153.6533203125\n",
      "\n",
      "Batch Loss: 8793.830078125\n",
      "\n",
      "Batch Loss: 8842.0\n",
      "\n",
      "Batch Loss: 8530.5888671875\n",
      "\n",
      "Batch Loss: 8750.8203125\n",
      "\n",
      "Batch Loss: 8688.546875\n",
      "\n",
      "Batch Loss: 8074.63525390625\n",
      "\n",
      "Batch Loss: 8787.6787109375\n",
      "\n",
      "Batch Loss: 8657.669921875\n",
      "\n",
      "Batch Loss: 8667.4013671875\n",
      "\n",
      "Batch Loss: 8717.8837890625\n",
      "\n",
      "Batch Loss: 8091.41064453125\n",
      "\n",
      "Batch Loss: 8390.5966796875\n",
      "\n",
      "Batch Loss: 7972.97265625\n",
      "\n",
      "Batch Loss: 8541.2451171875\n",
      "\n",
      "Batch Loss: 8422.26171875\n",
      "\n",
      "Batch Loss: 8023.1025390625\n",
      "\n",
      "Batch Loss: 7999.18994140625\n",
      "\n",
      "Batch Loss: 8195.9775390625\n",
      "\n",
      "Batch Loss: 8063.26171875\n",
      "\n",
      "Batch Loss: 8163.2626953125\n",
      "\n",
      "Batch Loss: 7819.921875\n",
      "\n",
      "Batch Loss: 7753.947265625\n",
      "\n",
      "Batch Loss: 7402.51416015625\n",
      "\n",
      "Batch Loss: 7676.63916015625\n",
      "\n",
      "Batch Loss: 8716.462890625\n",
      "\n",
      "Batch Loss: 7496.47314453125\n",
      "\n",
      "Batch Loss: 7695.0986328125\n",
      "\n",
      "Batch Loss: 7863.88720703125\n",
      "\n",
      "Batch Loss: 7593.00439453125\n",
      "\n",
      "Batch Loss: 7240.91162109375\n",
      "\n",
      "Batch Loss: 7414.64990234375\n",
      "\n",
      "Batch Loss: 8190.83740234375\n",
      "\n",
      "Batch Loss: 7540.38134765625\n",
      "\n",
      "Batch Loss: 7230.224609375\n",
      "\n",
      "Batch Loss: 7281.9638671875\n",
      "\n",
      "Batch Loss: 7110.962890625\n",
      "\n",
      "Batch Loss: 7542.96142578125\n",
      "\n",
      "Batch Loss: 6753.7724609375\n",
      "\n",
      "Batch Loss: 6659.65185546875\n",
      "\n",
      "Batch Loss: 7392.9033203125\n",
      "\n",
      "Batch Loss: 7012.8515625\n",
      "\n",
      "Batch Loss: 6950.41259765625\n",
      "\n",
      "Batch Loss: 7411.4296875\n",
      "\n",
      "Batch Loss: 7161.84326171875\n",
      "\n",
      "Batch Loss: 7328.50537109375\n",
      "\n",
      "Batch Loss: 7120.6201171875\n",
      "\n",
      "Batch Loss: 6933.07666015625\n",
      "\n",
      "Batch Loss: 6779.88916015625\n",
      "\n",
      "Batch Loss: 6746.1572265625\n",
      "\n",
      "Batch Loss: 6801.1279296875\n",
      "\n",
      "Batch Loss: 7289.03125\n",
      "\n",
      "Batch Loss: 7284.95654296875\n",
      "\n",
      "Batch Loss: 6707.94775390625\n",
      "\n",
      "Batch Loss: 6918.53271484375\n",
      "\n",
      "Batch Loss: 6764.185546875\n",
      "\n",
      "Batch Loss: 6488.7939453125\n",
      "\n",
      "Batch Loss: 6752.029296875\n",
      "\n",
      "Batch Loss: 6868.68994140625\n",
      "\n",
      "Batch Loss: 6683.2431640625\n",
      "\n",
      "Batch Loss: 6492.875\n",
      "\n",
      "Batch Loss: 7108.283203125\n",
      "\n",
      "Batch Loss: 6463.10546875\n",
      "\n",
      "Batch Loss: 6500.5478515625\n",
      "\n",
      "Batch Loss: 6922.02392578125\n",
      "\n",
      "Batch Loss: 6160.17431640625\n",
      "\n",
      "Batch Loss: 6579.34228515625\n",
      "\n",
      "Batch Loss: 6469.97021484375\n",
      "\n",
      "Batch Loss: 6469.66845703125\n",
      "\n",
      "Batch Loss: 6822.1708984375\n",
      "\n",
      "Batch Loss: 6612.84423828125\n",
      "\n",
      "Batch Loss: 6278.3076171875\n",
      "\n",
      "Batch Loss: 6208.083984375\n",
      "\n",
      "Batch Loss: 6092.9501953125\n",
      "\n",
      "Batch Loss: 5690.3271484375\n",
      "\n",
      "Batch Loss: 6062.6904296875\n",
      "\n",
      "Batch Loss: 6355.50732421875\n",
      "\n",
      "Batch Loss: 5795.21923828125\n",
      "\n",
      "Batch Loss: 6852.71826171875\n",
      "\n",
      "Batch Loss: 5692.39599609375\n",
      "\n",
      "Batch Loss: 6099.2685546875\n",
      "\n",
      "Batch Loss: 6139.83203125\n",
      "\n",
      "Batch Loss: 5808.177734375\n",
      "\n",
      "Batch Loss: 5811.080078125\n",
      "\n",
      "Batch Loss: 5916.728515625\n",
      "\n",
      "Batch Loss: 5763.2353515625\n",
      "\n",
      "Batch Loss: 5835.001953125\n",
      "\n",
      "Batch Loss: 5777.77294921875\n",
      "\n",
      "Batch Loss: 5849.72509765625\n",
      "\n",
      "Batch Loss: 6025.8212890625\n",
      "\n",
      "Batch Loss: 5752.2109375\n",
      "\n",
      "Batch Loss: 5505.94140625\n",
      "\n",
      "Batch Loss: 6329.88623046875\n",
      "\n",
      "Batch Loss: 5292.91650390625\n",
      "\n",
      "Batch Loss: 5627.189453125\n",
      "\n",
      "Batch Loss: 5839.6533203125\n",
      "\n",
      "Batch Loss: 5732.12646484375\n",
      "\n",
      "Batch Loss: 5879.52880859375\n",
      "\n",
      "Batch Loss: 5661.734375\n",
      "\n",
      "Batch Loss: 5437.619140625\n",
      "\n",
      "Batch Loss: 5733.03466796875\n",
      "\n",
      "Batch Loss: 5744.51611328125\n",
      "\n",
      "Batch Loss: 5569.15283203125\n",
      "\n",
      "Batch Loss: 5229.17236328125\n",
      "\n",
      "Batch Loss: 5685.1064453125\n",
      "\n",
      "Batch Loss: 5239.6123046875\n",
      "\n",
      "Batch Loss: 5359.45361328125\n",
      "\n",
      "Batch Loss: 5427.29443359375\n",
      "\n",
      "Batch Loss: 5283.24462890625\n",
      "\n",
      "Batch Loss: 5519.1796875\n",
      "\n",
      "Batch Loss: 5432.7412109375\n",
      "\n",
      "Batch Loss: 5184.359375\n",
      "\n",
      "Batch Loss: 4889.99169921875\n",
      "\n",
      "Batch Loss: 5811.17333984375\n",
      "\n",
      "Batch Loss: 5336.0966796875\n",
      "\n",
      "Batch Loss: 5237.67529296875\n",
      "\n",
      "Batch Loss: 5263.478515625\n",
      "\n",
      "Batch Loss: 5318.3193359375\n",
      "\n",
      "Batch Loss: 5919.41357421875\n",
      "\n",
      "Batch Loss: 5195.55029296875\n",
      "\n",
      "Batch Loss: 4983.53662109375\n",
      "\n",
      "Batch Loss: 5337.52099609375\n",
      "\n",
      "Batch Loss: 4843.18603515625\n",
      "\n",
      "Batch Loss: 5398.52978515625\n",
      "\n",
      "Batch Loss: 5200.42919921875\n",
      "\n",
      "Batch Loss: 5199.412109375\n",
      "\n",
      "Batch Loss: 5204.6767578125\n",
      "\n",
      "Batch Loss: 5041.7001953125\n",
      "\n",
      "Batch Loss: 4763.76708984375\n",
      "\n",
      "Batch Loss: 4640.15087890625\n",
      "\n",
      "Batch Loss: 4643.26708984375\n",
      "\n",
      "Batch Loss: 4769.2099609375\n",
      "\n",
      "Batch Loss: 4815.40380859375\n",
      "\n",
      "Batch Loss: 4800.017578125\n",
      "\n",
      "Batch Loss: 5705.1376953125\n",
      "\n",
      "Batch Loss: 4768.33203125\n",
      "\n",
      "Batch Loss: 4548.82275390625\n",
      "\n",
      "Batch Loss: 4499.6396484375\n",
      "\n",
      "Batch Loss: 5306.00244140625\n",
      "\n",
      "Batch Loss: 5297.8701171875\n",
      "\n",
      "Batch Loss: 4669.4482421875\n",
      "\n",
      "Batch Loss: 4370.71533203125\n",
      "\n",
      "Batch Loss: 4546.6064453125\n",
      "\n",
      "Batch Loss: 4894.96484375\n",
      "\n",
      "Batch Loss: 4819.83837890625\n",
      "\n",
      "Batch Loss: 4399.28662109375\n",
      "\n",
      "Batch Loss: 4541.9248046875\n",
      "\n",
      "Batch Loss: 4360.3173828125\n",
      "\n",
      "Batch Loss: 4687.10791015625\n",
      "\n",
      "Batch Loss: 4394.26513671875\n",
      "\n",
      "Batch Loss: 4933.17822265625\n",
      "\n",
      "Batch Loss: 4885.95263671875\n",
      "\n",
      "Batch Loss: 4621.4033203125\n",
      "\n",
      "Batch Loss: 4629.0107421875\n",
      "\n",
      "Batch Loss: 4402.50732421875\n",
      "\n",
      "Batch Loss: 4671.359375\n",
      "\n",
      "Batch Loss: 4689.05712890625\n",
      "\n",
      "Batch Loss: 4370.701171875\n",
      "\n",
      "Batch Loss: 4477.7734375\n",
      "\n",
      "Batch Loss: 4145.9130859375\n",
      "\n",
      "Batch Loss: 4281.31640625\n",
      "\n",
      "Batch Loss: 4258.25146484375\n",
      "\n",
      "Batch Loss: 4567.759765625\n",
      "\n",
      "Batch Loss: 4428.7685546875\n",
      "\n",
      "Batch Loss: 4485.02978515625\n",
      "\n",
      "Batch Loss: 4109.97265625\n",
      "\n",
      "Batch Loss: 4382.2880859375\n",
      "\n",
      "Batch Loss: 4229.42138671875\n",
      "\n",
      "Batch Loss: 4104.94384765625\n",
      "\n",
      "Batch Loss: 4925.39599609375\n",
      "\n",
      "Batch Loss: 4331.392578125\n",
      "\n",
      "Batch Loss: 4038.34716796875\n",
      "\n",
      "Batch Loss: 4172.826171875\n",
      "\n",
      "Batch Loss: 4095.197021484375\n",
      "\n",
      "Batch Loss: 3840.86962890625\n",
      "\n",
      "Batch Loss: 3903.063232421875\n",
      "\n",
      "Batch Loss: 4487.453125\n",
      "\n",
      "Batch Loss: 4362.5068359375\n",
      "\n",
      "Batch Loss: 3905.8359375\n",
      "\n",
      "Batch Loss: 3945.477783203125\n",
      "\n",
      "Batch Loss: 4097.771484375\n",
      "\n",
      "Batch Loss: 3934.7626953125\n",
      "\n",
      "Batch Loss: 3898.378173828125\n",
      "\n",
      "Batch Loss: 4043.4970703125\n",
      "\n",
      "Batch Loss: 3827.377197265625\n",
      "\n",
      "Batch Loss: 3826.759765625\n",
      "\n",
      "Batch Loss: 3899.096923828125\n",
      "\n",
      "Batch Loss: 3862.842041015625\n",
      "\n",
      "Batch Loss: 3748.561279296875\n",
      "\n",
      "Batch Loss: 3591.596923828125\n",
      "\n",
      "Batch Loss: 3777.26123046875\n",
      "\n",
      "Batch Loss: 3778.197021484375\n",
      "\n",
      "Batch Loss: 3984.558349609375\n",
      "\n",
      "Batch Loss: 3585.996826171875\n",
      "\n",
      "Batch Loss: 3785.873779296875\n",
      "\n",
      "Batch Loss: 3780.605712890625\n",
      "\n",
      "Batch Loss: 3858.34619140625\n",
      "\n",
      "Batch Loss: 3677.196533203125\n",
      "\n",
      "Batch Loss: 3505.007568359375\n",
      "\n",
      "Batch Loss: 3783.765869140625\n",
      "\n",
      "Batch Loss: 3908.195556640625\n",
      "\n",
      "Batch Loss: 3826.76416015625\n",
      "\n",
      "Batch Loss: 3833.251953125\n",
      "\n",
      "Batch Loss: 3452.19091796875\n",
      "\n",
      "Batch Loss: 3996.957763671875\n",
      "\n",
      "Batch Loss: 3701.12744140625\n",
      "\n",
      "Batch Loss: 3460.845947265625\n",
      "\n",
      "Batch Loss: 3533.938232421875\n",
      "\n",
      "Batch Loss: 3422.991455078125\n",
      "\n",
      "Batch Loss: 3531.201171875\n",
      "\n",
      "Batch Loss: 3734.8154296875\n",
      "\n",
      "Batch Loss: 3926.796142578125\n",
      "\n",
      "Batch Loss: 3772.331298828125\n",
      "\n",
      "Batch Loss: 3810.421875\n",
      "\n",
      "Batch Loss: 3374.662353515625\n",
      "\n",
      "Batch Loss: 3450.669677734375\n",
      "\n",
      "Batch Loss: 3605.63818359375\n",
      "\n",
      "Batch Loss: 3146.96484375\n",
      "\n",
      "Batch Loss: 3402.890625\n",
      "\n",
      "Batch Loss: 3528.706298828125\n",
      "\n",
      "Batch Loss: 3841.947998046875\n",
      "\n",
      "Batch Loss: 3605.859619140625\n",
      "\n",
      "Batch Loss: 3601.398193359375\n",
      "\n",
      "Batch Loss: 3436.083251953125\n",
      "\n",
      "Batch Loss: 3740.53759765625\n",
      "\n",
      "Batch Loss: 3750.39697265625\n",
      "\n",
      "Batch Loss: 3453.527099609375\n",
      "\n",
      "Batch Loss: 3250.81689453125\n",
      "\n",
      "Batch Loss: 3325.06884765625\n",
      "\n",
      "Batch Loss: 3574.52197265625\n",
      "\n",
      "Batch Loss: 3267.15625\n",
      "\n",
      "Batch Loss: 2938.8466796875\n",
      "\n",
      "Batch Loss: 3746.0908203125\n",
      "\n",
      "Batch Loss: 3524.772705078125\n",
      "\n",
      "Batch Loss: 3291.00390625\n",
      "\n",
      "Batch Loss: 3551.005615234375\n",
      "\n",
      "Batch Loss: 3317.0615234375\n",
      "\n",
      "Batch Loss: 3316.318603515625\n",
      "\n",
      "Batch Loss: 3407.89208984375\n",
      "\n",
      "Batch Loss: 3170.23828125\n",
      "\n",
      "Batch Loss: 3577.55029296875\n",
      "\n",
      "Batch Loss: 2988.771240234375\n",
      "\n",
      "Batch Loss: 3259.399169921875\n",
      "\n",
      "Batch Loss: 3435.341552734375\n",
      "\n",
      "Batch Loss: 3392.47607421875\n",
      "\n",
      "Batch Loss: 3532.60888671875\n",
      "\n",
      "Batch Loss: 3120.9619140625\n",
      "\n",
      "Batch Loss: 3100.879638671875\n",
      "\n",
      "Batch Loss: 2877.1689453125\n",
      "\n",
      "Batch Loss: 3273.681396484375\n",
      "\n",
      "Batch Loss: 3038.970947265625\n",
      "\n",
      "Batch Loss: 3470.748046875\n",
      "\n",
      "Batch Loss: 2802.37646484375\n",
      "\n",
      "Batch Loss: 3057.85888671875\n",
      "\n",
      "Batch Loss: 3028.45654296875\n",
      "\n",
      "Batch Loss: 2871.100830078125\n",
      "\n",
      "Batch Loss: 2988.74462890625\n",
      "\n",
      "Batch Loss: 2952.34521484375\n",
      "\n",
      "Batch Loss: 3220.857666015625\n",
      "\n",
      "Batch Loss: 2821.276123046875\n",
      "\n",
      "Batch Loss: 3051.712890625\n",
      "\n",
      "Batch Loss: 3131.547607421875\n",
      "\n",
      "Batch Loss: 2949.195556640625\n",
      "\n",
      "Batch Loss: 3159.897216796875\n",
      "\n",
      "Batch Loss: 3027.584228515625\n",
      "\n",
      "Batch Loss: 3174.96630859375\n",
      "\n",
      "Batch Loss: 2722.853515625\n",
      "\n",
      "Batch Loss: 2970.463623046875\n",
      "\n",
      "Batch Loss: 3015.784912109375\n",
      "\n",
      "Batch Loss: 2929.45849609375\n",
      "\n",
      "Batch Loss: 2867.149658203125\n",
      "\n",
      "Batch Loss: 3023.3408203125\n",
      "\n",
      "Batch Loss: 3359.908447265625\n",
      "\n",
      "Batch Loss: 3414.25732421875\n",
      "\n",
      "Batch Loss: 2842.292724609375\n",
      "\n",
      "Batch Loss: 3102.1875\n",
      "\n",
      "Batch Loss: 3063.440185546875\n",
      "\n",
      "Batch Loss: 3119.307373046875\n",
      "\n",
      "Batch Loss: 2684.22119140625\n",
      "\n",
      "Batch Loss: 2943.886474609375\n",
      "\n",
      "Batch Loss: 3266.418212890625\n",
      "\n",
      "Batch Loss: 2703.517333984375\n",
      "\n",
      "Batch Loss: 3156.58642578125\n",
      "\n",
      "Batch Loss: 2831.14453125\n",
      "\n",
      "Batch Loss: 2717.91943359375\n",
      "\n",
      "Batch Loss: 2731.66357421875\n",
      "\n",
      "Batch Loss: 2609.6875\n",
      "\n",
      "Batch Loss: 3000.595703125\n",
      "\n",
      "Batch Loss: 3202.546630859375\n",
      "\n",
      "Batch Loss: 3144.1015625\n",
      "\n",
      "Batch Loss: 3036.326171875\n",
      "\n",
      "Batch Loss: 2918.417724609375\n",
      "\n",
      "Batch Loss: 2552.184814453125\n",
      "\n",
      "Batch Loss: 2825.81396484375\n",
      "\n",
      "Batch Loss: 2671.299072265625\n",
      "\n",
      "Batch Loss: 2712.7080078125\n",
      "\n",
      "Batch Loss: 2813.99169921875\n",
      "\n",
      "Batch Loss: 2780.310791015625\n",
      "\n",
      "Batch Loss: 2694.462890625\n",
      "\n",
      "Batch Loss: 2699.83349609375\n",
      "\n",
      "Batch Loss: 2728.003662109375\n",
      "\n",
      "Batch Loss: 2816.03857421875\n",
      "\n",
      "Batch Loss: 2767.000244140625\n",
      "\n",
      "Batch Loss: 2893.68408203125\n",
      "\n",
      "Batch Loss: 2642.10107421875\n",
      "\n",
      "Batch Loss: 2701.740234375\n",
      "\n",
      "Batch Loss: 2699.49609375\n",
      "\n",
      "Batch Loss: 2789.0810546875\n",
      "\n",
      "Batch Loss: 2644.758056640625\n",
      "\n",
      "Batch Loss: 3040.138916015625\n",
      "\n",
      "Batch Loss: 2651.41796875\n",
      "\n",
      "Batch Loss: 2588.59423828125\n",
      "\n",
      "Batch Loss: 2319.19970703125\n",
      "\n",
      "Batch Loss: 2542.802001953125\n",
      "\n",
      "Batch Loss: 2593.5341796875\n",
      "\n",
      "Batch Loss: 2643.3095703125\n",
      "\n",
      "Batch Loss: 2713.792236328125\n",
      "\n",
      "Batch Loss: 2303.490234375\n",
      "\n",
      "Batch Loss: 2688.16455078125\n",
      "\n",
      "Batch Loss: 2424.306640625\n",
      "\n",
      "Batch Loss: 2626.502197265625\n",
      "\n",
      "Batch Loss: 2508.26953125\n",
      "\n",
      "Batch Loss: 2371.796630859375\n",
      "\n",
      "Batch Loss: 2654.4462890625\n",
      "\n",
      "Batch Loss: 2684.869873046875\n",
      "\n",
      "Batch Loss: 2557.408447265625\n",
      "\n",
      "Batch Loss: 2648.62109375\n",
      "\n",
      "Batch Loss: 2694.247802734375\n",
      "\n",
      "Batch Loss: 1473.342529296875\n",
      "\n",
      "Epoch 2, Average Loss: 5994.29917004039\n",
      "\n",
      "Batch Loss: 2449.222412109375\n",
      "\n",
      "Batch Loss: 2429.4619140625\n",
      "\n",
      "Batch Loss: 2381.42431640625\n",
      "\n",
      "Batch Loss: 2542.158935546875\n",
      "\n",
      "Batch Loss: 2222.503662109375\n",
      "\n",
      "Batch Loss: 2480.666015625\n",
      "\n",
      "Batch Loss: 2421.6181640625\n",
      "\n",
      "Batch Loss: 2364.979248046875\n",
      "\n",
      "Batch Loss: 2124.880859375\n",
      "\n",
      "Batch Loss: 2349.71240234375\n",
      "\n",
      "Batch Loss: 2458.201904296875\n",
      "\n",
      "Batch Loss: 2402.547607421875\n",
      "\n",
      "Batch Loss: 2352.068603515625\n",
      "\n",
      "Batch Loss: 2537.51611328125\n",
      "\n",
      "Batch Loss: 2334.80078125\n",
      "\n",
      "Batch Loss: 2427.676513671875\n",
      "\n",
      "Batch Loss: 2317.460693359375\n",
      "\n",
      "Batch Loss: 2229.7373046875\n",
      "\n",
      "Batch Loss: 2245.669921875\n",
      "\n",
      "Batch Loss: 2489.4423828125\n",
      "\n",
      "Batch Loss: 2363.371826171875\n",
      "\n",
      "Batch Loss: 2478.52099609375\n",
      "\n",
      "Batch Loss: 2247.275146484375\n",
      "\n",
      "Batch Loss: 2214.53466796875\n",
      "\n",
      "Batch Loss: 2408.178466796875\n",
      "\n",
      "Batch Loss: 2403.52734375\n",
      "\n",
      "Batch Loss: 2333.07958984375\n",
      "\n",
      "Batch Loss: 2217.629638671875\n",
      "\n",
      "Batch Loss: 2296.917236328125\n",
      "\n",
      "Batch Loss: 2177.837158203125\n",
      "\n",
      "Batch Loss: 2110.353271484375\n",
      "\n",
      "Batch Loss: 2319.1181640625\n",
      "\n",
      "Batch Loss: 2341.7470703125\n",
      "\n",
      "Batch Loss: 2166.069091796875\n",
      "\n",
      "Batch Loss: 2212.335205078125\n",
      "\n",
      "Batch Loss: 2240.468994140625\n",
      "\n",
      "Batch Loss: 2243.355224609375\n",
      "\n",
      "Batch Loss: 2271.57958984375\n",
      "\n",
      "Batch Loss: 2070.89453125\n",
      "\n",
      "Batch Loss: 2072.177734375\n",
      "\n",
      "Batch Loss: 2282.6484375\n",
      "\n",
      "Batch Loss: 2433.15234375\n",
      "\n",
      "Batch Loss: 2143.74072265625\n",
      "\n",
      "Batch Loss: 2117.807861328125\n",
      "\n",
      "Batch Loss: 1934.7061767578125\n",
      "\n",
      "Batch Loss: 2072.397705078125\n",
      "\n",
      "Batch Loss: 2070.3134765625\n",
      "\n",
      "Batch Loss: 2095.59326171875\n",
      "\n",
      "Batch Loss: 2017.708984375\n",
      "\n",
      "Batch Loss: 2367.489501953125\n",
      "\n",
      "Batch Loss: 2179.52587890625\n",
      "\n",
      "Batch Loss: 2082.485107421875\n",
      "\n",
      "Batch Loss: 2099.697509765625\n",
      "\n",
      "Batch Loss: 2320.140625\n",
      "\n",
      "Batch Loss: 2227.073974609375\n",
      "\n",
      "Batch Loss: 2173.805419921875\n",
      "\n",
      "Batch Loss: 2144.030029296875\n",
      "\n",
      "Batch Loss: 1955.7052001953125\n",
      "\n",
      "Batch Loss: 2125.271240234375\n",
      "\n",
      "Batch Loss: 1827.6470947265625\n",
      "\n",
      "Batch Loss: 2339.308349609375\n",
      "\n",
      "Batch Loss: 2161.545654296875\n",
      "\n",
      "Batch Loss: 1990.8216552734375\n",
      "\n",
      "Batch Loss: 1908.2713623046875\n",
      "\n",
      "Batch Loss: 2239.782470703125\n",
      "\n",
      "Batch Loss: 1933.10888671875\n",
      "\n",
      "Batch Loss: 2088.274658203125\n",
      "\n",
      "Batch Loss: 2210.9111328125\n",
      "\n",
      "Batch Loss: 2016.626953125\n",
      "\n",
      "Batch Loss: 2144.3671875\n",
      "\n",
      "Batch Loss: 2088.3251953125\n",
      "\n",
      "Batch Loss: 2057.686767578125\n",
      "\n",
      "Batch Loss: 2119.6767578125\n",
      "\n",
      "Batch Loss: 2040.657958984375\n",
      "\n",
      "Batch Loss: 1899.81396484375\n",
      "\n",
      "Batch Loss: 2027.919921875\n",
      "\n",
      "Batch Loss: 2192.34619140625\n",
      "\n",
      "Batch Loss: 1724.5126953125\n",
      "\n",
      "Batch Loss: 2057.333251953125\n",
      "\n",
      "Batch Loss: 2116.73095703125\n",
      "\n",
      "Batch Loss: 2152.856201171875\n",
      "\n",
      "Batch Loss: 1882.005859375\n",
      "\n",
      "Batch Loss: 1890.407958984375\n",
      "\n",
      "Batch Loss: 2068.0341796875\n",
      "\n",
      "Batch Loss: 1965.74072265625\n",
      "\n",
      "Batch Loss: 1905.4449462890625\n",
      "\n",
      "Batch Loss: 2104.375\n",
      "\n",
      "Batch Loss: 2075.042236328125\n",
      "\n",
      "Batch Loss: 1899.0557861328125\n",
      "\n",
      "Batch Loss: 1953.6141357421875\n",
      "\n",
      "Batch Loss: 1915.0711669921875\n",
      "\n",
      "Batch Loss: 1925.4708251953125\n",
      "\n",
      "Batch Loss: 1901.846435546875\n",
      "\n",
      "Batch Loss: 1991.24169921875\n",
      "\n",
      "Batch Loss: 1941.5379638671875\n",
      "\n",
      "Batch Loss: 1770.6463623046875\n",
      "\n",
      "Batch Loss: 2016.2783203125\n",
      "\n",
      "Batch Loss: 1796.2142333984375\n",
      "\n",
      "Batch Loss: 1864.9674072265625\n",
      "\n",
      "Batch Loss: 1994.7012939453125\n",
      "\n",
      "Batch Loss: 1763.0498046875\n",
      "\n",
      "Batch Loss: 1872.164306640625\n",
      "\n",
      "Batch Loss: 1846.4251708984375\n",
      "\n",
      "Batch Loss: 2033.166015625\n",
      "\n",
      "Batch Loss: 2082.647216796875\n",
      "\n",
      "Batch Loss: 2062.546630859375\n",
      "\n",
      "Batch Loss: 1779.1595458984375\n",
      "\n",
      "Batch Loss: 1706.4150390625\n",
      "\n",
      "Batch Loss: 1857.64208984375\n",
      "\n",
      "Batch Loss: 1882.6083984375\n",
      "\n",
      "Batch Loss: 1810.234130859375\n",
      "\n",
      "Batch Loss: 1796.4862060546875\n",
      "\n",
      "Batch Loss: 1801.8682861328125\n",
      "\n",
      "Batch Loss: 1695.912353515625\n",
      "\n",
      "Batch Loss: 1867.0628662109375\n",
      "\n",
      "Batch Loss: 2141.984619140625\n",
      "\n",
      "Batch Loss: 1812.73876953125\n",
      "\n",
      "Batch Loss: 1850.828125\n",
      "\n",
      "Batch Loss: 1752.861572265625\n",
      "\n",
      "Batch Loss: 1848.327880859375\n",
      "\n",
      "Batch Loss: 2000.2796630859375\n",
      "\n",
      "Batch Loss: 1976.0245361328125\n",
      "\n",
      "Batch Loss: 1666.38330078125\n",
      "\n",
      "Batch Loss: 1684.194580078125\n",
      "\n",
      "Batch Loss: 1794.947998046875\n",
      "\n",
      "Batch Loss: 1947.6173095703125\n",
      "\n",
      "Batch Loss: 1915.9283447265625\n",
      "\n",
      "Batch Loss: 1854.5770263671875\n",
      "\n",
      "Batch Loss: 1808.59326171875\n",
      "\n",
      "Batch Loss: 1666.9923095703125\n",
      "\n",
      "Batch Loss: 1641.6976318359375\n",
      "\n",
      "Batch Loss: 1706.045166015625\n",
      "\n",
      "Batch Loss: 1732.12890625\n",
      "\n",
      "Batch Loss: 1959.998046875\n",
      "\n",
      "Batch Loss: 1417.4061279296875\n",
      "\n",
      "Batch Loss: 1535.537353515625\n",
      "\n",
      "Batch Loss: 1792.2979736328125\n",
      "\n",
      "Batch Loss: 1477.001953125\n",
      "\n",
      "Batch Loss: 2106.76611328125\n",
      "\n",
      "Batch Loss: 1696.8607177734375\n",
      "\n",
      "Batch Loss: 1726.8082275390625\n",
      "\n",
      "Batch Loss: 1732.44580078125\n",
      "\n",
      "Batch Loss: 1770.0081787109375\n",
      "\n",
      "Batch Loss: 1651.580078125\n",
      "\n",
      "Batch Loss: 1690.6424560546875\n",
      "\n",
      "Batch Loss: 1559.9422607421875\n",
      "\n",
      "Batch Loss: 1861.7388916015625\n",
      "\n",
      "Batch Loss: 1565.975830078125\n",
      "\n",
      "Batch Loss: 1778.90185546875\n",
      "\n",
      "Batch Loss: 1512.5611572265625\n",
      "\n",
      "Batch Loss: 1602.641357421875\n",
      "\n",
      "Batch Loss: 1772.624267578125\n",
      "\n",
      "Batch Loss: 1766.522216796875\n",
      "\n",
      "Batch Loss: 1625.773193359375\n",
      "\n",
      "Batch Loss: 1815.9681396484375\n",
      "\n",
      "Batch Loss: 1772.2486572265625\n",
      "\n",
      "Batch Loss: 1500.1258544921875\n",
      "\n",
      "Batch Loss: 1627.7725830078125\n",
      "\n",
      "Batch Loss: 1709.3294677734375\n",
      "\n",
      "Batch Loss: 1688.4871826171875\n",
      "\n",
      "Batch Loss: 1551.36328125\n",
      "\n",
      "Batch Loss: 1723.74755859375\n",
      "\n",
      "Batch Loss: 1522.7083740234375\n",
      "\n",
      "Batch Loss: 1544.27978515625\n",
      "\n",
      "Batch Loss: 1714.0001220703125\n",
      "\n",
      "Batch Loss: 1582.6820068359375\n",
      "\n",
      "Batch Loss: 1574.4320068359375\n",
      "\n",
      "Batch Loss: 1729.650146484375\n",
      "\n",
      "Batch Loss: 1659.778564453125\n",
      "\n",
      "Batch Loss: 1520.1124267578125\n",
      "\n",
      "Batch Loss: 1484.39794921875\n",
      "\n",
      "Batch Loss: 1647.222412109375\n",
      "\n",
      "Batch Loss: 1675.5966796875\n",
      "\n",
      "Batch Loss: 1409.2620849609375\n",
      "\n",
      "Batch Loss: 1451.7130126953125\n",
      "\n",
      "Batch Loss: 1483.0286865234375\n",
      "\n",
      "Batch Loss: 1521.19677734375\n",
      "\n",
      "Batch Loss: 1620.94189453125\n",
      "\n",
      "Batch Loss: 1410.8104248046875\n",
      "\n",
      "Batch Loss: 1572.629150390625\n",
      "\n",
      "Batch Loss: 1409.80126953125\n",
      "\n",
      "Batch Loss: 1720.548095703125\n",
      "\n",
      "Batch Loss: 1661.67333984375\n",
      "\n",
      "Batch Loss: 1538.6456298828125\n",
      "\n",
      "Batch Loss: 1441.50146484375\n",
      "\n",
      "Batch Loss: 1554.009521484375\n",
      "\n",
      "Batch Loss: 1412.8018798828125\n",
      "\n",
      "Batch Loss: 1739.598876953125\n",
      "\n",
      "Batch Loss: 1454.965576171875\n",
      "\n",
      "Batch Loss: 1493.182373046875\n",
      "\n",
      "Batch Loss: 1516.442626953125\n",
      "\n",
      "Batch Loss: 1326.4764404296875\n",
      "\n",
      "Batch Loss: 1661.27783203125\n",
      "\n",
      "Batch Loss: 1731.373046875\n",
      "\n",
      "Batch Loss: 1541.9517822265625\n",
      "\n",
      "Batch Loss: 1444.8819580078125\n",
      "\n",
      "Batch Loss: 1337.131103515625\n",
      "\n",
      "Batch Loss: 1338.3104248046875\n",
      "\n",
      "Batch Loss: 1663.2061767578125\n",
      "\n",
      "Batch Loss: 1914.8958740234375\n",
      "\n",
      "Batch Loss: 1321.278564453125\n",
      "\n",
      "Batch Loss: 1626.29443359375\n",
      "\n",
      "Batch Loss: 1429.89599609375\n",
      "\n",
      "Batch Loss: 1464.1680908203125\n",
      "\n",
      "Batch Loss: 1446.4945068359375\n",
      "\n",
      "Batch Loss: 1654.9796142578125\n",
      "\n",
      "Batch Loss: 1581.8336181640625\n",
      "\n",
      "Batch Loss: 1326.3399658203125\n",
      "\n",
      "Batch Loss: 1464.925537109375\n",
      "\n",
      "Batch Loss: 1414.37890625\n",
      "\n",
      "Batch Loss: 1513.2528076171875\n",
      "\n",
      "Batch Loss: 1326.141845703125\n",
      "\n",
      "Batch Loss: 1583.7838134765625\n",
      "\n",
      "Batch Loss: 1644.771240234375\n",
      "\n",
      "Batch Loss: 1591.924072265625\n",
      "\n",
      "Batch Loss: 1343.619873046875\n",
      "\n",
      "Batch Loss: 1449.5186767578125\n",
      "\n",
      "Batch Loss: 1620.3931884765625\n",
      "\n",
      "Batch Loss: 1394.1572265625\n",
      "\n",
      "Batch Loss: 1472.92236328125\n",
      "\n",
      "Batch Loss: 1490.7779541015625\n",
      "\n",
      "Batch Loss: 1501.5841064453125\n",
      "\n",
      "Batch Loss: 1487.114990234375\n",
      "\n",
      "Batch Loss: 1390.6180419921875\n",
      "\n",
      "Batch Loss: 1284.0133056640625\n",
      "\n",
      "Batch Loss: 1454.0845947265625\n",
      "\n",
      "Batch Loss: 1253.4461669921875\n",
      "\n",
      "Batch Loss: 1278.755126953125\n",
      "\n",
      "Batch Loss: 1415.326171875\n",
      "\n",
      "Batch Loss: 1365.4775390625\n",
      "\n",
      "Batch Loss: 1314.605224609375\n",
      "\n",
      "Batch Loss: 1333.962646484375\n",
      "\n",
      "Batch Loss: 1459.18798828125\n",
      "\n",
      "Batch Loss: 1403.0648193359375\n",
      "\n",
      "Batch Loss: 1384.5240478515625\n",
      "\n",
      "Batch Loss: 1257.7855224609375\n",
      "\n",
      "Batch Loss: 1354.5220947265625\n",
      "\n",
      "Batch Loss: 1349.794677734375\n",
      "\n",
      "Batch Loss: 1424.60498046875\n",
      "\n",
      "Batch Loss: 1421.3218994140625\n",
      "\n",
      "Batch Loss: 1541.5955810546875\n",
      "\n",
      "Batch Loss: 1321.0074462890625\n",
      "\n",
      "Batch Loss: 1360.016357421875\n",
      "\n",
      "Batch Loss: 1325.1153564453125\n",
      "\n",
      "Batch Loss: 1449.9599609375\n",
      "\n",
      "Batch Loss: 1443.420654296875\n",
      "\n",
      "Batch Loss: 1457.1729736328125\n",
      "\n",
      "Batch Loss: 1462.4287109375\n",
      "\n",
      "Batch Loss: 1434.852783203125\n",
      "\n",
      "Batch Loss: 1461.5758056640625\n",
      "\n",
      "Batch Loss: 1476.7335205078125\n",
      "\n",
      "Batch Loss: 1418.376708984375\n",
      "\n",
      "Batch Loss: 1373.0946044921875\n",
      "\n",
      "Batch Loss: 1325.436767578125\n",
      "\n",
      "Batch Loss: 1309.99609375\n",
      "\n",
      "Batch Loss: 1196.3643798828125\n",
      "\n",
      "Batch Loss: 1295.1796875\n",
      "\n",
      "Batch Loss: 1305.642333984375\n",
      "\n",
      "Batch Loss: 1203.9942626953125\n",
      "\n",
      "Batch Loss: 1477.668212890625\n",
      "\n",
      "Batch Loss: 1293.8353271484375\n",
      "\n",
      "Batch Loss: 1220.7655029296875\n",
      "\n",
      "Batch Loss: 1390.522705078125\n",
      "\n",
      "Batch Loss: 1538.162109375\n",
      "\n",
      "Batch Loss: 1397.37890625\n",
      "\n",
      "Batch Loss: 1380.0506591796875\n",
      "\n",
      "Batch Loss: 1248.6937255859375\n",
      "\n",
      "Batch Loss: 1153.0037841796875\n",
      "\n",
      "Batch Loss: 1394.06201171875\n",
      "\n",
      "Batch Loss: 1271.010986328125\n",
      "\n",
      "Batch Loss: 1312.955322265625\n",
      "\n",
      "Batch Loss: 1420.166748046875\n",
      "\n",
      "Batch Loss: 1240.911865234375\n",
      "\n",
      "Batch Loss: 1105.6690673828125\n",
      "\n",
      "Batch Loss: 1257.843017578125\n",
      "\n",
      "Batch Loss: 1229.8770751953125\n",
      "\n",
      "Batch Loss: 1359.9161376953125\n",
      "\n",
      "Batch Loss: 1178.803955078125\n",
      "\n",
      "Batch Loss: 1227.2821044921875\n",
      "\n",
      "Batch Loss: 1214.00634765625\n",
      "\n",
      "Batch Loss: 1328.4810791015625\n",
      "\n",
      "Batch Loss: 1213.6357421875\n",
      "\n",
      "Batch Loss: 1263.2698974609375\n",
      "\n",
      "Batch Loss: 1121.5047607421875\n",
      "\n",
      "Batch Loss: 1262.1556396484375\n",
      "\n",
      "Batch Loss: 1194.2703857421875\n",
      "\n",
      "Batch Loss: 1322.6649169921875\n",
      "\n",
      "Batch Loss: 1185.765380859375\n",
      "\n",
      "Batch Loss: 1255.153564453125\n",
      "\n",
      "Batch Loss: 1302.87646484375\n",
      "\n",
      "Batch Loss: 1245.326171875\n",
      "\n",
      "Batch Loss: 1080.7236328125\n",
      "\n",
      "Batch Loss: 1308.2080078125\n",
      "\n",
      "Batch Loss: 1137.107421875\n",
      "\n",
      "Batch Loss: 1111.0859375\n",
      "\n",
      "Batch Loss: 1064.779541015625\n",
      "\n",
      "Batch Loss: 1288.890869140625\n",
      "\n",
      "Batch Loss: 1194.23583984375\n",
      "\n",
      "Batch Loss: 1114.5032958984375\n",
      "\n",
      "Batch Loss: 1214.509765625\n",
      "\n",
      "Batch Loss: 1235.5045166015625\n",
      "\n",
      "Batch Loss: 1394.274169921875\n",
      "\n",
      "Batch Loss: 1375.0364990234375\n",
      "\n",
      "Batch Loss: 1216.12548828125\n",
      "\n",
      "Batch Loss: 1232.1865234375\n",
      "\n",
      "Batch Loss: 1305.0426025390625\n",
      "\n",
      "Batch Loss: 1103.4761962890625\n",
      "\n",
      "Batch Loss: 1072.595947265625\n",
      "\n",
      "Batch Loss: 1162.0074462890625\n",
      "\n",
      "Batch Loss: 1074.5001220703125\n",
      "\n",
      "Batch Loss: 1170.4609375\n",
      "\n",
      "Batch Loss: 1196.47705078125\n",
      "\n",
      "Batch Loss: 1125.4705810546875\n",
      "\n",
      "Batch Loss: 1126.6959228515625\n",
      "\n",
      "Batch Loss: 1148.5753173828125\n",
      "\n",
      "Batch Loss: 1153.6361083984375\n",
      "\n",
      "Batch Loss: 1155.849853515625\n",
      "\n",
      "Batch Loss: 1200.6448974609375\n",
      "\n",
      "Batch Loss: 1052.6798095703125\n",
      "\n",
      "Batch Loss: 1091.150390625\n",
      "\n",
      "Batch Loss: 1275.7427978515625\n",
      "\n",
      "Batch Loss: 1092.4212646484375\n",
      "\n",
      "Batch Loss: 1115.0611572265625\n",
      "\n",
      "Batch Loss: 1113.3328857421875\n",
      "\n",
      "Batch Loss: 1123.6343994140625\n",
      "\n",
      "Batch Loss: 1237.509765625\n",
      "\n",
      "Batch Loss: 1042.5902099609375\n",
      "\n",
      "Batch Loss: 1067.0150146484375\n",
      "\n",
      "Batch Loss: 1215.158935546875\n",
      "\n",
      "Batch Loss: 1240.724853515625\n",
      "\n",
      "Batch Loss: 1002.2295532226562\n",
      "\n",
      "Batch Loss: 1232.3828125\n",
      "\n",
      "Batch Loss: 1177.8486328125\n",
      "\n",
      "Batch Loss: 1089.14013671875\n",
      "\n",
      "Batch Loss: 1042.1290283203125\n",
      "\n",
      "Batch Loss: 1174.4296875\n",
      "\n",
      "Batch Loss: 1038.5341796875\n",
      "\n",
      "Batch Loss: 1003.0948486328125\n",
      "\n",
      "Batch Loss: 1151.880859375\n",
      "\n",
      "Batch Loss: 1080.730712890625\n",
      "\n",
      "Batch Loss: 1073.26123046875\n",
      "\n",
      "Batch Loss: 878.307861328125\n",
      "\n",
      "Batch Loss: 1136.1700439453125\n",
      "\n",
      "Batch Loss: 961.2613525390625\n",
      "\n",
      "Batch Loss: 1103.984619140625\n",
      "\n",
      "Batch Loss: 1055.5262451171875\n",
      "\n",
      "Batch Loss: 985.712158203125\n",
      "\n",
      "Batch Loss: 987.270263671875\n",
      "\n",
      "Batch Loss: 1028.684326171875\n",
      "\n",
      "Batch Loss: 1019.8685913085938\n",
      "\n",
      "Batch Loss: 1101.974365234375\n",
      "\n",
      "Batch Loss: 923.8335571289062\n",
      "\n",
      "Batch Loss: 1018.2578735351562\n",
      "\n",
      "Batch Loss: 1055.2327880859375\n",
      "\n",
      "Batch Loss: 979.9013061523438\n",
      "\n",
      "Batch Loss: 1128.3583984375\n",
      "\n",
      "Batch Loss: 1101.252197265625\n",
      "\n",
      "Batch Loss: 1161.2247314453125\n",
      "\n",
      "Batch Loss: 1116.091552734375\n",
      "\n",
      "Batch Loss: 1099.451416015625\n",
      "\n",
      "Batch Loss: 905.8368530273438\n",
      "\n",
      "Batch Loss: 880.6025390625\n",
      "\n",
      "Batch Loss: 1180.1689453125\n",
      "\n",
      "Batch Loss: 989.1505737304688\n",
      "\n",
      "Batch Loss: 985.7289428710938\n",
      "\n",
      "Batch Loss: 1045.3433837890625\n",
      "\n",
      "Batch Loss: 1108.1494140625\n",
      "\n",
      "Batch Loss: 1253.941162109375\n",
      "\n",
      "Batch Loss: 1096.569091796875\n",
      "\n",
      "Batch Loss: 893.5352172851562\n",
      "\n",
      "Batch Loss: 1161.7479248046875\n",
      "\n",
      "Batch Loss: 1139.7281494140625\n",
      "\n",
      "Batch Loss: 945.1558837890625\n",
      "\n",
      "Batch Loss: 1099.4268798828125\n",
      "\n",
      "Batch Loss: 921.4117431640625\n",
      "\n",
      "Batch Loss: 1033.80908203125\n",
      "\n",
      "Batch Loss: 1035.5909423828125\n",
      "\n",
      "Batch Loss: 1082.095947265625\n",
      "\n",
      "Batch Loss: 972.3274536132812\n",
      "\n",
      "Batch Loss: 1050.2093505859375\n",
      "\n",
      "Batch Loss: 1059.653076171875\n",
      "\n",
      "Batch Loss: 1039.3046875\n",
      "\n",
      "Batch Loss: 983.60205078125\n",
      "\n",
      "Batch Loss: 921.745849609375\n",
      "\n",
      "Batch Loss: 800.3894653320312\n",
      "\n",
      "Batch Loss: 955.1881103515625\n",
      "\n",
      "Batch Loss: 953.194580078125\n",
      "\n",
      "Batch Loss: 1002.4327392578125\n",
      "\n",
      "Batch Loss: 896.8411865234375\n",
      "\n",
      "Batch Loss: 970.7036743164062\n",
      "\n",
      "Batch Loss: 938.6761474609375\n",
      "\n",
      "Batch Loss: 971.847900390625\n",
      "\n",
      "Batch Loss: 997.01171875\n",
      "\n",
      "Batch Loss: 891.7871704101562\n",
      "\n",
      "Batch Loss: 1206.5439453125\n",
      "\n",
      "Batch Loss: 925.0390625\n",
      "\n",
      "Batch Loss: 1143.316650390625\n",
      "\n",
      "Batch Loss: 1037.780517578125\n",
      "\n",
      "Batch Loss: 813.0196533203125\n",
      "\n",
      "Batch Loss: 838.2149047851562\n",
      "\n",
      "Batch Loss: 981.080322265625\n",
      "\n",
      "Batch Loss: 974.8323364257812\n",
      "\n",
      "Batch Loss: 1067.187744140625\n",
      "\n",
      "Batch Loss: 856.1522827148438\n",
      "\n",
      "Batch Loss: 991.74853515625\n",
      "\n",
      "Batch Loss: 1002.0271606445312\n",
      "\n",
      "Batch Loss: 1261.24560546875\n",
      "\n",
      "Batch Loss: 977.9818725585938\n",
      "\n",
      "Batch Loss: 821.2846069335938\n",
      "\n",
      "Batch Loss: 953.7832641601562\n",
      "\n",
      "Batch Loss: 834.168701171875\n",
      "\n",
      "Batch Loss: 461.2048645019531\n",
      "\n",
      "Epoch 3, Average Loss: 1548.025341219115\n",
      "\n",
      "Batch Loss: 1057.26806640625\n",
      "\n",
      "Batch Loss: 960.2041625976562\n",
      "\n",
      "Batch Loss: 1025.280029296875\n",
      "\n",
      "Batch Loss: 1009.0804443359375\n",
      "\n",
      "Batch Loss: 959.8300170898438\n",
      "\n",
      "Batch Loss: 885.879638671875\n",
      "\n",
      "Batch Loss: 1041.2529296875\n",
      "\n",
      "Batch Loss: 1104.8170166015625\n",
      "\n",
      "Batch Loss: 895.7476196289062\n",
      "\n",
      "Batch Loss: 911.3953247070312\n",
      "\n",
      "Batch Loss: 1116.3824462890625\n",
      "\n",
      "Batch Loss: 896.0781860351562\n",
      "\n",
      "Batch Loss: 863.8704833984375\n",
      "\n",
      "Batch Loss: 947.0828857421875\n",
      "\n",
      "Batch Loss: 964.8585205078125\n",
      "\n",
      "Batch Loss: 987.3601684570312\n",
      "\n",
      "Batch Loss: 916.595703125\n",
      "\n",
      "Batch Loss: 834.7711791992188\n",
      "\n",
      "Batch Loss: 894.3344116210938\n",
      "\n",
      "Batch Loss: 851.898681640625\n",
      "\n",
      "Batch Loss: 994.1859130859375\n",
      "\n",
      "Batch Loss: 1043.9986572265625\n",
      "\n",
      "Batch Loss: 927.3218994140625\n",
      "\n",
      "Batch Loss: 814.8795776367188\n",
      "\n",
      "Batch Loss: 860.0701904296875\n",
      "\n",
      "Batch Loss: 784.3106689453125\n",
      "\n",
      "Batch Loss: 1036.29931640625\n",
      "\n",
      "Batch Loss: 849.6682739257812\n",
      "\n",
      "Batch Loss: 926.87158203125\n",
      "\n",
      "Batch Loss: 929.09326171875\n",
      "\n",
      "Batch Loss: 895.3309326171875\n",
      "\n",
      "Batch Loss: 825.8180541992188\n",
      "\n",
      "Batch Loss: 882.7723999023438\n",
      "\n",
      "Batch Loss: 934.3997802734375\n",
      "\n",
      "Batch Loss: 949.2857055664062\n",
      "\n",
      "Batch Loss: 919.91748046875\n",
      "\n",
      "Batch Loss: 915.4208984375\n",
      "\n",
      "Batch Loss: 899.0588989257812\n",
      "\n",
      "Batch Loss: 954.48291015625\n",
      "\n",
      "Batch Loss: 763.0008544921875\n",
      "\n",
      "Batch Loss: 812.8623657226562\n",
      "\n",
      "Batch Loss: 935.61279296875\n",
      "\n",
      "Batch Loss: 957.52392578125\n",
      "\n",
      "Batch Loss: 791.764404296875\n",
      "\n",
      "Batch Loss: 734.4471435546875\n",
      "\n",
      "Batch Loss: 839.8926391601562\n",
      "\n",
      "Batch Loss: 901.0410766601562\n",
      "\n",
      "Batch Loss: 735.193115234375\n",
      "\n",
      "Batch Loss: 830.81298828125\n",
      "\n",
      "Batch Loss: 776.9741821289062\n",
      "\n",
      "Batch Loss: 985.6145629882812\n",
      "\n",
      "Batch Loss: 972.7343139648438\n",
      "\n",
      "Batch Loss: 898.7659912109375\n",
      "\n",
      "Batch Loss: 896.893310546875\n",
      "\n",
      "Batch Loss: 828.946533203125\n",
      "\n",
      "Batch Loss: 783.4581909179688\n",
      "\n",
      "Batch Loss: 782.0016479492188\n",
      "\n",
      "Batch Loss: 842.0006103515625\n",
      "\n",
      "Batch Loss: 809.2429809570312\n",
      "\n",
      "Batch Loss: 902.6686401367188\n",
      "\n",
      "Batch Loss: 809.3046264648438\n",
      "\n",
      "Batch Loss: 1072.8021240234375\n",
      "\n",
      "Batch Loss: 856.4688720703125\n",
      "\n",
      "Batch Loss: 901.0333862304688\n",
      "\n",
      "Batch Loss: 713.23876953125\n",
      "\n",
      "Batch Loss: 724.6499633789062\n",
      "\n",
      "Batch Loss: 801.8255615234375\n",
      "\n",
      "Batch Loss: 828.6354370117188\n",
      "\n",
      "Batch Loss: 763.59130859375\n",
      "\n",
      "Batch Loss: 833.8126220703125\n",
      "\n",
      "Batch Loss: 965.205810546875\n",
      "\n",
      "Batch Loss: 792.881591796875\n",
      "\n",
      "Batch Loss: 875.4278564453125\n",
      "\n",
      "Batch Loss: 847.5095825195312\n",
      "\n",
      "Batch Loss: 887.4515991210938\n",
      "\n",
      "Batch Loss: 862.5887451171875\n",
      "\n",
      "Batch Loss: 850.0130004882812\n",
      "\n",
      "Batch Loss: 793.504150390625\n",
      "\n",
      "Batch Loss: 830.7326049804688\n",
      "\n",
      "Batch Loss: 854.2894897460938\n",
      "\n",
      "Batch Loss: 699.0850830078125\n",
      "\n",
      "Batch Loss: 723.2608642578125\n",
      "\n",
      "Batch Loss: 791.033203125\n",
      "\n",
      "Batch Loss: 791.3458251953125\n",
      "\n",
      "Batch Loss: 817.2191162109375\n",
      "\n",
      "Batch Loss: 813.4352416992188\n",
      "\n",
      "Batch Loss: 830.5899658203125\n",
      "\n",
      "Batch Loss: 630.9005737304688\n",
      "\n",
      "Batch Loss: 672.06494140625\n",
      "\n",
      "Batch Loss: 769.9137573242188\n",
      "\n",
      "Batch Loss: 879.6914672851562\n",
      "\n",
      "Batch Loss: 747.1871948242188\n",
      "\n",
      "Batch Loss: 711.6775512695312\n",
      "\n",
      "Batch Loss: 714.426025390625\n",
      "\n",
      "Batch Loss: 715.137451171875\n",
      "\n",
      "Batch Loss: 881.8922119140625\n",
      "\n",
      "Batch Loss: 873.9019775390625\n",
      "\n",
      "Batch Loss: 801.5745849609375\n",
      "\n",
      "Batch Loss: 774.611328125\n",
      "\n",
      "Batch Loss: 811.6639404296875\n",
      "\n",
      "Batch Loss: 789.1554565429688\n",
      "\n",
      "Batch Loss: 775.289306640625\n",
      "\n",
      "Batch Loss: 724.71875\n",
      "\n",
      "Batch Loss: 761.5400390625\n",
      "\n",
      "Batch Loss: 730.7510986328125\n",
      "\n",
      "Batch Loss: 716.3206787109375\n",
      "\n",
      "Batch Loss: 780.7257690429688\n",
      "\n",
      "Batch Loss: 713.9749145507812\n",
      "\n",
      "Batch Loss: 931.6123657226562\n",
      "\n",
      "Batch Loss: 708.6116333007812\n",
      "\n",
      "Batch Loss: 786.830322265625\n",
      "\n",
      "Batch Loss: 748.396484375\n",
      "\n",
      "Batch Loss: 894.3162231445312\n",
      "\n",
      "Batch Loss: 814.4403686523438\n",
      "\n",
      "Batch Loss: 835.7218017578125\n",
      "\n",
      "Batch Loss: 711.8685302734375\n",
      "\n",
      "Batch Loss: 854.2697143554688\n",
      "\n",
      "Batch Loss: 777.8644409179688\n",
      "\n",
      "Batch Loss: 691.38818359375\n",
      "\n",
      "Batch Loss: 736.4794921875\n",
      "\n",
      "Batch Loss: 821.0656127929688\n",
      "\n",
      "Batch Loss: 787.697509765625\n",
      "\n",
      "Batch Loss: 651.8282470703125\n",
      "\n",
      "Batch Loss: 762.3970336914062\n",
      "\n",
      "Batch Loss: 741.12646484375\n",
      "\n",
      "Batch Loss: 835.818603515625\n",
      "\n",
      "Batch Loss: 648.3998413085938\n",
      "\n",
      "Batch Loss: 648.481201171875\n",
      "\n",
      "Batch Loss: 639.3515625\n",
      "\n",
      "Batch Loss: 721.189453125\n",
      "\n",
      "Batch Loss: 640.410888671875\n",
      "\n",
      "Batch Loss: 665.073974609375\n",
      "\n",
      "Batch Loss: 672.7708740234375\n",
      "\n",
      "Batch Loss: 716.2075805664062\n",
      "\n",
      "Batch Loss: 689.6954345703125\n",
      "\n",
      "Batch Loss: 700.6808471679688\n",
      "\n",
      "Batch Loss: 751.2035522460938\n",
      "\n",
      "Batch Loss: 856.0315551757812\n",
      "\n",
      "Batch Loss: 862.2460327148438\n",
      "\n",
      "Batch Loss: 705.488525390625\n",
      "\n",
      "Batch Loss: 664.2831420898438\n",
      "\n",
      "Batch Loss: 792.2059326171875\n",
      "\n",
      "Batch Loss: 568.5513305664062\n",
      "\n",
      "Batch Loss: 716.5321044921875\n",
      "\n",
      "Batch Loss: 746.20849609375\n",
      "\n",
      "Batch Loss: 562.3121948242188\n",
      "\n",
      "Batch Loss: 882.2412109375\n",
      "\n",
      "Batch Loss: 718.438232421875\n",
      "\n",
      "Batch Loss: 689.3185424804688\n",
      "\n",
      "Batch Loss: 762.0510864257812\n",
      "\n",
      "Batch Loss: 753.80322265625\n",
      "\n",
      "Batch Loss: 869.3331298828125\n",
      "\n",
      "Batch Loss: 680.3414916992188\n",
      "\n",
      "Batch Loss: 753.8140869140625\n",
      "\n",
      "Batch Loss: 709.5187377929688\n",
      "\n",
      "Batch Loss: 694.4212036132812\n",
      "\n",
      "Batch Loss: 808.6686401367188\n",
      "\n",
      "Batch Loss: 626.392578125\n",
      "\n",
      "Batch Loss: 660.0985717773438\n",
      "\n",
      "Batch Loss: 704.5010986328125\n",
      "\n",
      "Batch Loss: 736.1326293945312\n",
      "\n",
      "Batch Loss: 584.8217163085938\n",
      "\n",
      "Batch Loss: 818.7597045898438\n",
      "\n",
      "Batch Loss: 643.008056640625\n",
      "\n",
      "Batch Loss: 659.0723876953125\n",
      "\n",
      "Batch Loss: 722.1987915039062\n",
      "\n",
      "Batch Loss: 859.2722778320312\n",
      "\n",
      "Batch Loss: 782.6697998046875\n",
      "\n",
      "Batch Loss: 724.2445678710938\n",
      "\n",
      "Batch Loss: 691.751708984375\n",
      "\n",
      "Batch Loss: 653.3340454101562\n",
      "\n",
      "Batch Loss: 711.9735107421875\n",
      "\n",
      "Batch Loss: 624.4185180664062\n",
      "\n",
      "Batch Loss: 721.9376220703125\n",
      "\n",
      "Batch Loss: 752.8184204101562\n",
      "\n",
      "Batch Loss: 794.3896484375\n",
      "\n",
      "Batch Loss: 657.943359375\n",
      "\n",
      "Batch Loss: 607.8077392578125\n",
      "\n",
      "Batch Loss: 627.7947387695312\n",
      "\n",
      "Batch Loss: 579.4143676757812\n",
      "\n",
      "Batch Loss: 710.5706176757812\n",
      "\n",
      "Batch Loss: 636.009521484375\n",
      "\n",
      "Batch Loss: 768.2390747070312\n",
      "\n",
      "Batch Loss: 640.6210327148438\n",
      "\n",
      "Batch Loss: 534.9465942382812\n",
      "\n",
      "Batch Loss: 722.9909057617188\n",
      "\n",
      "Batch Loss: 627.1566772460938\n",
      "\n",
      "Batch Loss: 656.8969116210938\n",
      "\n",
      "Batch Loss: 632.577392578125\n",
      "\n",
      "Batch Loss: 681.5452880859375\n",
      "\n",
      "Batch Loss: 539.1024780273438\n",
      "\n",
      "Batch Loss: 607.683837890625\n",
      "\n",
      "Batch Loss: 583.5252685546875\n",
      "\n",
      "Batch Loss: 666.7223510742188\n",
      "\n",
      "Batch Loss: 643.37646484375\n",
      "\n",
      "Batch Loss: 649.6190795898438\n",
      "\n",
      "Batch Loss: 650.2459106445312\n",
      "\n",
      "Batch Loss: 736.5321655273438\n",
      "\n",
      "Batch Loss: 917.3853149414062\n",
      "\n",
      "Batch Loss: 616.1618041992188\n",
      "\n",
      "Batch Loss: 684.3638916015625\n",
      "\n",
      "Batch Loss: 676.97998046875\n",
      "\n",
      "Batch Loss: 596.9124145507812\n",
      "\n",
      "Batch Loss: 569.56689453125\n",
      "\n",
      "Batch Loss: 588.0013427734375\n",
      "\n",
      "Batch Loss: 644.2618408203125\n",
      "\n",
      "Batch Loss: 581.2608032226562\n",
      "\n",
      "Batch Loss: 670.2943115234375\n",
      "\n",
      "Batch Loss: 718.3780517578125\n",
      "\n",
      "Batch Loss: 672.1925048828125\n",
      "\n",
      "Batch Loss: 581.9506225585938\n",
      "\n",
      "Batch Loss: 676.4877319335938\n",
      "\n",
      "Batch Loss: 714.0743408203125\n",
      "\n",
      "Batch Loss: 641.0153198242188\n",
      "\n",
      "Batch Loss: 661.6924438476562\n",
      "\n",
      "Batch Loss: 683.3546142578125\n",
      "\n",
      "Batch Loss: 657.0899047851562\n",
      "\n",
      "Batch Loss: 743.817138671875\n",
      "\n",
      "Batch Loss: 634.546142578125\n",
      "\n",
      "Batch Loss: 548.8921508789062\n",
      "\n",
      "Batch Loss: 581.598388671875\n",
      "\n",
      "Batch Loss: 646.24267578125\n",
      "\n",
      "Batch Loss: 635.3629760742188\n",
      "\n",
      "Batch Loss: 660.59423828125\n",
      "\n",
      "Batch Loss: 623.2739868164062\n",
      "\n",
      "Batch Loss: 715.8364868164062\n",
      "\n",
      "Batch Loss: 754.3026733398438\n",
      "\n",
      "Batch Loss: 557.0433349609375\n",
      "\n",
      "Batch Loss: 586.5899658203125\n",
      "\n",
      "Batch Loss: 506.7851257324219\n",
      "\n",
      "Batch Loss: 585.352294921875\n",
      "\n",
      "Batch Loss: 605.897705078125\n",
      "\n",
      "Batch Loss: 700.6751708984375\n",
      "\n",
      "Batch Loss: 553.8836669921875\n",
      "\n",
      "Batch Loss: 564.04052734375\n",
      "\n",
      "Batch Loss: 720.9310913085938\n",
      "\n",
      "Batch Loss: 523.9108276367188\n",
      "\n",
      "Batch Loss: 542.9791259765625\n",
      "\n",
      "Batch Loss: 533.205078125\n",
      "\n",
      "Batch Loss: 598.095458984375\n",
      "\n",
      "Batch Loss: 623.603271484375\n",
      "\n",
      "Batch Loss: 525.9511108398438\n",
      "\n",
      "Batch Loss: 746.9410400390625\n",
      "\n",
      "Batch Loss: 678.99853515625\n",
      "\n",
      "Batch Loss: 599.4276733398438\n",
      "\n",
      "Batch Loss: 584.9749145507812\n",
      "\n",
      "Batch Loss: 591.9315795898438\n",
      "\n",
      "Batch Loss: 498.6924743652344\n",
      "\n",
      "Batch Loss: 693.3399047851562\n",
      "\n",
      "Batch Loss: 698.337646484375\n",
      "\n",
      "Batch Loss: 488.1279602050781\n",
      "\n",
      "Batch Loss: 687.4281005859375\n",
      "\n",
      "Batch Loss: 542.5242309570312\n",
      "\n",
      "Batch Loss: 816.2174682617188\n",
      "\n",
      "Batch Loss: 614.2881469726562\n",
      "\n",
      "Batch Loss: 570.623779296875\n",
      "\n",
      "Batch Loss: 621.9614868164062\n",
      "\n",
      "Batch Loss: 747.7645263671875\n",
      "\n",
      "Batch Loss: 486.432861328125\n",
      "\n",
      "Batch Loss: 754.6805419921875\n",
      "\n",
      "Batch Loss: 762.9554443359375\n",
      "\n",
      "Batch Loss: 551.9400634765625\n",
      "\n",
      "Batch Loss: 675.54150390625\n",
      "\n",
      "Batch Loss: 538.9569091796875\n",
      "\n",
      "Batch Loss: 558.0962524414062\n",
      "\n",
      "Batch Loss: 714.2343139648438\n",
      "\n",
      "Batch Loss: 577.6036987304688\n",
      "\n",
      "Batch Loss: 651.5576171875\n",
      "\n",
      "Batch Loss: 585.7001342773438\n",
      "\n",
      "Batch Loss: 535.5780639648438\n",
      "\n",
      "Batch Loss: 613.2029418945312\n",
      "\n",
      "Batch Loss: 430.2998046875\n",
      "\n",
      "Batch Loss: 590.0184326171875\n",
      "\n",
      "Batch Loss: 516.4778442382812\n",
      "\n",
      "Batch Loss: 626.3780517578125\n",
      "\n",
      "Batch Loss: 462.70977783203125\n",
      "\n",
      "Batch Loss: 584.354248046875\n",
      "\n",
      "Batch Loss: 639.2570190429688\n",
      "\n",
      "Batch Loss: 603.4734497070312\n",
      "\n",
      "Batch Loss: 533.1879272460938\n",
      "\n",
      "Batch Loss: 533.5841064453125\n",
      "\n",
      "Batch Loss: 586.2252807617188\n",
      "\n",
      "Batch Loss: 550.8330688476562\n",
      "\n",
      "Batch Loss: 621.6898803710938\n",
      "\n",
      "Batch Loss: 826.4844360351562\n",
      "\n",
      "Batch Loss: 532.5245361328125\n",
      "\n",
      "Batch Loss: 558.24072265625\n",
      "\n",
      "Batch Loss: 516.659423828125\n",
      "\n",
      "Batch Loss: 519.7930297851562\n",
      "\n",
      "Batch Loss: 612.3321533203125\n",
      "\n",
      "Batch Loss: 638.6878662109375\n",
      "\n",
      "Batch Loss: 498.2104187011719\n",
      "\n",
      "Batch Loss: 692.3491821289062\n",
      "\n",
      "Batch Loss: 515.8106079101562\n",
      "\n",
      "Batch Loss: 515.1282958984375\n",
      "\n",
      "Batch Loss: 542.8103637695312\n",
      "\n",
      "Batch Loss: 566.001708984375\n",
      "\n",
      "Batch Loss: 591.696044921875\n",
      "\n",
      "Batch Loss: 593.17138671875\n",
      "\n",
      "Batch Loss: 502.8513488769531\n",
      "\n",
      "Batch Loss: 680.6227416992188\n",
      "\n",
      "Batch Loss: 557.8313598632812\n",
      "\n",
      "Batch Loss: 595.54296875\n",
      "\n",
      "Batch Loss: 524.811767578125\n",
      "\n",
      "Batch Loss: 589.9038696289062\n",
      "\n",
      "Batch Loss: 581.7716674804688\n",
      "\n",
      "Batch Loss: 586.0100708007812\n",
      "\n",
      "Batch Loss: 539.8688354492188\n",
      "\n",
      "Batch Loss: 561.4678955078125\n",
      "\n",
      "Batch Loss: 506.55877685546875\n",
      "\n",
      "Batch Loss: 619.9175415039062\n",
      "\n",
      "Batch Loss: 469.4073791503906\n",
      "\n",
      "Batch Loss: 524.94091796875\n",
      "\n",
      "Batch Loss: 486.496337890625\n",
      "\n",
      "Batch Loss: 502.6154479980469\n",
      "\n",
      "Batch Loss: 607.3724365234375\n",
      "\n",
      "Batch Loss: 500.658203125\n",
      "\n",
      "Batch Loss: 489.9625549316406\n",
      "\n",
      "Batch Loss: 522.3318481445312\n",
      "\n",
      "Batch Loss: 544.9614868164062\n",
      "\n",
      "Batch Loss: 635.63525390625\n",
      "\n",
      "Batch Loss: 528.0438232421875\n",
      "\n",
      "Batch Loss: 493.1215515136719\n",
      "\n",
      "Batch Loss: 462.0315246582031\n",
      "\n",
      "Batch Loss: 554.7003784179688\n",
      "\n",
      "Batch Loss: 592.1844482421875\n",
      "\n",
      "Batch Loss: 483.2366027832031\n",
      "\n",
      "Batch Loss: 566.24951171875\n",
      "\n",
      "Batch Loss: 521.6823120117188\n",
      "\n",
      "Batch Loss: 484.2699890136719\n",
      "\n",
      "Batch Loss: 630.5764770507812\n",
      "\n",
      "Batch Loss: 493.9350280761719\n",
      "\n",
      "Batch Loss: 610.7832641601562\n",
      "\n",
      "Batch Loss: 529.6771240234375\n",
      "\n",
      "Batch Loss: 459.9787292480469\n",
      "\n",
      "Batch Loss: 515.84912109375\n",
      "\n",
      "Batch Loss: 620.0924072265625\n",
      "\n",
      "Batch Loss: 463.0767822265625\n",
      "\n",
      "Batch Loss: 540.5015869140625\n",
      "\n",
      "Batch Loss: 540.036376953125\n",
      "\n",
      "Batch Loss: 482.60919189453125\n",
      "\n",
      "Batch Loss: 533.8435668945312\n",
      "\n",
      "Batch Loss: 609.2526245117188\n",
      "\n",
      "Batch Loss: 425.7738342285156\n",
      "\n",
      "Batch Loss: 595.6468505859375\n",
      "\n",
      "Batch Loss: 568.7921142578125\n",
      "\n",
      "Batch Loss: 513.724365234375\n",
      "\n",
      "Batch Loss: 540.5157470703125\n",
      "\n",
      "Batch Loss: 479.232421875\n",
      "\n",
      "Batch Loss: 459.3620300292969\n",
      "\n",
      "Batch Loss: 623.5909423828125\n",
      "\n",
      "Batch Loss: 529.3170166015625\n",
      "\n",
      "Batch Loss: 645.185302734375\n",
      "\n",
      "Batch Loss: 614.8959350585938\n",
      "\n",
      "Batch Loss: 525.6082153320312\n",
      "\n",
      "Batch Loss: 603.9500122070312\n",
      "\n",
      "Batch Loss: 540.3787231445312\n",
      "\n",
      "Batch Loss: 522.8541259765625\n",
      "\n",
      "Batch Loss: 546.3350219726562\n",
      "\n",
      "Batch Loss: 441.5957336425781\n",
      "\n",
      "Batch Loss: 586.57666015625\n",
      "\n",
      "Batch Loss: 446.686279296875\n",
      "\n",
      "Batch Loss: 421.76397705078125\n",
      "\n",
      "Batch Loss: 565.9529418945312\n",
      "\n",
      "Batch Loss: 512.2919311523438\n",
      "\n",
      "Batch Loss: 506.40753173828125\n",
      "\n",
      "Batch Loss: 517.6334228515625\n",
      "\n",
      "Batch Loss: 569.8475952148438\n",
      "\n",
      "Batch Loss: 559.9826049804688\n",
      "\n",
      "Batch Loss: 469.2010498046875\n",
      "\n",
      "Batch Loss: 507.81451416015625\n",
      "\n",
      "Batch Loss: 525.0834350585938\n",
      "\n",
      "Batch Loss: 500.2465515136719\n",
      "\n",
      "Batch Loss: 459.3265686035156\n",
      "\n",
      "Batch Loss: 534.3429565429688\n",
      "\n",
      "Batch Loss: 516.1168212890625\n",
      "\n",
      "Batch Loss: 479.02655029296875\n",
      "\n",
      "Batch Loss: 536.1619262695312\n",
      "\n",
      "Batch Loss: 526.217041015625\n",
      "\n",
      "Batch Loss: 524.5380859375\n",
      "\n",
      "Batch Loss: 501.9853820800781\n",
      "\n",
      "Batch Loss: 478.13458251953125\n",
      "\n",
      "Batch Loss: 536.1432495117188\n",
      "\n",
      "Batch Loss: 446.6892395019531\n",
      "\n",
      "Batch Loss: 430.9673767089844\n",
      "\n",
      "Batch Loss: 520.9608154296875\n",
      "\n",
      "Batch Loss: 487.90826416015625\n",
      "\n",
      "Batch Loss: 467.14031982421875\n",
      "\n",
      "Batch Loss: 646.82763671875\n",
      "\n",
      "Batch Loss: 503.4815673828125\n",
      "\n",
      "Batch Loss: 457.32073974609375\n",
      "\n",
      "Batch Loss: 456.65447998046875\n",
      "\n",
      "Batch Loss: 475.0550842285156\n",
      "\n",
      "Batch Loss: 417.07281494140625\n",
      "\n",
      "Batch Loss: 545.0906372070312\n",
      "\n",
      "Batch Loss: 548.71728515625\n",
      "\n",
      "Batch Loss: 473.94622802734375\n",
      "\n",
      "Batch Loss: 519.74365234375\n",
      "\n",
      "Batch Loss: 398.5084533691406\n",
      "\n",
      "Batch Loss: 448.0450439453125\n",
      "\n",
      "Batch Loss: 644.81640625\n",
      "\n",
      "Batch Loss: 496.4971923828125\n",
      "\n",
      "Batch Loss: 410.6505126953125\n",
      "\n",
      "Batch Loss: 410.8025207519531\n",
      "\n",
      "Batch Loss: 474.0083312988281\n",
      "\n",
      "Batch Loss: 620.5594482421875\n",
      "\n",
      "Batch Loss: 454.09918212890625\n",
      "\n",
      "Batch Loss: 484.7991638183594\n",
      "\n",
      "Batch Loss: 477.3226013183594\n",
      "\n",
      "Batch Loss: 528.967041015625\n",
      "\n",
      "Batch Loss: 495.14727783203125\n",
      "\n",
      "Batch Loss: 213.08831787109375\n",
      "\n",
      "Epoch 4, Average Loss: 673.1613859898836\n",
      "\n",
      "Batch Loss: 458.1585388183594\n",
      "\n",
      "Batch Loss: 424.077392578125\n",
      "\n",
      "Batch Loss: 444.9711608886719\n",
      "\n",
      "Batch Loss: 417.3591003417969\n",
      "\n",
      "Batch Loss: 469.5540466308594\n",
      "\n",
      "Batch Loss: 509.502685546875\n",
      "\n",
      "Batch Loss: 555.0521240234375\n",
      "\n",
      "Batch Loss: 436.773681640625\n",
      "\n",
      "Batch Loss: 511.2703857421875\n",
      "\n",
      "Batch Loss: 460.36444091796875\n",
      "\n",
      "Batch Loss: 474.4799499511719\n",
      "\n",
      "Batch Loss: 479.8081359863281\n",
      "\n",
      "Batch Loss: 463.6396789550781\n",
      "\n",
      "Batch Loss: 423.6941223144531\n",
      "\n",
      "Batch Loss: 477.85174560546875\n",
      "\n",
      "Batch Loss: 519.7554321289062\n",
      "\n",
      "Batch Loss: 594.5294799804688\n",
      "\n",
      "Batch Loss: 472.71160888671875\n",
      "\n",
      "Batch Loss: 420.55523681640625\n",
      "\n",
      "Batch Loss: 506.8094482421875\n",
      "\n",
      "Batch Loss: 427.822998046875\n",
      "\n",
      "Batch Loss: 423.73504638671875\n",
      "\n",
      "Batch Loss: 443.140380859375\n",
      "\n",
      "Batch Loss: 459.5749816894531\n",
      "\n",
      "Batch Loss: 422.3038330078125\n",
      "\n",
      "Batch Loss: 525.262939453125\n",
      "\n",
      "Batch Loss: 511.4626770019531\n",
      "\n",
      "Batch Loss: 410.7041015625\n",
      "\n",
      "Batch Loss: 393.91796875\n",
      "\n",
      "Batch Loss: 434.509521484375\n",
      "\n",
      "Batch Loss: 424.4957275390625\n",
      "\n",
      "Batch Loss: 445.2939758300781\n",
      "\n",
      "Batch Loss: 508.7131652832031\n",
      "\n",
      "Batch Loss: 484.278076171875\n",
      "\n",
      "Batch Loss: 624.3067626953125\n",
      "\n",
      "Batch Loss: 463.1109313964844\n",
      "\n",
      "Batch Loss: 370.908935546875\n",
      "\n",
      "Batch Loss: 465.72259521484375\n",
      "\n",
      "Batch Loss: 466.2551574707031\n",
      "\n",
      "Batch Loss: 422.3569030761719\n",
      "\n",
      "Batch Loss: 425.2840881347656\n",
      "\n",
      "Batch Loss: 490.3116455078125\n",
      "\n",
      "Batch Loss: 503.1269226074219\n",
      "\n",
      "Batch Loss: 406.6590881347656\n",
      "\n",
      "Batch Loss: 428.63250732421875\n",
      "\n",
      "Batch Loss: 431.25811767578125\n",
      "\n",
      "Batch Loss: 529.7916870117188\n",
      "\n",
      "Batch Loss: 380.0352783203125\n",
      "\n",
      "Batch Loss: 425.2768859863281\n",
      "\n",
      "Batch Loss: 440.5542907714844\n",
      "\n",
      "Batch Loss: 404.8995056152344\n",
      "\n",
      "Batch Loss: 410.3270568847656\n",
      "\n",
      "Batch Loss: 402.94183349609375\n",
      "\n",
      "Batch Loss: 499.78082275390625\n",
      "\n",
      "Batch Loss: 516.3622436523438\n",
      "\n",
      "Batch Loss: 399.90777587890625\n",
      "\n",
      "Batch Loss: 479.08746337890625\n",
      "\n",
      "Batch Loss: 347.1840515136719\n",
      "\n",
      "Batch Loss: 519.8922729492188\n",
      "\n",
      "Batch Loss: 409.9416809082031\n",
      "\n",
      "Batch Loss: 477.67974853515625\n",
      "\n",
      "Batch Loss: 446.71502685546875\n",
      "\n",
      "Batch Loss: 510.44158935546875\n",
      "\n",
      "Batch Loss: 489.7197570800781\n",
      "\n",
      "Batch Loss: 392.5157470703125\n",
      "\n",
      "Batch Loss: 409.2043762207031\n",
      "\n",
      "Batch Loss: 368.8033447265625\n",
      "\n",
      "Batch Loss: 397.48626708984375\n",
      "\n",
      "Batch Loss: 581.7091064453125\n",
      "\n",
      "Batch Loss: 437.89276123046875\n",
      "\n",
      "Batch Loss: 441.69622802734375\n",
      "\n",
      "Batch Loss: 379.6226501464844\n",
      "\n",
      "Batch Loss: 455.8593444824219\n",
      "\n",
      "Batch Loss: 341.2972106933594\n",
      "\n",
      "Batch Loss: 395.6277770996094\n",
      "\n",
      "Batch Loss: 415.7815856933594\n",
      "\n",
      "Batch Loss: 451.36767578125\n",
      "\n",
      "Batch Loss: 380.9803161621094\n",
      "\n",
      "Batch Loss: 421.7066650390625\n",
      "\n",
      "Batch Loss: 476.23907470703125\n",
      "\n",
      "Batch Loss: 301.2444152832031\n",
      "\n",
      "Batch Loss: 421.49798583984375\n",
      "\n",
      "Batch Loss: 378.3908996582031\n",
      "\n",
      "Batch Loss: 455.96832275390625\n",
      "\n",
      "Batch Loss: 448.0141906738281\n",
      "\n",
      "Batch Loss: 426.9286804199219\n",
      "\n",
      "Batch Loss: 381.09796142578125\n",
      "\n",
      "Batch Loss: 424.9305114746094\n",
      "\n",
      "Batch Loss: 485.2723388671875\n",
      "\n",
      "Batch Loss: 429.97100830078125\n",
      "\n",
      "Batch Loss: 409.7585144042969\n",
      "\n",
      "Batch Loss: 340.7907409667969\n",
      "\n",
      "Batch Loss: 371.862060546875\n",
      "\n",
      "Batch Loss: 478.7050476074219\n",
      "\n",
      "Batch Loss: 449.9864501953125\n",
      "\n",
      "Batch Loss: 374.3780822753906\n",
      "\n",
      "Batch Loss: 439.8387756347656\n",
      "\n",
      "Batch Loss: 371.84759521484375\n",
      "\n",
      "Batch Loss: 451.677734375\n",
      "\n",
      "Batch Loss: 423.374267578125\n",
      "\n",
      "Batch Loss: 374.3772277832031\n",
      "\n",
      "Batch Loss: 444.15087890625\n",
      "\n",
      "Batch Loss: 470.11907958984375\n",
      "\n",
      "Batch Loss: 388.69622802734375\n",
      "\n",
      "Batch Loss: 367.2854919433594\n",
      "\n",
      "Batch Loss: 308.4092102050781\n",
      "\n",
      "Batch Loss: 409.599853515625\n",
      "\n",
      "Batch Loss: 477.06781005859375\n",
      "\n",
      "Batch Loss: 521.198974609375\n",
      "\n",
      "Batch Loss: 359.1502380371094\n",
      "\n",
      "Batch Loss: 479.9367370605469\n",
      "\n",
      "Batch Loss: 428.73272705078125\n",
      "\n",
      "Batch Loss: 405.946044921875\n",
      "\n",
      "Batch Loss: 390.26104736328125\n",
      "\n",
      "Batch Loss: 301.8663330078125\n",
      "\n",
      "Batch Loss: 413.3419494628906\n",
      "\n",
      "Batch Loss: 391.1040344238281\n",
      "\n",
      "Batch Loss: 489.38592529296875\n",
      "\n",
      "Batch Loss: 374.9972839355469\n",
      "\n",
      "Batch Loss: 422.833984375\n",
      "\n",
      "Batch Loss: 368.6590576171875\n",
      "\n",
      "Batch Loss: 384.14984130859375\n",
      "\n",
      "Batch Loss: 377.3129577636719\n",
      "\n",
      "Batch Loss: 348.24200439453125\n",
      "\n",
      "Batch Loss: 394.7602844238281\n",
      "\n",
      "Batch Loss: 417.8699645996094\n",
      "\n",
      "Batch Loss: 366.12603759765625\n",
      "\n",
      "Batch Loss: 331.1010437011719\n",
      "\n",
      "Batch Loss: 347.4847717285156\n",
      "\n",
      "Batch Loss: 439.2259216308594\n",
      "\n",
      "Batch Loss: 390.5968322753906\n",
      "\n",
      "Batch Loss: 421.75177001953125\n",
      "\n",
      "Batch Loss: 368.9896545410156\n",
      "\n",
      "Batch Loss: 380.9945983886719\n",
      "\n",
      "Batch Loss: 347.7326965332031\n",
      "\n",
      "Batch Loss: 519.9821166992188\n",
      "\n",
      "Batch Loss: 447.3567810058594\n",
      "\n",
      "Batch Loss: 351.474609375\n",
      "\n",
      "Batch Loss: 401.3563232421875\n",
      "\n",
      "Batch Loss: 336.70343017578125\n",
      "\n",
      "Batch Loss: 347.7784118652344\n",
      "\n",
      "Batch Loss: 361.5391540527344\n",
      "\n",
      "Batch Loss: 373.3277282714844\n",
      "\n",
      "Batch Loss: 387.94281005859375\n",
      "\n",
      "Batch Loss: 363.7701110839844\n",
      "\n",
      "Batch Loss: 423.2337646484375\n",
      "\n",
      "Batch Loss: 474.2391662597656\n",
      "\n",
      "Batch Loss: 361.3324279785156\n",
      "\n",
      "Batch Loss: 317.0531311035156\n",
      "\n",
      "Batch Loss: 396.87158203125\n",
      "\n",
      "Batch Loss: 356.9937438964844\n",
      "\n",
      "Batch Loss: 333.1836853027344\n",
      "\n",
      "Batch Loss: 426.14776611328125\n",
      "\n",
      "Batch Loss: 381.1888122558594\n",
      "\n",
      "Batch Loss: 372.5739440917969\n",
      "\n",
      "Batch Loss: 409.7991027832031\n",
      "\n",
      "Batch Loss: 314.25799560546875\n",
      "\n",
      "Batch Loss: 367.57427978515625\n",
      "\n",
      "Batch Loss: 311.9677734375\n",
      "\n",
      "Batch Loss: 367.6289367675781\n",
      "\n",
      "Batch Loss: 358.9000244140625\n",
      "\n",
      "Batch Loss: 390.8869323730469\n",
      "\n",
      "Batch Loss: 354.5809020996094\n",
      "\n",
      "Batch Loss: 350.1822204589844\n",
      "\n",
      "Batch Loss: 368.0562438964844\n",
      "\n",
      "Batch Loss: 426.983154296875\n",
      "\n",
      "Batch Loss: 308.8976745605469\n",
      "\n",
      "Batch Loss: 335.3273010253906\n",
      "\n",
      "Batch Loss: 463.7606506347656\n",
      "\n",
      "Batch Loss: 420.7975158691406\n",
      "\n",
      "Batch Loss: 432.1488342285156\n",
      "\n",
      "Batch Loss: 436.26715087890625\n",
      "\n",
      "Batch Loss: 457.36767578125\n",
      "\n",
      "Batch Loss: 417.519775390625\n",
      "\n",
      "Batch Loss: 404.0350036621094\n",
      "\n",
      "Batch Loss: 339.5982666015625\n",
      "\n",
      "Batch Loss: 423.9580993652344\n",
      "\n",
      "Batch Loss: 330.6692810058594\n",
      "\n",
      "Batch Loss: 357.9300537109375\n",
      "\n",
      "Batch Loss: 305.68548583984375\n",
      "\n",
      "Batch Loss: 375.4172668457031\n",
      "\n",
      "Batch Loss: 383.44287109375\n",
      "\n",
      "Batch Loss: 385.7183837890625\n",
      "\n",
      "Batch Loss: 397.3503112792969\n",
      "\n",
      "Batch Loss: 348.37506103515625\n",
      "\n",
      "Batch Loss: 382.5578918457031\n",
      "\n",
      "Batch Loss: 293.9627380371094\n",
      "\n",
      "Batch Loss: 323.4227600097656\n",
      "\n",
      "Batch Loss: 349.4464111328125\n",
      "\n",
      "Batch Loss: 329.7647399902344\n",
      "\n",
      "Batch Loss: 338.8703308105469\n",
      "\n",
      "Batch Loss: 376.5052490234375\n",
      "\n",
      "Batch Loss: 360.3047180175781\n",
      "\n",
      "Batch Loss: 416.50128173828125\n",
      "\n",
      "Batch Loss: 360.35137939453125\n",
      "\n",
      "Batch Loss: 336.1452331542969\n",
      "\n",
      "Batch Loss: 310.72113037109375\n",
      "\n",
      "Batch Loss: 321.8041076660156\n",
      "\n",
      "Batch Loss: 376.1318664550781\n",
      "\n",
      "Batch Loss: 471.8067626953125\n",
      "\n",
      "Batch Loss: 404.3477783203125\n",
      "\n",
      "Batch Loss: 390.56011962890625\n",
      "\n",
      "Batch Loss: 266.251220703125\n",
      "\n",
      "Batch Loss: 303.0897521972656\n",
      "\n",
      "Batch Loss: 349.5550231933594\n",
      "\n",
      "Batch Loss: 336.7938537597656\n",
      "\n",
      "Batch Loss: 329.1938171386719\n",
      "\n",
      "Batch Loss: 339.95654296875\n",
      "\n",
      "Batch Loss: 310.40203857421875\n",
      "\n",
      "Batch Loss: 344.3152770996094\n",
      "\n",
      "Batch Loss: 486.5635986328125\n",
      "\n",
      "Batch Loss: 293.2881164550781\n",
      "\n",
      "Batch Loss: 431.9263610839844\n",
      "\n",
      "Batch Loss: 378.3050842285156\n",
      "\n",
      "Batch Loss: 376.32403564453125\n",
      "\n",
      "Batch Loss: 288.9686584472656\n",
      "\n",
      "Batch Loss: 345.0648193359375\n",
      "\n",
      "Batch Loss: 352.5316467285156\n",
      "\n",
      "Batch Loss: 297.6829528808594\n",
      "\n",
      "Batch Loss: 388.19610595703125\n",
      "\n",
      "Batch Loss: 330.29705810546875\n",
      "\n",
      "Batch Loss: 344.7942199707031\n",
      "\n",
      "Batch Loss: 311.1150207519531\n",
      "\n",
      "Batch Loss: 322.285400390625\n",
      "\n",
      "Batch Loss: 298.3975830078125\n",
      "\n",
      "Batch Loss: 510.70257568359375\n",
      "\n",
      "Batch Loss: 355.7679138183594\n",
      "\n",
      "Batch Loss: 293.2433166503906\n",
      "\n",
      "Batch Loss: 332.85833740234375\n",
      "\n",
      "Batch Loss: 364.829345703125\n",
      "\n",
      "Batch Loss: 387.28582763671875\n",
      "\n",
      "Batch Loss: 392.2334289550781\n",
      "\n",
      "Batch Loss: 277.2693176269531\n",
      "\n",
      "Batch Loss: 305.6562805175781\n",
      "\n",
      "Batch Loss: 278.3370056152344\n",
      "\n",
      "Batch Loss: 315.13079833984375\n",
      "\n",
      "Batch Loss: 278.6871643066406\n",
      "\n",
      "Batch Loss: 289.3575134277344\n",
      "\n",
      "Batch Loss: 299.96807861328125\n",
      "\n",
      "Batch Loss: 331.5341796875\n",
      "\n",
      "Batch Loss: 365.56268310546875\n",
      "\n",
      "Batch Loss: 310.9818115234375\n",
      "\n",
      "Batch Loss: 297.54254150390625\n",
      "\n",
      "Batch Loss: 314.5869445800781\n",
      "\n",
      "Batch Loss: 360.5957336425781\n",
      "\n",
      "Batch Loss: 330.01995849609375\n",
      "\n",
      "Batch Loss: 321.0070495605469\n",
      "\n",
      "Batch Loss: 312.2781066894531\n",
      "\n",
      "Batch Loss: 312.7652587890625\n",
      "\n",
      "Batch Loss: 350.4591979980469\n",
      "\n",
      "Batch Loss: 282.6620178222656\n",
      "\n",
      "Batch Loss: 453.7784118652344\n",
      "\n",
      "Batch Loss: 383.61328125\n",
      "\n",
      "Batch Loss: 397.6882629394531\n",
      "\n",
      "Batch Loss: 280.32275390625\n",
      "\n",
      "Batch Loss: 323.7352600097656\n",
      "\n",
      "Batch Loss: 288.0517883300781\n",
      "\n",
      "Batch Loss: 321.4438171386719\n",
      "\n",
      "Batch Loss: 309.891357421875\n",
      "\n",
      "Batch Loss: 309.8482971191406\n",
      "\n",
      "Batch Loss: 304.4652404785156\n",
      "\n",
      "Batch Loss: 330.79815673828125\n",
      "\n",
      "Batch Loss: 339.4123229980469\n",
      "\n",
      "Batch Loss: 357.34417724609375\n",
      "\n",
      "Batch Loss: 571.8892822265625\n",
      "\n",
      "Batch Loss: 390.35198974609375\n",
      "\n",
      "Batch Loss: 279.0172424316406\n",
      "\n",
      "Batch Loss: 374.0609436035156\n",
      "\n",
      "Batch Loss: 275.1210632324219\n",
      "\n",
      "Batch Loss: 309.2499084472656\n",
      "\n",
      "Batch Loss: 297.7022705078125\n",
      "\n",
      "Batch Loss: 304.7987365722656\n",
      "\n",
      "Batch Loss: 278.3522644042969\n",
      "\n",
      "Batch Loss: 374.5878601074219\n",
      "\n",
      "Batch Loss: 349.81414794921875\n",
      "\n",
      "Batch Loss: 295.2795104980469\n",
      "\n",
      "Batch Loss: 293.0851745605469\n",
      "\n",
      "Batch Loss: 317.8027038574219\n",
      "\n",
      "Batch Loss: 428.032958984375\n",
      "\n",
      "Batch Loss: 261.0746154785156\n",
      "\n",
      "Batch Loss: 346.05657958984375\n",
      "\n",
      "Batch Loss: 319.6278381347656\n",
      "\n",
      "Batch Loss: 257.8822937011719\n",
      "\n",
      "Batch Loss: 308.1064147949219\n",
      "\n",
      "Batch Loss: 385.26141357421875\n",
      "\n",
      "Batch Loss: 286.3274841308594\n",
      "\n",
      "Batch Loss: 372.5477294921875\n",
      "\n",
      "Batch Loss: 378.6263122558594\n",
      "\n",
      "Batch Loss: 259.5731201171875\n",
      "\n",
      "Batch Loss: 379.8213195800781\n",
      "\n",
      "Batch Loss: 281.5927429199219\n",
      "\n",
      "Batch Loss: 232.51776123046875\n",
      "\n",
      "Batch Loss: 281.2077331542969\n",
      "\n",
      "Batch Loss: 306.9489440917969\n",
      "\n",
      "Batch Loss: 348.3724670410156\n",
      "\n",
      "Batch Loss: 347.56304931640625\n",
      "\n",
      "Batch Loss: 294.6156311035156\n",
      "\n",
      "Batch Loss: 275.42626953125\n",
      "\n",
      "Batch Loss: 435.97869873046875\n",
      "\n",
      "Batch Loss: 289.5755920410156\n",
      "\n",
      "Batch Loss: 370.8839416503906\n",
      "\n",
      "Batch Loss: 423.4566955566406\n",
      "\n",
      "Batch Loss: 340.45428466796875\n",
      "\n",
      "Batch Loss: 335.67401123046875\n",
      "\n",
      "Batch Loss: 293.1585998535156\n",
      "\n",
      "Batch Loss: 396.6020812988281\n",
      "\n",
      "Batch Loss: 310.35040283203125\n",
      "\n",
      "Batch Loss: 313.9361572265625\n",
      "\n",
      "Batch Loss: 381.2294006347656\n",
      "\n",
      "Batch Loss: 364.8129577636719\n",
      "\n",
      "Batch Loss: 380.36456298828125\n",
      "\n",
      "Batch Loss: 262.5797119140625\n",
      "\n",
      "Batch Loss: 341.6897277832031\n",
      "\n",
      "Batch Loss: 410.7065734863281\n",
      "\n",
      "Batch Loss: 501.3974914550781\n",
      "\n",
      "Batch Loss: 245.00413513183594\n",
      "\n",
      "Batch Loss: 291.7759704589844\n",
      "\n",
      "Batch Loss: 297.9988098144531\n",
      "\n",
      "Batch Loss: 412.52752685546875\n",
      "\n",
      "Batch Loss: 237.7312469482422\n",
      "\n",
      "Batch Loss: 298.4776611328125\n",
      "\n",
      "Batch Loss: 346.0216369628906\n",
      "\n",
      "Batch Loss: 329.41668701171875\n",
      "\n",
      "Batch Loss: 305.7606201171875\n",
      "\n",
      "Batch Loss: 357.7879333496094\n",
      "\n",
      "Batch Loss: 265.04290771484375\n",
      "\n",
      "Batch Loss: 219.94720458984375\n",
      "\n",
      "Batch Loss: 289.9385986328125\n",
      "\n",
      "Batch Loss: 363.4000244140625\n",
      "\n",
      "Batch Loss: 356.9098815917969\n",
      "\n",
      "Batch Loss: 261.55706787109375\n",
      "\n",
      "Batch Loss: 341.88665771484375\n",
      "\n",
      "Batch Loss: 277.2925109863281\n",
      "\n",
      "Batch Loss: 283.8948669433594\n",
      "\n",
      "Batch Loss: 368.384521484375\n",
      "\n",
      "Batch Loss: 261.9591369628906\n",
      "\n",
      "Batch Loss: 308.0696105957031\n",
      "\n",
      "Batch Loss: 336.1823425292969\n",
      "\n",
      "Batch Loss: 285.59954833984375\n",
      "\n",
      "Batch Loss: 241.4086151123047\n",
      "\n",
      "Batch Loss: 325.434814453125\n",
      "\n",
      "Batch Loss: 390.46087646484375\n",
      "\n",
      "Batch Loss: 264.0560302734375\n",
      "\n",
      "Batch Loss: 346.0616149902344\n",
      "\n",
      "Batch Loss: 292.4162902832031\n",
      "\n",
      "Batch Loss: 374.9638977050781\n",
      "\n",
      "Batch Loss: 282.3648986816406\n",
      "\n",
      "Batch Loss: 329.9150390625\n",
      "\n",
      "Batch Loss: 281.951416015625\n",
      "\n",
      "Batch Loss: 296.0412902832031\n",
      "\n",
      "Batch Loss: 281.0223083496094\n",
      "\n",
      "Batch Loss: 309.2506103515625\n",
      "\n",
      "Batch Loss: 383.0260009765625\n",
      "\n",
      "Batch Loss: 260.3616027832031\n",
      "\n",
      "Batch Loss: 290.27978515625\n",
      "\n",
      "Batch Loss: 323.2593078613281\n",
      "\n",
      "Batch Loss: 300.0233459472656\n",
      "\n",
      "Batch Loss: 285.9349670410156\n",
      "\n",
      "Batch Loss: 330.306884765625\n",
      "\n",
      "Batch Loss: 270.2518615722656\n",
      "\n",
      "Batch Loss: 234.45046997070312\n",
      "\n",
      "Batch Loss: 249.01136779785156\n",
      "\n",
      "Batch Loss: 341.5604248046875\n",
      "\n",
      "Batch Loss: 239.82797241210938\n",
      "\n",
      "Batch Loss: 320.2569580078125\n",
      "\n",
      "Batch Loss: 330.4049377441406\n",
      "\n",
      "Batch Loss: 261.281005859375\n",
      "\n",
      "Batch Loss: 306.8573913574219\n",
      "\n",
      "Batch Loss: 336.12249755859375\n",
      "\n",
      "Batch Loss: 275.8681335449219\n",
      "\n",
      "Batch Loss: 244.1994171142578\n",
      "\n",
      "Batch Loss: 231.80886840820312\n",
      "\n",
      "Batch Loss: 315.292724609375\n",
      "\n",
      "Batch Loss: 266.269287109375\n",
      "\n",
      "Batch Loss: 256.7940979003906\n",
      "\n",
      "Batch Loss: 249.75579833984375\n",
      "\n",
      "Batch Loss: 221.66212463378906\n",
      "\n",
      "Batch Loss: 230.84889221191406\n",
      "\n",
      "Batch Loss: 306.2056579589844\n",
      "\n",
      "Batch Loss: 331.65948486328125\n",
      "\n",
      "Batch Loss: 285.0542297363281\n",
      "\n",
      "Batch Loss: 286.4447326660156\n",
      "\n",
      "Batch Loss: 229.1684112548828\n",
      "\n",
      "Batch Loss: 280.0307922363281\n",
      "\n",
      "Batch Loss: 276.153076171875\n",
      "\n",
      "Batch Loss: 292.2639465332031\n",
      "\n",
      "Batch Loss: 296.2662658691406\n",
      "\n",
      "Batch Loss: 245.19212341308594\n",
      "\n",
      "Batch Loss: 259.58209228515625\n",
      "\n",
      "Batch Loss: 258.80316162109375\n",
      "\n",
      "Batch Loss: 292.4347839355469\n",
      "\n",
      "Batch Loss: 281.8153991699219\n",
      "\n",
      "Batch Loss: 330.54638671875\n",
      "\n",
      "Batch Loss: 292.93975830078125\n",
      "\n",
      "Batch Loss: 222.48663330078125\n",
      "\n",
      "Batch Loss: 256.30096435546875\n",
      "\n",
      "Batch Loss: 296.7354431152344\n",
      "\n",
      "Batch Loss: 254.09654235839844\n",
      "\n",
      "Batch Loss: 273.18988037109375\n",
      "\n",
      "Batch Loss: 282.5865173339844\n",
      "\n",
      "Batch Loss: 236.61915588378906\n",
      "\n",
      "Batch Loss: 357.63812255859375\n",
      "\n",
      "Batch Loss: 264.10626220703125\n",
      "\n",
      "Batch Loss: 258.45220947265625\n",
      "\n",
      "Batch Loss: 287.74554443359375\n",
      "\n",
      "Batch Loss: 267.03369140625\n",
      "\n",
      "Batch Loss: 228.0507354736328\n",
      "\n",
      "Batch Loss: 283.2754211425781\n",
      "\n",
      "Batch Loss: 205.53713989257812\n",
      "\n",
      "Batch Loss: 393.4153747558594\n",
      "\n",
      "Batch Loss: 290.50048828125\n",
      "\n",
      "Batch Loss: 139.33534240722656\n",
      "\n",
      "Epoch 5, Average Loss: 363.18249011734156\n",
      "\n",
      "Model saved to sae/20240708_195600_sae.pkl\n",
      "Configuration saved to sae/20240708_195600_config.json\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "from utils.sae import SparseAutoencoder, SparseAutoencoderConfig\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "# Config \n",
    "config = {\n",
    "    \"batch_size\": 512,\n",
    "    \"dimensions\": 768,\n",
    "    \"sparsity_alpha\": 1,\n",
    "    \"lr\": 0.00001,\n",
    "    \"num_epochs\": 5,\n",
    "    \"sparsity_scale\": 1\n",
    "}\n",
    "\n",
    "# Assuming `dataset` is a PyTorch Dataset loaded and ready to use\n",
    "data_loader = torch.utils.data.DataLoader(loaded_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "sae_config = SparseAutoencoderConfig(d_model=config[\"dimensions\"], d_sparse=8 * config[\"dimensions\"], sparsity_alpha=config[\"sparsity_alpha\"])\n",
    "model = SparseAutoencoder(sae_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "with open(f\"sae/training_output_{timestamp}.out\", \"w\") as out_file:\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for sentences, embeddings in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Assuming data is already on the correct device and in the correct format\n",
    "            _, _, loss, _ = model.forward(embeddings, return_loss=True, sparsity_scale=config[\"sparsity_scale\"])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Print the loss for every batch\n",
    "            batch_loss_str = f\"Batch Loss: {loss.item()}\\n\"\n",
    "            print(batch_loss_str)\n",
    "            out_file.write(batch_loss_str)\n",
    "\n",
    "        epoch_loss_str = f\"Epoch {epoch+1}, Average Loss: {total_loss / len(data_loader)}\\n\"\n",
    "        out_file.write(epoch_loss_str)\n",
    "        print(epoch_loss_str)\n",
    "\n",
    "# Save the model to a pickle file\n",
    "model_path = f\"sae/{timestamp}_sae.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model.state_dict(), f)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the model from the pickle file\n",
    "model_path = f\"sae/{timestamp}_sae.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model_state_dict = pickle.load(f)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "\n",
    "# Save configuration\n",
    "config_path = f\"sae/{timestamp}_config.json\"\n",
    "with open(config_path, \"w\") as config_file:\n",
    "    json.dump(config, config_file)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Plotting the histogram\u001b[39;00m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 18\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_non_zero_count\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_non_zero_count\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskyblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature Index\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Zero Count\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/pyplot.py:2956\u001b[0m, in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[1;32m   2947\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarContainer:\n\u001b[0;32m-> 2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2597\u001b[0m, in \u001b[0;36mAxes.bar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# horizontal\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m         r\u001b[38;5;241m.\u001b[39msticky_edges\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mappend(l)\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_patch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2598\u001b[0m     patches\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m yerr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/axes/_base.py:2413\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_artist_props(p)\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2413\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_clip_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_patch_limits(p)\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children\u001b[38;5;241m.\u001b[39mappend(p)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/artist.py:799\u001b[0m, in \u001b[0;36mArtist.set_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, Rectangle):\n\u001b[0;32m--> 799\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipbox \u001b[38;5;241m=\u001b[39m TransformedBbox(\u001b[43mBbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    800\u001b[0m                                        path\u001b[38;5;241m.\u001b[39mget_transform())\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clippath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/transforms.py:798\u001b[0m, in \u001b[0;36mBbox.unit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munit\u001b[39m():\n\u001b[1;32m    797\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new unit `Bbox` from (0, 0) to (1, 1).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/matplotlib/transforms.py:767\u001b[0m, in \u001b[0;36mBbox.__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mpoints : `~numpy.ndarray`\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    A (2, 2) array of the form ``[[x0, y0], [x1, y1]]``.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 767\u001b[0m points \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m points\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBbox points must be of the form \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    770\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[[x0, y0], [x1, y1]]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a dictionary to count non-zero activations for each feature\n",
    "feature_non_zero_count = defaultdict(int)\n",
    "num_samples = min(1000, len(loaded_dataset.embeddings))\n",
    "\n",
    "# Count non-zero activations for each feature across samples\n",
    "for sample_idx in range(num_samples):\n",
    "    feature_activations = model.forward(loaded_dataset.embeddings[sample_idx])[1]\n",
    "    non_zero_indices = torch.nonzero(feature_activations, as_tuple=True)[0]\n",
    "    for idx in non_zero_indices:\n",
    "        feature_non_zero_count[idx.item()] += 1\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_non_zero_count.keys(), feature_non_zero_count.values(), color='skyblue')\n",
    "plt.xlabel('Feature Index', fontsize=14)\n",
    "plt.ylabel('Non-Zero Count', fontsize=14)\n",
    "plt.title(f'Feature Non-Zero Count for first {num_samples} samples', fontsize=16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Non-Zero Elements for first 100 samples: 12.819999694824219\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "total = 0\n",
    "num_samples = min(100, len(loaded_dataset.embeddings))\n",
    "\n",
    "for sample_idx in range(num_samples):\n",
    "    feature_activations = model.forward(loaded_dataset.embeddings[sample_idx])[1]\n",
    "    non_zero_elements = torch.count_nonzero(feature_activations)\n",
    "    total += non_zero_elements\n",
    "\n",
    "average_non_zero = total / num_samples\n",
    "\n",
    "print(f\"Average Non-Zero Elements for first {num_samples} samples: {average_non_zero}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3810\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_non_zero_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6144])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(model.decoder.weight, dim=0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
