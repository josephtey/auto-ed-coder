{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "from shared.models import MiniPileDataset\n",
    "from shared.interp import count_non_zero_feature_activations, plot_feature_activation_histogram, plot_feature_activation_histogram_from_log_feature_densities, plot_highest_activating_feature_for_each_sentence, create_feature_heatmap_widget\n",
    "from shared.features import Feature, FeatureSample\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Enable automatic reloading of modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: [ 1.84332848e-01 -1.71771586e-01  3.56589220e-02 -1.51276067e-01\n",
      " -1.85860202e-01 -4.03527260e-01  1.03313997e-01  2.31959239e-01\n",
      "  1.80198148e-01  2.54761260e-02 -2.52800286e-01 -2.99641520e-01\n",
      " -3.12144727e-01  1.36302456e-01 -1.11550286e-01  1.23413272e-01\n",
      " -1.41808733e-01  1.18051521e-01  3.99289727e-02 -7.64348805e-02\n",
      "  5.78769222e-02  1.11669153e-01 -5.93145967e-01  2.25300901e-02\n",
      "  9.98427927e-01 -4.05542165e-01 -3.07221293e-01 -3.29373479e-01\n",
      " -8.23361218e-01 -8.15446228e-02  5.12531400e-03  2.83867598e-01\n",
      " -2.70921260e-01 -1.25358090e-01 -1.67635307e-01 -1.80304632e-01\n",
      "  2.93781102e-01 -4.39690351e-02  3.00429743e-02  2.44957164e-01\n",
      " -5.87249517e-01 -4.26605135e-01  2.32217878e-01  1.81071863e-01\n",
      "  5.76134287e-02 -3.51474196e-01 -5.17745763e-02 -2.05165073e-02\n",
      " -9.27073210e-02 -9.01999176e-02 -9.37584698e-01  4.22418475e-01\n",
      " -3.02045234e-02  5.15738368e-01  2.13608995e-01  3.28836083e-01\n",
      "  2.70725578e-01 -9.43124831e-01 -1.13463707e-01 -1.97403207e-01\n",
      "  5.24869144e-01  2.38241330e-01 -3.28210831e-01 -3.99866402e-01\n",
      "  4.86791611e-01  1.93856955e-01 -7.39812292e-03  2.89789010e-02\n",
      " -1.11855531e+00  8.93208683e-02 -5.20291924e-01 -6.01975322e-01\n",
      " -1.69080302e-01 -1.71540722e-01 -2.09085599e-01  2.21900463e-01\n",
      " -4.84300584e-01  3.84844810e-01 -2.22211257e-01 -2.88314998e-01\n",
      " -1.09354801e-01  4.53198820e-01 -1.38316542e-01  3.97461861e-01\n",
      "  4.00453091e-01 -2.02784881e-01 -3.41472596e-01  1.06619574e-01\n",
      " -4.54022408e-01  2.85128713e-01 -1.99032109e-03 -5.06218448e-02\n",
      " -2.70794421e-01  9.01027024e-03  7.11987317e-01 -5.52167520e-02\n",
      " -4.91496533e-01  8.49598423e-02 -1.36652619e-01  1.74213365e-01\n",
      "  1.76105201e-01 -8.18498671e-01  3.35421592e-01  1.66528463e-01\n",
      " -2.84861803e-01 -6.19144179e-02 -2.34911740e-01  5.45087278e-01\n",
      "  4.32829559e-01  3.26146752e-01  3.97653401e-01 -2.38637447e-01\n",
      "  8.55914652e-02 -2.72238374e-01 -3.73603344e-01  1.18951365e-01\n",
      "  5.15809096e-02  1.12514623e-01  1.09589867e-01 -4.06011105e-01\n",
      " -2.50200868e-01  1.16383940e-01  1.90810814e-01  9.86904681e-01\n",
      " -2.06674874e-01  5.68773270e-01  2.94426456e-02  5.99673212e-01\n",
      "  3.48193139e-01 -2.16506347e-01  3.54760289e-01  6.71547353e-01\n",
      "  7.02141523e-01 -2.28119031e-01 -1.91575915e-01 -2.09049694e-02\n",
      "  1.36392236e-01 -3.41456831e-01 -6.40114844e-01 -1.36449412e-01\n",
      "  1.27465740e-01  4.36289795e-02  5.60470521e-01 -1.45850405e-01\n",
      "  3.38997751e-01  2.06563488e-01 -5.20347834e-01 -3.69103253e-01\n",
      "  1.27959788e-01  4.21688169e-01  2.50988722e-01  6.14808463e-02\n",
      " -4.90972131e-01 -2.43931159e-01 -6.50052309e-01  2.71236241e-01\n",
      "  2.32744038e-01  1.76081136e-01  8.07637721e-02  1.67829618e-01\n",
      "  6.05685532e-01  1.57420769e-01 -2.05251388e-02 -2.41255715e-01\n",
      "  1.56844646e-01 -2.40174070e-01  9.82337222e-02  4.06815916e-01\n",
      "  8.08200538e-02 -8.90297219e-02 -3.40777785e-01 -1.50410503e-01\n",
      "  4.40777510e-01 -4.26629856e-02 -3.61712098e-01  3.05118471e-01\n",
      "  1.24892838e-01  2.46040821e-02  5.74860871e-01 -2.43467465e-01\n",
      " -1.94530487e+00  6.29263163e-01  1.11401208e-01 -4.39669490e-01\n",
      "  1.55987978e-01 -8.83343890e-02  5.07671118e-01 -7.15770662e-01\n",
      " -3.27078640e-01 -5.07888459e-02 -3.37870806e-01 -1.08473487e-01\n",
      " -3.67167920e-01 -3.68711084e-01  6.18597567e-01 -3.83514911e-01\n",
      " -5.28768264e-02 -1.93741277e-01 -2.28900418e-01 -2.84703560e-02\n",
      "  6.03478067e-02  2.06118807e-01  9.49083865e-02 -1.16995722e-01\n",
      "  1.49401292e-01 -2.64349073e-01  2.43329182e-01 -3.20513435e-02\n",
      " -3.85452092e-01  1.02498487e-01 -2.76675254e-01  9.01556194e-01\n",
      "  2.61748075e-01 -1.39335543e-01  3.10586095e-01 -5.43899313e-02\n",
      "  5.91374747e-02 -1.88724417e-02 -2.43270218e-01 -7.84347728e-02\n",
      " -1.17974758e-01 -1.73325837e-01 -6.63170636e-01  4.59944278e-01\n",
      " -4.73020256e-01  7.75357008e-01  1.63281694e-01  1.16640374e-01\n",
      "  8.41027200e-01  1.87457651e-01  1.89613830e-02 -3.73942375e-01\n",
      "  3.94541472e-01  3.25324833e-02 -2.23938167e-01  3.58625539e-02\n",
      " -3.66799496e-02  4.05229449e-01  9.76401120e-02  2.83943623e-01\n",
      "  7.00924918e-02  1.75997779e-01  4.43293273e-01 -5.43319046e-01\n",
      " -2.34690800e-01 -5.29378116e-01 -1.88668981e-01  7.37436712e-02\n",
      "  4.25363630e-01 -3.55168372e-01  7.92965293e-02 -5.22769809e-01\n",
      "  5.54556362e-02 -5.82257569e-01 -3.46066058e-01  1.72047183e-01\n",
      " -4.61951375e-01 -2.01587543e-01  8.01316053e-02  6.46951437e-01\n",
      " -1.89663216e-01  8.65733624e-02  3.06884646e-01  2.53265530e-01\n",
      " -7.62029827e-01 -3.38904738e-01  1.72850803e-01  2.90252388e-01\n",
      "  5.39870381e-01  3.50318551e-01 -4.13599700e-01 -4.77460861e-01\n",
      " -1.17312334e-01  7.55386055e-02 -3.13133359e-01 -2.05436006e-01\n",
      " -1.50396734e-01  1.96661130e-01  1.54111639e-01 -9.92752090e-02\n",
      "  1.30958661e-01  4.38898861e-01 -1.01894528e-01  1.76706374e-01\n",
      "  6.46051839e-02 -4.95651513e-01 -1.19422697e-01 -1.79747090e-01\n",
      " -2.01200500e-01 -4.46486652e-01 -4.92887050e-01  1.59111992e-01\n",
      " -3.74611676e-01 -1.42987430e-01  3.35612893e-01  3.60003501e-01\n",
      "  4.56390023e-01  1.49179220e-01 -8.30710009e-02 -4.53734964e-01\n",
      "  1.90530170e-03 -5.09532034e-01 -1.84213027e-01  1.06493086e-01\n",
      " -7.37985790e-01  2.95863748e-01 -2.97658294e-01 -5.07092059e-01\n",
      " -2.88789082e+00 -6.46748068e-03 -8.59494582e-02 -2.44239241e-01\n",
      " -8.84420127e-02 -3.83003682e-01 -3.05283278e-01 -4.69035119e-01\n",
      " -1.50302067e-01  2.43573844e-01 -5.00724971e-01 -5.44328630e-01\n",
      "  4.50647622e-02  5.34825265e-01  4.39885318e-01  2.74110317e-01\n",
      "  2.68779963e-01  1.15547560e-01 -2.05139071e-01  4.53340799e-01\n",
      "  1.02118440e-01 -1.94548517e-01  8.48869979e-02 -9.83454734e-02\n",
      "  3.52300823e-01  2.32621372e-01 -2.50088125e-01  2.19452754e-01\n",
      " -3.12835962e-01 -5.23625314e-01 -1.17476225e-01 -1.16808370e-01\n",
      "  3.21626216e-01 -1.34121910e-01  1.08065374e-01  2.90559649e-01\n",
      "  2.51802653e-01 -2.77786642e-01  2.27655992e-01 -4.19227690e-01\n",
      " -1.91684902e-01  8.87292325e-02  1.43754885e-01 -7.96287358e-01\n",
      "  5.19389212e-01 -4.93237466e-01 -1.96727023e-01 -2.26318106e-01\n",
      "  3.00869435e-01  3.86158556e-01 -2.84131259e-01  2.14403905e-02\n",
      " -8.95936564e-02  7.63747394e-02 -1.68838009e-01 -2.35677227e-01\n",
      "  6.28427267e-01  4.04258043e-01 -3.30862015e-01 -2.68223017e-01\n",
      "  6.11354411e-01 -4.10597116e-01 -6.13594502e-02 -4.12298203e-01\n",
      " -1.26606748e-01 -3.42226535e-01 -2.23295540e-01 -4.28971082e-01\n",
      " -3.92581075e-01  3.17645222e-01 -5.32603800e-01 -7.49291554e-02\n",
      " -4.75908279e-01 -9.68175352e-01  9.52409580e-02 -2.12513059e-01\n",
      "  5.65168373e-02 -1.01700589e-01  1.89940020e-01 -4.43336606e-01\n",
      " -7.20804989e-01 -1.05935633e+00 -1.65572055e-02 -4.34781685e-02\n",
      " -7.86919892e-02 -1.59220949e-01 -8.41434151e-02 -2.00505555e-01\n",
      " -4.76118922e-03 -3.67660224e-01  2.80541331e-01  9.67242196e-02\n",
      "  2.03510538e-01  1.78329483e-01 -2.61431664e-01  1.06651813e-01\n",
      "  6.70660377e-01 -6.54616296e-01  6.25170946e-01 -4.07324731e-03\n",
      "  2.30991930e-01 -2.93680787e-01  2.58547813e-01 -6.20294809e-02\n",
      " -1.72861278e-01  6.06309660e-02 -9.05545175e-01  1.86379179e-02\n",
      "  3.29349548e-01  2.14904606e-01  9.80646834e-02 -5.03782667e-02\n",
      "  2.58854687e-01 -2.74916589e-01 -4.61114585e-01  5.39851248e-01\n",
      " -1.20327003e-01  5.33770502e-01  3.46242249e-01 -1.02885865e-01\n",
      "  2.07501784e-01  5.19259274e-01 -4.42767620e-01 -2.58111507e-01\n",
      " -3.69678408e-01 -6.27285019e-02  3.90462019e-02  8.18418860e-02\n",
      " -8.34587589e-02 -3.03656250e-01 -1.70111358e-01 -3.80658984e-01\n",
      "  2.68816888e-01  3.99530619e-01  4.51147258e-01 -4.19878006e-01\n",
      "  1.52693376e-01  8.54129970e-01  4.19375151e-01 -2.11843535e-01\n",
      "  2.06767470e-01  3.36922914e-01 -1.30501553e-01 -1.69572309e-01\n",
      " -1.34190187e-01  7.28272676e-01  1.61185265e-01 -7.39404038e-02\n",
      "  2.29872808e-01 -8.06810856e-02 -2.62280315e-01  9.04846843e-03\n",
      "  4.36411798e-02 -2.34168440e-01  1.33841515e-01 -4.87366676e-01\n",
      "  9.40187946e-02 -2.92565554e-01 -1.42727911e-01 -6.77573562e-01\n",
      "  9.04370099e-02  5.54577827e-01  3.13430011e-01  2.52544850e-01\n",
      "  6.76773340e-02  1.05100177e-01  1.35782495e-01  3.57523113e-02\n",
      " -3.21551681e-01  3.56524497e-01 -1.82050005e-01 -1.06989294e-01\n",
      "  4.02673900e-01  2.68155158e-01  1.46698073e-01  7.41977453e-01\n",
      "  2.48449698e-01 -6.10867366e-02  3.43116760e-01  9.69204366e-01\n",
      "  2.52374500e-01  1.64143190e-01  4.45282320e-03  9.07698199e-02\n",
      "  7.41428928e-03  6.83543205e-01 -4.49091941e-02  1.90159544e-01\n",
      " -1.08530670e-01  1.74646631e-01 -8.32020998e-01  2.05453739e-01\n",
      " -3.44204992e-01 -3.79467398e-01 -4.70844239e-01 -4.21978295e-01\n",
      "  2.20239863e-01  3.11728436e-02  3.16692084e-01 -2.31599614e-01\n",
      "  7.08728373e-01  1.60696149e-01 -5.44118166e-01 -1.50985762e-01\n",
      " -3.43097299e-01 -1.47267088e-01  1.02178268e-01  2.66330987e-01\n",
      "  1.27743021e-01 -1.34556755e-01  1.63021788e-01 -4.23554033e-01\n",
      " -2.29439691e-01 -8.33189011e-01 -6.39231363e-03  1.32580101e-01\n",
      "  3.38443041e-01  8.64432082e-02  5.24421930e-02 -3.93161088e-01\n",
      " -5.01638114e-01  1.36397734e-01  2.10766137e-01  4.49232787e-01\n",
      " -4.39736366e-01 -1.33903593e-01 -2.49062538e-01 -3.17483693e-01\n",
      " -9.86562729e-01  2.51162816e-02  4.11117375e-01 -9.46079269e-02\n",
      "  7.82202408e-02  2.19533592e-01  2.54221946e-01 -5.55053473e-01\n",
      " -7.24167049e-01 -1.80166766e-01  8.06767941e-02 -3.80235106e-01\n",
      " -4.52134311e-02  1.30804926e-01  5.95639795e-02 -4.49925721e-01\n",
      "  1.16272859e-01 -2.70594954e-01 -9.54328254e-02  5.42314947e-01\n",
      " -2.39731789e-01  3.81362051e-01 -1.27521977e-01  5.67524210e-02\n",
      " -2.97713637e-01 -2.10127160e-01  4.43590671e-01 -9.17517394e-02\n",
      " -5.58518350e-01  1.44271478e-01 -6.51284635e-01  4.12470609e-01\n",
      "  9.17718112e-01 -2.11993173e-01 -2.68445283e-01  6.13672853e-01\n",
      "  7.03846216e-01  3.51380199e-01  1.67661726e-01  4.31342512e-01\n",
      "  1.69049636e-01 -1.27338037e-01 -4.87378627e-01 -1.62444077e-02\n",
      "  3.86898592e-02  4.04595464e-01  5.28637059e-02 -5.20553172e-01\n",
      "  1.49998546e-01 -5.72479665e-01  1.15426421e-01 -1.62772164e-01\n",
      "  2.03778073e-01  4.00180697e-01  1.56156600e-01  2.65952229e-01\n",
      "  4.72948521e-01 -1.38975888e-01 -2.84612447e-01  2.19336644e-01\n",
      "  2.73059547e-01 -7.83561319e-02 -3.14102143e-01  6.28074825e-01\n",
      " -4.55607891e-01 -1.60528719e-03 -2.26280645e-01 -6.75200857e-03\n",
      "  4.54709798e-01  1.74850360e-01  5.00869602e-02 -3.58166657e-02\n",
      "  1.19468018e-01  6.40266359e-01  1.33192480e-01  7.60062737e-03\n",
      " -2.24515676e-01  1.39466658e-01  2.93824494e-01 -1.03835016e-01\n",
      "  1.22974515e-01  5.11467874e-01 -3.46491337e-01 -4.12545234e-01\n",
      "  6.12567186e-01  5.78426421e-01 -5.34381509e-01 -3.81392479e-01\n",
      "  2.71824360e-01 -3.63139361e-01  9.42737982e-02  4.88521084e-02\n",
      "  5.27676148e-03 -1.66862309e-01  4.02379155e-01  1.04968168e-01\n",
      " -6.17148757e-01  5.67100942e-01 -3.55313599e-01 -3.81077468e-01\n",
      " -1.34508312e-01  1.84286445e-01  6.01793043e-02  6.47933900e-01\n",
      " -1.65984407e-01  3.96734327e-01 -5.28452516e-01 -5.17195642e-01\n",
      "  2.09688231e-01  4.37361211e-01  2.78446853e-01 -4.86959904e-01\n",
      "  5.02451837e-01  3.17413211e-01 -9.98046175e-02  2.79825181e-01\n",
      " -4.87911664e-02 -5.28197736e-02 -5.96285202e-02  1.16708223e-02\n",
      "  3.37624997e-01  8.59709501e-01  2.46558324e-01  2.73967236e-01\n",
      "  1.58479229e-01 -3.26929614e-02  1.94980398e-01  2.56678611e-01\n",
      "  4.90221381e-01  4.35896128e-01  2.94141829e-01  1.39319569e-01\n",
      "  2.06975996e-01  1.16326831e-01  1.93720296e-01 -4.96255606e-01\n",
      " -3.53079736e-01  4.21195209e-01  7.80160904e-01  1.71035990e-01\n",
      " -1.58888519e-01  3.60118388e-03 -1.55743942e-01  2.41549581e-01\n",
      "  7.65400454e-02 -2.09494054e-01 -1.55497342e-01  3.89145106e-01\n",
      "  2.18627974e-01  6.59086183e-02 -6.04412556e-01 -2.73640126e-01\n",
      "  9.35675297e-03  4.19967733e-02 -2.13664725e-01 -2.24155843e-01\n",
      " -2.09685549e-01 -5.09338737e-01  2.33814232e-02 -1.18733212e-01\n",
      " -1.46852434e-01 -3.02303672e-01 -2.37115532e-01 -4.44256127e-01\n",
      "  3.45178246e-01 -5.36383092e-01  4.42761868e-01 -3.02120954e-01\n",
      " -1.50089592e-01  4.75949973e-01  7.08679140e-01 -1.60310581e-01\n",
      "  1.02260463e-01 -3.39663982e-01 -2.36622810e-01 -3.80150974e-02\n",
      " -1.17973546e-02  2.41448626e-01  1.87617969e-02  4.31512333e-02\n",
      " -5.72245345e-02 -5.13237119e-01  3.45932931e-01  2.40959421e-01\n",
      " -6.25297725e-01 -1.68495495e-02 -4.38436508e-01  4.03073877e-01\n",
      " -1.51193753e-01 -2.22317696e-01  6.25932515e-02  5.10176301e-01\n",
      " -6.36595070e-01 -6.74313828e-02  4.30710793e-01  3.21490973e-01\n",
      " -1.18900202e-01  9.16589946e-02  1.32829294e-01 -9.90768429e-03\n",
      " -2.55487680e-01 -2.74422765e-01  6.82758152e-01  4.71755236e-01\n",
      "  2.80199200e-01  3.81901383e-01  3.33388060e-01 -4.54117298e-01\n",
      " -1.70497671e-01  3.34014505e-01  5.29027125e-03 -1.50554478e-01\n",
      " -4.23703194e-01  3.21384519e-01  2.06245750e-01 -1.41856492e-01\n",
      " -5.71753025e-01  1.95348009e-01  1.27584562e-01 -4.44040745e-01\n",
      " -1.57328889e-01 -3.29751521e-01  3.62259269e-01  4.16534096e-02\n",
      " -1.91411644e-01 -3.24715883e-01 -1.93683490e-01  1.36172459e-01\n",
      "  3.80617976e-01 -1.27539635e-01 -2.56648391e-01  3.69657487e-01]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME_1 = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME_1)\n",
    "\n",
    "# Encode some text\n",
    "text = \"This is a sample text to encode.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Move model to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Get the embedding from the last hidden state\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "print(\"Embedding:\", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shared'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse_autoencoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseAutoencoder, SparseAutoencoderConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shared'"
     ]
    }
   ],
   "source": [
    "# Load the model from the pickle file\n",
    "import pickle \n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from shared.sparse_autoencoder import SparseAutoencoder, SparseAutoencoderConfig\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load the dataset\n",
    "pre_trained_sentences_file = \"pre-trained/data/asap_sentences_3_bert-base-uncased.npy\"\n",
    "pre_trained_embeddings_file = \"pre-trained/data/asap_embeddings_3_bert-base-uncased.npy\"\n",
    "pre_trained_mini_pile_dataset = MiniPileDataset(pre_trained_sentences_file, pre_trained_embeddings_file)\n",
    "\n",
    "# Load the configuration from the JSON file\n",
    "pre_trained_config_path = \"pre-trained/sae/config.json\"\n",
    "with open(pre_trained_config_path, \"r\") as pre_trained_config_file:\n",
    "    pre_trained_config = json.load(pre_trained_config_file)\n",
    "\n",
    "# Load the pre-trained model from the pickle file\n",
    "pre_trained_sae_config = SparseAutoencoderConfig(d_model=pre_trained_config[\"dimensions\"], d_sparse=8 * pre_trained_config[\"dimensions\"], sparsity_alpha=pre_trained_config[\"sparsity_alpha\"])\n",
    "pre_trained_model = SparseAutoencoder(pre_trained_sae_config)\n",
    "pre_trained_model_path = \"pre-trained/sae/sae.pkl\"\n",
    "with open(pre_trained_model_path, \"rb\") as pre_trained_f:\n",
    "    pre_trained_model_state_dict = pickle.load(pre_trained_f)\n",
    "    pre_trained_model.load_state_dict(pre_trained_model_state_dict)\n",
    "\n",
    "# Load the log feature densities from the JSON file\n",
    "pre_trained_log_feature_densities_path = \"pre-trained/sae/log_feature_densities.json\"\n",
    "with open(pre_trained_log_feature_densities_path, \"r\") as pre_trained_json_file:\n",
    "    pre_trained_log_feature_densities = json.load(pre_trained_json_file)\n",
    "\n",
    "# Load features\n",
    "pre_trained_features_folder = \"pre-trained/features\"\n",
    "pre_trained_features = []\n",
    "pre_trained_filtered_out_count = 0\n",
    "\n",
    "for pre_trained_filename in os.listdir(pre_trained_features_folder):\n",
    "    if pre_trained_filename.startswith(\"feature_\") and pre_trained_filename.endswith(\".json\"):\n",
    "        with open(os.path.join(pre_trained_features_folder, pre_trained_filename), \"r\") as pre_trained_json_file:\n",
    "            pre_trained_feature_data = json.load(pre_trained_json_file)\n",
    "            pre_trained_feature = Feature(**pre_trained_feature_data)\n",
    "            pre_trained_features.append(pre_trained_feature)\n",
    "            # if len(pre_trained_feature.high_act_samples) >= 10:\n",
    "            #     pre_trained_features.append(pre_trained_feature)\n",
    "            # else:\n",
    "            #     pre_trained_filtered_out_count += 1\n",
    "\n",
    "# Load the scores from the JSON file\n",
    "with open(\"asap_sentences_scores_3.json\", \"r\") as score_file:\n",
    "    score_dict = json.load(score_file)\n",
    "\n",
    "# Add scores to high acting samples in pre_trained_features\n",
    "for feature in pre_trained_features:\n",
    "    for sample in feature.high_act_samples:\n",
    "        sample_text = sample.text\n",
    "        if sample_text in score_dict:\n",
    "            sample.score = score_dict[sample_text]\n",
    "        else:\n",
    "            sample.score = None  # or some default value if the text is not found\n",
    "\n",
    "print(\"Scores added to high acting samples in pre_trained_features.\")\n",
    "\n",
    "# pre_trained_features.sort(key=lambda x: x.confidence, reverse=True)\n",
    "\n",
    "# print(f\"Number of filtered out samples: {pre_trained_filtered_out_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class BottleneckT5Autoencoder:\n",
    "    def __init__(self, model_path: str, device='cpu'):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=512, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, text: str) -> torch.FloatTensor:\n",
    "        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        decoder_inputs = self.tokenizer('', return_tensors='pt').to(self.device)\n",
    "        return self.model(\n",
    "            **inputs,\n",
    "            decoder_input_ids=decoder_inputs['input_ids'],\n",
    "            encode_only=True,\n",
    "        )[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_latent(self, latent: torch.FloatTensor, max_length=512, temperature=1.0) -> str:\n",
    "        dummy_text = '.'\n",
    "        dummy = self.embed(dummy_text)\n",
    "        perturb_vector = latent - dummy\n",
    "        self.model.perturb_vector = perturb_vector\n",
    "        input_ids = self.tokenizer(dummy_text, return_tensors='pt').to(self.device).input_ids\n",
    "        output = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b44bc94dc354cd2bdc6b5e3d6bda35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26637a1e8e2544da9c6ea6cd8838b06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bottleneck_t5.py:   0%|          | 0.00/18.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/thesephist/contra-bottleneck-t5-base-wikipedia:\n",
      "- bottleneck_t5.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940d03eabd694d83ac0d78d1a2c5e7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b7820ee9f44343b662b77065f801d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "autoencoder = BottleneckT5Autoencoder(model_path='thesephist/contra-bottleneck-t5-base-wikipedia', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 12 at dim 1 (got 29)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHi there! My name is Linus, and I spend a lot of my time thinking about latent spaces of neural network models.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNotion is a single space where you can think, write, and plan. Capture thoughts, manage projects, or even run an entire company â€” and do it exactly the way you want.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m----> 8\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     reconstruction \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mgenerate_from_latent(embedding)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reconstruction)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mBottleneckT5Autoencoder.embed\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m---> 18\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m     decoder_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     22\u001b[0m         decoder_input_ids\u001b[38;5;241m=\u001b[39mdecoder_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     23\u001b[0m         encode_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3016\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3015\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3016\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3018\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3104\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3099\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3100\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3101\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3102\u001b[0m         )\n\u001b[1;32m   3103\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3127\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3128\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3147\u001b[0m     )\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3306\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3297\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3298\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3299\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3303\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3304\u001b[0m )\n\u001b[0;32m-> 3306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3308\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:889\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m     second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n\u001b[0;32m--> 889\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_prepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[0;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    965\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    967\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    968\u001b[0m     batch_outputs,\n\u001b[1;32m    969\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding_strategy\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    974\u001b[0m )\n\u001b[0;32m--> 976\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/piech/u/joetey/auto-ed-coder/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'The quick brown fox jumps over the lazy dog',\n",
    "    'Hi there! My name is Linus, and I spend a lot of my time thinking about latent spaces of neural network models.',\n",
    "    'Notion is a single space where you can think, write, and plan. Capture thoughts, manage projects, or even run an entire company â€” and do it exactly the way you want.',\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    embedding = autoencoder.embed(texts)\n",
    "    reconstruction = autoencoder.generate_from_latent(embedding)\n",
    "    print(reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.load('../../data_preparation/embedding_chunks/embedded_chunks/contra_minipile_20241013_110604/sentences_checkpoint.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600870"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings with shape: (600870, 768)\n",
      "First embedding: [-4.65309201e-03 -1.46815956e-01  2.68288124e-02 -1.82312667e-01\n",
      " -5.12253754e-02  8.20278600e-02  6.27405494e-02 -4.61224355e-02\n",
      " -1.93447977e-01 -7.58789629e-02  4.99881096e-02  9.96305496e-02\n",
      "  2.84313019e-02  5.58600128e-02 -9.69482958e-02  7.02655688e-02\n",
      " -1.45358011e-01 -6.26241043e-02  6.66467622e-02 -4.22734581e-02\n",
      " -1.02560729e-01 -6.23875484e-02 -2.08742358e-02  6.34844080e-02\n",
      " -4.33241278e-02 -1.38137385e-01 -7.97744468e-03 -2.95009054e-02\n",
      "  6.19277880e-02 -1.92917585e-02 -8.89736414e-02  1.59900472e-01\n",
      " -2.73081698e-02 -2.42129453e-02 -5.17058596e-02  2.60039475e-02\n",
      "  7.85812065e-02 -3.24544013e-02  4.26443443e-02  8.86103436e-02\n",
      " -4.94930930e-02 -6.03097156e-02 -4.58362438e-02  4.38228846e-02\n",
      " -4.66443487e-02 -4.33461778e-02 -4.30061556e-02 -1.47030637e-01\n",
      " -1.29455496e-02  4.05242071e-02  5.88647537e-02  1.68228522e-01\n",
      " -4.22835760e-02  2.52656136e-02 -1.14559084e-01 -2.92030182e-02\n",
      "  5.10108881e-02 -4.73567052e-03 -7.33316466e-02 -9.10021812e-02\n",
      " -6.04312904e-02 -1.52385058e-02  7.06155077e-02  8.92905798e-03\n",
      "  1.20028397e-02 -2.01991443e-02  1.63437556e-02  7.03784376e-02\n",
      "  6.19917139e-02 -6.74844161e-02  2.45433208e-02  3.39462794e-02\n",
      "  2.83225179e-02  7.06643984e-02  1.25538737e-01  3.91467437e-02\n",
      " -4.29248102e-02  5.33473901e-02  5.97352572e-02  5.37462123e-02\n",
      "  1.03820182e-01 -6.67668432e-02 -3.56998146e-02  9.45062283e-03\n",
      "  1.96211953e-02  1.82224791e-02  2.56540552e-02 -2.52429936e-02\n",
      "  8.83488059e-02 -8.20580125e-02  2.37714895e-03 -1.26845494e-01\n",
      " -4.18605991e-02  3.74951847e-02  5.68925403e-02  3.65721174e-02\n",
      "  4.99615818e-02  1.78370476e-02 -6.33585900e-02 -2.53140070e-02\n",
      "  6.44109398e-02  9.53653902e-02  1.08764851e-02 -5.11489026e-02\n",
      " -1.49879698e-02  4.20981273e-02  1.04518876e-05 -3.01037589e-03\n",
      "  3.80872656e-03  4.70667072e-02  5.54220080e-02 -4.34604064e-02\n",
      "  7.59204924e-02 -1.47404177e-02 -1.39682209e-02 -7.37180188e-02\n",
      "  4.15997468e-02 -3.36103477e-02  3.61181200e-02  2.64173541e-02\n",
      " -7.24227279e-02  1.16975894e-02 -1.70428623e-02 -3.19236033e-02\n",
      "  5.51711470e-02 -6.68404028e-02  7.07412139e-02  1.01099998e-01\n",
      " -8.54801852e-03 -1.14317276e-02 -1.60112053e-01  8.01638067e-02\n",
      " -6.36964813e-02  3.20190042e-02 -3.83612402e-02  1.52408361e-01\n",
      " -1.18634012e-02 -3.80650721e-02 -1.22837462e-01  1.93568841e-02\n",
      " -7.39592388e-02 -2.03110371e-02 -5.32527305e-02 -2.32590660e-02\n",
      " -1.00776009e-01 -1.16835929e-01 -2.70514600e-02  3.24844532e-02\n",
      " -8.54415819e-02 -8.97925906e-03  6.71414882e-02  9.66651514e-02\n",
      " -3.21432762e-02 -8.46253186e-02 -2.84393299e-02 -1.02039307e-01\n",
      " -2.58518364e-02 -1.97133534e-02  2.20346451e-03  1.79602839e-02\n",
      " -8.81134905e-03 -2.06628293e-02  8.47890154e-02  6.04293644e-02\n",
      "  4.00476940e-02  4.12354469e-02  1.31437019e-01 -1.05195478e-01\n",
      " -1.33872196e-01 -5.31205684e-02 -1.01141527e-01 -7.87886754e-02\n",
      " -1.18609546e-02 -2.35486552e-02 -5.40938564e-02 -2.32262686e-02\n",
      " -1.16528869e-01  2.90474147e-02  5.68511747e-02 -2.89755929e-02\n",
      " -7.46704862e-02  1.42863914e-01 -4.40275930e-02 -1.07050017e-02\n",
      " -1.14432476e-01 -1.70100689e-01 -4.29557450e-02  1.30980447e-01\n",
      " -4.24569063e-02  4.37434949e-02  1.04296759e-01 -1.27578989e-01\n",
      " -3.63113135e-02  2.00843941e-02  9.81711373e-02 -7.23765418e-02\n",
      "  4.65667471e-02  4.03269334e-03 -8.00054297e-02  8.44367072e-02\n",
      "  7.22517958e-04 -1.13357000e-01 -2.58082990e-02 -4.12812978e-02\n",
      "  1.14813253e-01  2.61559095e-02 -1.48923313e-02 -1.41610116e-01\n",
      "  2.56788414e-02  8.38798732e-02  4.04372104e-02  3.12094782e-02\n",
      "  3.20060328e-02 -7.58525953e-02  1.37633324e-01  8.21414888e-02\n",
      "  5.05657159e-02 -4.95299092e-03 -8.96864235e-02  6.11086236e-03\n",
      " -2.89045796e-02 -1.15971208e-01  8.23883563e-02 -5.63897192e-03\n",
      " -4.39032912e-04 -2.77084932e-02  9.75412428e-02 -2.35899594e-02\n",
      "  2.39019282e-03 -8.26739967e-02 -1.14704324e-02  6.56765923e-02\n",
      " -6.31696880e-02 -1.22991517e-01  5.60912527e-02  1.35730067e-02\n",
      " -4.70721573e-02  9.32774320e-03  1.18285529e-01 -1.23416586e-02\n",
      "  1.86497923e-02  6.61678761e-02  7.62255043e-02 -3.76524404e-02\n",
      "  6.34839386e-02  6.85153306e-02  4.71478421e-03  5.34426682e-02\n",
      " -2.93466095e-02 -1.35659873e-01  5.72574362e-02 -8.91072601e-02\n",
      " -7.85015747e-02 -5.15615009e-03  2.14536171e-02  1.60728499e-01\n",
      " -1.22114392e-02 -1.83923095e-02 -1.10119034e-03 -8.73028114e-02\n",
      " -4.13433537e-02 -1.39422379e-02 -2.14657951e-02  4.60107103e-02\n",
      "  2.34083068e-02  6.09847642e-02 -1.11010939e-01  7.30189234e-02\n",
      "  6.35518059e-02  6.18034266e-02 -7.20462203e-02  9.43601690e-03\n",
      " -3.53712961e-02  3.53186801e-02 -4.96958382e-02 -1.02199733e-01\n",
      "  5.22170663e-02 -4.05819900e-02  4.47010659e-02  1.04610018e-01\n",
      " -1.61236096e-02 -1.88807454e-02 -2.52831597e-02  1.30335107e-01\n",
      " -7.62506053e-02  6.15924597e-02 -2.01714262e-02 -3.05488314e-02\n",
      " -6.24435470e-02  1.32188676e-02  1.36615094e-02 -6.92823529e-02\n",
      " -5.10857478e-02 -1.34830430e-01  1.33318650e-02  3.29200290e-02\n",
      " -5.19280024e-02 -6.94981888e-02 -9.05638188e-03  1.54709056e-01\n",
      " -6.99350387e-02 -1.22102946e-01 -9.46669504e-02 -3.91895212e-02\n",
      " -5.49576320e-02  5.82441874e-02 -9.12172198e-02 -3.71181443e-02\n",
      "  7.12591037e-03  3.30547197e-03  2.89206393e-02  1.24177285e-01\n",
      "  5.50115891e-02 -7.44912550e-02  9.47279297e-03 -2.79789865e-02\n",
      "  6.99129775e-02  6.24308474e-02 -3.07066143e-02  8.69453847e-02\n",
      " -4.68211807e-02 -9.30920243e-02 -2.80051343e-02 -1.27444947e-02\n",
      " -3.26879285e-02  9.58431289e-02  1.37845546e-01  6.62644356e-02\n",
      " -5.27004786e-02  2.09665392e-02  5.73315024e-02 -1.12178743e-01\n",
      "  9.45213065e-03  6.05265833e-02 -5.81431501e-02 -1.08477501e-02\n",
      " -5.35750389e-02  5.97787797e-02  1.78396940e-01 -5.22301532e-02\n",
      "  1.98657736e-02  7.75958411e-03 -3.38977426e-02 -1.12441303e-02\n",
      "  2.36682687e-02  1.33892428e-02  1.36296069e-02 -2.42751110e-02\n",
      "  3.80304419e-02 -7.08924383e-02 -7.93634430e-02 -2.73142401e-02\n",
      " -2.72287037e-02 -5.65164350e-03  2.11878549e-02  2.77923252e-02\n",
      "  1.49065098e-02 -1.03766121e-01 -1.13232611e-02  8.98379534e-02\n",
      "  6.71908632e-02 -3.36156674e-02 -4.47223075e-02  5.58437183e-02\n",
      " -8.04424882e-02 -1.33979186e-01  8.35232716e-03  6.77911565e-02\n",
      " -7.41757825e-02 -4.63022478e-02 -3.91459651e-03 -1.04146786e-01\n",
      " -1.19331218e-01  2.44320165e-02  6.41766191e-02 -8.86450037e-02\n",
      "  6.23570662e-03 -2.05354840e-02  1.08466959e-02  2.12157257e-02\n",
      "  8.04042593e-02 -7.12044537e-02 -7.73080513e-02 -3.82683123e-03\n",
      " -3.99072096e-02  1.26807131e-02 -1.51968211e-01  1.45603865e-01\n",
      " -1.89554654e-02 -1.46045461e-02  2.34812908e-02  6.21542819e-02\n",
      " -4.75981049e-02  7.19346255e-02  1.51819619e-03  1.09393388e-01\n",
      "  4.99438727e-03 -3.11625618e-02 -5.99467829e-02  1.45765897e-02\n",
      " -4.84947264e-02  6.05835244e-02  1.78966019e-02  6.31916970e-02\n",
      " -5.24806464e-03  1.17071858e-02 -6.91704452e-02  2.81897053e-04\n",
      "  3.36324461e-02 -2.36318111e-02  5.88447182e-03  6.31984398e-02\n",
      " -3.00163124e-03 -1.65603861e-01 -2.69979928e-02 -2.59871278e-02\n",
      "  5.38855083e-02  3.08234617e-02  2.62982212e-03  4.10439596e-02\n",
      "  5.18935509e-02  6.54321909e-02  9.78493392e-02  3.81099768e-02\n",
      "  1.82246063e-02  1.38144195e-01  9.44567248e-02 -1.67962193e-01\n",
      " -6.46538585e-02 -9.51632932e-02  1.73468012e-02  3.03558800e-02\n",
      " -1.39828262e-04  5.37656508e-02  3.11142858e-02  9.74894986e-02\n",
      "  6.78984448e-02  2.36279741e-02 -3.77678797e-02  3.78550962e-02\n",
      "  3.59659158e-02 -3.08538526e-02  7.56328255e-02 -2.54556793e-03\n",
      "  3.46883796e-02 -9.07932967e-02 -2.76519880e-02 -4.39504161e-03\n",
      "  1.20651014e-02 -1.87162198e-02  3.69492024e-02  3.05647980e-02\n",
      "  2.07196455e-03  5.95244318e-02 -3.82245220e-02 -3.80454510e-02\n",
      " -1.91010553e-02  2.22603586e-02 -3.77169959e-02 -1.43506229e-02\n",
      " -5.84575422e-02  4.00674753e-02 -3.50123160e-02  7.45718703e-02\n",
      " -4.46467102e-02 -5.81109710e-02  7.34244585e-02 -1.46825999e-01\n",
      " -5.07059060e-02  2.20234250e-03  3.61210071e-02 -2.77210157e-02\n",
      "  8.22070539e-02  5.91697171e-02 -2.98465211e-02 -1.90778282e-02\n",
      " -1.39032369e-02 -1.24603156e-02  1.27853751e-02 -1.29467413e-01\n",
      " -5.16122878e-02 -2.31568646e-02 -1.68747846e-02  3.87788080e-02\n",
      "  1.85477752e-02 -3.63945477e-02  9.73862484e-02  1.23884127e-01\n",
      "  3.60022811e-03  1.59888670e-01 -6.78025931e-02  3.00654154e-02\n",
      " -2.20333543e-02  3.19180675e-02 -2.88933832e-02  1.04543827e-01\n",
      " -2.58707870e-02 -2.59260070e-02 -2.76774075e-02  9.81523693e-02\n",
      "  5.73525615e-02  6.36302009e-02  4.81633320e-02  8.38623121e-02\n",
      "  4.13016677e-02  4.28896472e-02  8.13123584e-02  4.96535413e-02\n",
      "  2.07799133e-02  1.06618432e-02 -1.62261248e-01 -4.04991582e-02\n",
      "  7.01355413e-02  2.82404274e-02  7.94203393e-03 -2.65493542e-02\n",
      "  3.09139956e-02  1.21469207e-01 -1.68405585e-02  4.43960577e-02\n",
      " -1.22262977e-01  6.93645701e-02 -4.76440452e-02  3.90052330e-03\n",
      "  2.00634040e-02  7.38082628e-04  3.06107234e-02  8.81390832e-03\n",
      " -3.52900755e-03 -8.24789852e-02  4.97855358e-02  1.14362925e-01\n",
      " -8.20858404e-03 -7.93508887e-02 -1.75243104e-03  4.94023152e-02\n",
      " -1.25351161e-01 -4.34845462e-02 -4.60708290e-02  2.20495872e-02\n",
      "  9.19350982e-02 -4.28293180e-03  9.38549489e-02  5.68760224e-02\n",
      " -1.30414158e-01  4.42003980e-02  4.28768899e-03  2.73670405e-02\n",
      "  3.79474945e-02  7.14861080e-02 -6.68391138e-02 -8.55967924e-02\n",
      "  1.38072772e-02  1.27862617e-01  5.29104322e-02  8.98203850e-02\n",
      "  2.15527005e-02  3.22985910e-02  2.71530803e-02 -7.43465498e-02\n",
      "  3.00270668e-03  9.99904145e-03 -2.32133679e-02 -8.30925349e-03\n",
      " -4.17522490e-02  1.03319183e-01 -7.13551790e-02 -3.03310025e-02\n",
      "  2.57016928e-03 -5.46733439e-02 -2.13616379e-02 -4.91909217e-03\n",
      "  9.97482166e-02 -4.62482646e-02  4.24100719e-02  1.30074834e-02\n",
      "  8.37848987e-03  3.19764093e-02  1.03558432e-02 -8.50697160e-02\n",
      " -1.14334874e-01 -3.59184369e-02  9.73772109e-02  6.48466870e-02\n",
      "  4.23288904e-02 -1.44115627e-01  3.24393250e-02  3.65709402e-02\n",
      "  9.96272117e-02 -3.28778580e-04  2.99324598e-02 -4.22376096e-02\n",
      " -4.47782949e-02  5.14718555e-02  8.34276751e-02 -2.46726293e-02\n",
      " -2.83246320e-02  3.55126709e-02 -5.79545870e-02  9.44208950e-02\n",
      "  1.39297843e-01 -9.13036019e-02 -8.78534541e-02 -8.65633134e-03\n",
      "  3.68166082e-02  6.49033189e-02 -9.83647406e-02  1.68459937e-02\n",
      "  3.83465290e-02 -1.03975490e-01  2.76748586e-04  6.27722442e-02\n",
      "  8.42412263e-02 -5.96391857e-02 -5.15821483e-03 -1.50092430e-02\n",
      " -9.63527411e-02  5.48827276e-02 -2.18261957e-01 -6.30251244e-02\n",
      " -8.11699405e-02  8.72412324e-02  1.13339648e-01 -8.31162632e-02\n",
      " -2.77205203e-02  1.22341871e-01  1.26574971e-02  2.95541044e-02\n",
      "  8.00770000e-02 -3.08510698e-02 -1.76650286e-01  3.45673561e-02\n",
      "  4.95573990e-02 -1.43094976e-02  4.65669116e-04  3.42682712e-02\n",
      " -5.52260205e-02 -4.80101295e-02 -1.52611077e-01  6.57511726e-02\n",
      "  7.73860216e-02  4.33544256e-02  8.52036476e-02 -6.10535778e-02\n",
      "  1.63103305e-02 -5.21591045e-02  3.91920358e-02 -1.35336742e-01\n",
      " -1.02604471e-01 -8.43324289e-02  2.55793054e-02  5.72150126e-02\n",
      " -3.07390429e-02 -5.76971099e-02  1.18377358e-01  7.98508700e-04\n",
      " -1.60967633e-02 -1.36484444e-01 -8.74916464e-03  4.39230390e-02\n",
      " -5.54425791e-02  2.77476385e-02  3.11801732e-02  2.67013609e-02\n",
      "  8.01198035e-02 -3.21715586e-02 -3.23607661e-02 -4.68940735e-02\n",
      "  6.51321486e-02  1.02633156e-01  1.36785582e-01  2.08666753e-02\n",
      " -2.59465668e-02 -1.03641935e-01  2.83754722e-04  1.36362389e-02\n",
      " -3.14879343e-02  8.72453451e-02  4.92591783e-02 -2.04427801e-02\n",
      "  1.73777729e-01 -5.37464656e-02  3.58891785e-02  1.08468775e-02\n",
      "  1.82077978e-02 -2.61720829e-02  5.92903560e-03  9.30643976e-02\n",
      "  4.67969775e-02  4.24854569e-02 -4.99790534e-02 -9.50069055e-02\n",
      "  9.44017768e-02  3.67237255e-02 -1.49614230e-01  1.21167256e-02\n",
      " -1.22964628e-01  4.77557592e-02 -5.73341921e-02  1.00726411e-01\n",
      "  7.84181505e-02 -1.45414714e-02 -4.09658514e-02  1.98782030e-02\n",
      "  7.50905946e-02  9.43466350e-02  7.90193975e-02  1.31480508e-02\n",
      "  8.59210640e-02 -6.65916922e-03  4.18488868e-02  8.17604735e-02\n",
      "  5.46683967e-02 -2.19773389e-02  1.47383986e-02 -7.99484700e-02\n",
      " -1.42550450e-02  3.35371234e-02  6.84968987e-03 -1.04293197e-01\n",
      " -1.55039020e-02  4.85756695e-02  7.25371689e-02  8.56898874e-02\n",
      "  2.02009659e-02 -4.20288816e-02  7.39439577e-02  2.00753417e-02\n",
      " -7.76858851e-02  4.78953980e-02 -6.17089197e-02 -5.49254902e-02\n",
      "  3.58211026e-02  1.21644236e-01 -6.78422749e-02 -7.14830533e-02\n",
      " -6.67195674e-03 -3.87060381e-02  1.49635691e-02  1.15308471e-01\n",
      "  1.23681277e-02 -2.87229363e-02 -4.52653691e-03  1.71216745e-02\n",
      " -7.52867907e-02 -5.84884360e-02  1.21986277e-01 -2.24622013e-03\n",
      "  1.00136483e-02 -5.06096594e-02 -2.36400478e-02 -1.61019117e-02\n",
      " -6.25538379e-02 -6.64488748e-02 -1.64952382e-01  6.76948903e-03\n",
      " -1.19511873e-01 -2.64448375e-02  4.53168862e-02  1.52420904e-02\n",
      "  6.06382564e-02 -1.85824987e-02  1.36904925e-01  6.68567494e-02\n",
      " -6.83673471e-03 -4.65135574e-02  1.05194552e-02  6.65983781e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the embeddings from the checkpoint file\n",
    "embeddings = np.load('../../data_preparation/embedding_chunks/embedded_chunks/contra_minipile_20241013_110604/embeddings_checkpoint.npy', allow_pickle=True)\n",
    "\n",
    "print(f\"Loaded embeddings with shape: {embeddings.shape}\")\n",
    "print(f\"First embedding: {embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
