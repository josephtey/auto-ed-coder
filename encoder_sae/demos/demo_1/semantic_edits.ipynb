{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephtey/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_mutation' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "from shared.models import MiniPileDataset\n",
    "from shared.interp import count_non_zero_feature_activations, plot_feature_activation_histogram, plot_feature_activation_histogram_from_log_feature_densities, plot_highest_activating_feature_for_each_sentence, create_feature_heatmap_widget\n",
    "from shared.features import Feature, FeatureSample\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Enable automatic reloading of modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: [ 1.84332848e-01 -1.71771586e-01  3.56589220e-02 -1.51276067e-01\n",
      " -1.85860202e-01 -4.03527260e-01  1.03313997e-01  2.31959239e-01\n",
      "  1.80198148e-01  2.54761260e-02 -2.52800286e-01 -2.99641520e-01\n",
      " -3.12144727e-01  1.36302456e-01 -1.11550286e-01  1.23413272e-01\n",
      " -1.41808733e-01  1.18051521e-01  3.99289727e-02 -7.64348805e-02\n",
      "  5.78769222e-02  1.11669153e-01 -5.93145967e-01  2.25300901e-02\n",
      "  9.98427927e-01 -4.05542165e-01 -3.07221293e-01 -3.29373479e-01\n",
      " -8.23361218e-01 -8.15446228e-02  5.12531400e-03  2.83867598e-01\n",
      " -2.70921260e-01 -1.25358090e-01 -1.67635307e-01 -1.80304632e-01\n",
      "  2.93781102e-01 -4.39690351e-02  3.00429743e-02  2.44957164e-01\n",
      " -5.87249517e-01 -4.26605135e-01  2.32217878e-01  1.81071863e-01\n",
      "  5.76134287e-02 -3.51474196e-01 -5.17745763e-02 -2.05165073e-02\n",
      " -9.27073210e-02 -9.01999176e-02 -9.37584698e-01  4.22418475e-01\n",
      " -3.02045234e-02  5.15738368e-01  2.13608995e-01  3.28836083e-01\n",
      "  2.70725578e-01 -9.43124831e-01 -1.13463707e-01 -1.97403207e-01\n",
      "  5.24869144e-01  2.38241330e-01 -3.28210831e-01 -3.99866402e-01\n",
      "  4.86791611e-01  1.93856955e-01 -7.39812292e-03  2.89789010e-02\n",
      " -1.11855531e+00  8.93208683e-02 -5.20291924e-01 -6.01975322e-01\n",
      " -1.69080302e-01 -1.71540722e-01 -2.09085599e-01  2.21900463e-01\n",
      " -4.84300584e-01  3.84844810e-01 -2.22211257e-01 -2.88314998e-01\n",
      " -1.09354801e-01  4.53198820e-01 -1.38316542e-01  3.97461861e-01\n",
      "  4.00453091e-01 -2.02784881e-01 -3.41472596e-01  1.06619574e-01\n",
      " -4.54022408e-01  2.85128713e-01 -1.99032109e-03 -5.06218448e-02\n",
      " -2.70794421e-01  9.01027024e-03  7.11987317e-01 -5.52167520e-02\n",
      " -4.91496533e-01  8.49598423e-02 -1.36652619e-01  1.74213365e-01\n",
      "  1.76105201e-01 -8.18498671e-01  3.35421592e-01  1.66528463e-01\n",
      " -2.84861803e-01 -6.19144179e-02 -2.34911740e-01  5.45087278e-01\n",
      "  4.32829559e-01  3.26146752e-01  3.97653401e-01 -2.38637447e-01\n",
      "  8.55914652e-02 -2.72238374e-01 -3.73603344e-01  1.18951365e-01\n",
      "  5.15809096e-02  1.12514623e-01  1.09589867e-01 -4.06011105e-01\n",
      " -2.50200868e-01  1.16383940e-01  1.90810814e-01  9.86904681e-01\n",
      " -2.06674874e-01  5.68773270e-01  2.94426456e-02  5.99673212e-01\n",
      "  3.48193139e-01 -2.16506347e-01  3.54760289e-01  6.71547353e-01\n",
      "  7.02141523e-01 -2.28119031e-01 -1.91575915e-01 -2.09049694e-02\n",
      "  1.36392236e-01 -3.41456831e-01 -6.40114844e-01 -1.36449412e-01\n",
      "  1.27465740e-01  4.36289795e-02  5.60470521e-01 -1.45850405e-01\n",
      "  3.38997751e-01  2.06563488e-01 -5.20347834e-01 -3.69103253e-01\n",
      "  1.27959788e-01  4.21688169e-01  2.50988722e-01  6.14808463e-02\n",
      " -4.90972131e-01 -2.43931159e-01 -6.50052309e-01  2.71236241e-01\n",
      "  2.32744038e-01  1.76081136e-01  8.07637721e-02  1.67829618e-01\n",
      "  6.05685532e-01  1.57420769e-01 -2.05251388e-02 -2.41255715e-01\n",
      "  1.56844646e-01 -2.40174070e-01  9.82337222e-02  4.06815916e-01\n",
      "  8.08200538e-02 -8.90297219e-02 -3.40777785e-01 -1.50410503e-01\n",
      "  4.40777510e-01 -4.26629856e-02 -3.61712098e-01  3.05118471e-01\n",
      "  1.24892838e-01  2.46040821e-02  5.74860871e-01 -2.43467465e-01\n",
      " -1.94530487e+00  6.29263163e-01  1.11401208e-01 -4.39669490e-01\n",
      "  1.55987978e-01 -8.83343890e-02  5.07671118e-01 -7.15770662e-01\n",
      " -3.27078640e-01 -5.07888459e-02 -3.37870806e-01 -1.08473487e-01\n",
      " -3.67167920e-01 -3.68711084e-01  6.18597567e-01 -3.83514911e-01\n",
      " -5.28768264e-02 -1.93741277e-01 -2.28900418e-01 -2.84703560e-02\n",
      "  6.03478067e-02  2.06118807e-01  9.49083865e-02 -1.16995722e-01\n",
      "  1.49401292e-01 -2.64349073e-01  2.43329182e-01 -3.20513435e-02\n",
      " -3.85452092e-01  1.02498487e-01 -2.76675254e-01  9.01556194e-01\n",
      "  2.61748075e-01 -1.39335543e-01  3.10586095e-01 -5.43899313e-02\n",
      "  5.91374747e-02 -1.88724417e-02 -2.43270218e-01 -7.84347728e-02\n",
      " -1.17974758e-01 -1.73325837e-01 -6.63170636e-01  4.59944278e-01\n",
      " -4.73020256e-01  7.75357008e-01  1.63281694e-01  1.16640374e-01\n",
      "  8.41027200e-01  1.87457651e-01  1.89613830e-02 -3.73942375e-01\n",
      "  3.94541472e-01  3.25324833e-02 -2.23938167e-01  3.58625539e-02\n",
      " -3.66799496e-02  4.05229449e-01  9.76401120e-02  2.83943623e-01\n",
      "  7.00924918e-02  1.75997779e-01  4.43293273e-01 -5.43319046e-01\n",
      " -2.34690800e-01 -5.29378116e-01 -1.88668981e-01  7.37436712e-02\n",
      "  4.25363630e-01 -3.55168372e-01  7.92965293e-02 -5.22769809e-01\n",
      "  5.54556362e-02 -5.82257569e-01 -3.46066058e-01  1.72047183e-01\n",
      " -4.61951375e-01 -2.01587543e-01  8.01316053e-02  6.46951437e-01\n",
      " -1.89663216e-01  8.65733624e-02  3.06884646e-01  2.53265530e-01\n",
      " -7.62029827e-01 -3.38904738e-01  1.72850803e-01  2.90252388e-01\n",
      "  5.39870381e-01  3.50318551e-01 -4.13599700e-01 -4.77460861e-01\n",
      " -1.17312334e-01  7.55386055e-02 -3.13133359e-01 -2.05436006e-01\n",
      " -1.50396734e-01  1.96661130e-01  1.54111639e-01 -9.92752090e-02\n",
      "  1.30958661e-01  4.38898861e-01 -1.01894528e-01  1.76706374e-01\n",
      "  6.46051839e-02 -4.95651513e-01 -1.19422697e-01 -1.79747090e-01\n",
      " -2.01200500e-01 -4.46486652e-01 -4.92887050e-01  1.59111992e-01\n",
      " -3.74611676e-01 -1.42987430e-01  3.35612893e-01  3.60003501e-01\n",
      "  4.56390023e-01  1.49179220e-01 -8.30710009e-02 -4.53734964e-01\n",
      "  1.90530170e-03 -5.09532034e-01 -1.84213027e-01  1.06493086e-01\n",
      " -7.37985790e-01  2.95863748e-01 -2.97658294e-01 -5.07092059e-01\n",
      " -2.88789082e+00 -6.46748068e-03 -8.59494582e-02 -2.44239241e-01\n",
      " -8.84420127e-02 -3.83003682e-01 -3.05283278e-01 -4.69035119e-01\n",
      " -1.50302067e-01  2.43573844e-01 -5.00724971e-01 -5.44328630e-01\n",
      "  4.50647622e-02  5.34825265e-01  4.39885318e-01  2.74110317e-01\n",
      "  2.68779963e-01  1.15547560e-01 -2.05139071e-01  4.53340799e-01\n",
      "  1.02118440e-01 -1.94548517e-01  8.48869979e-02 -9.83454734e-02\n",
      "  3.52300823e-01  2.32621372e-01 -2.50088125e-01  2.19452754e-01\n",
      " -3.12835962e-01 -5.23625314e-01 -1.17476225e-01 -1.16808370e-01\n",
      "  3.21626216e-01 -1.34121910e-01  1.08065374e-01  2.90559649e-01\n",
      "  2.51802653e-01 -2.77786642e-01  2.27655992e-01 -4.19227690e-01\n",
      " -1.91684902e-01  8.87292325e-02  1.43754885e-01 -7.96287358e-01\n",
      "  5.19389212e-01 -4.93237466e-01 -1.96727023e-01 -2.26318106e-01\n",
      "  3.00869435e-01  3.86158556e-01 -2.84131259e-01  2.14403905e-02\n",
      " -8.95936564e-02  7.63747394e-02 -1.68838009e-01 -2.35677227e-01\n",
      "  6.28427267e-01  4.04258043e-01 -3.30862015e-01 -2.68223017e-01\n",
      "  6.11354411e-01 -4.10597116e-01 -6.13594502e-02 -4.12298203e-01\n",
      " -1.26606748e-01 -3.42226535e-01 -2.23295540e-01 -4.28971082e-01\n",
      " -3.92581075e-01  3.17645222e-01 -5.32603800e-01 -7.49291554e-02\n",
      " -4.75908279e-01 -9.68175352e-01  9.52409580e-02 -2.12513059e-01\n",
      "  5.65168373e-02 -1.01700589e-01  1.89940020e-01 -4.43336606e-01\n",
      " -7.20804989e-01 -1.05935633e+00 -1.65572055e-02 -4.34781685e-02\n",
      " -7.86919892e-02 -1.59220949e-01 -8.41434151e-02 -2.00505555e-01\n",
      " -4.76118922e-03 -3.67660224e-01  2.80541331e-01  9.67242196e-02\n",
      "  2.03510538e-01  1.78329483e-01 -2.61431664e-01  1.06651813e-01\n",
      "  6.70660377e-01 -6.54616296e-01  6.25170946e-01 -4.07324731e-03\n",
      "  2.30991930e-01 -2.93680787e-01  2.58547813e-01 -6.20294809e-02\n",
      " -1.72861278e-01  6.06309660e-02 -9.05545175e-01  1.86379179e-02\n",
      "  3.29349548e-01  2.14904606e-01  9.80646834e-02 -5.03782667e-02\n",
      "  2.58854687e-01 -2.74916589e-01 -4.61114585e-01  5.39851248e-01\n",
      " -1.20327003e-01  5.33770502e-01  3.46242249e-01 -1.02885865e-01\n",
      "  2.07501784e-01  5.19259274e-01 -4.42767620e-01 -2.58111507e-01\n",
      " -3.69678408e-01 -6.27285019e-02  3.90462019e-02  8.18418860e-02\n",
      " -8.34587589e-02 -3.03656250e-01 -1.70111358e-01 -3.80658984e-01\n",
      "  2.68816888e-01  3.99530619e-01  4.51147258e-01 -4.19878006e-01\n",
      "  1.52693376e-01  8.54129970e-01  4.19375151e-01 -2.11843535e-01\n",
      "  2.06767470e-01  3.36922914e-01 -1.30501553e-01 -1.69572309e-01\n",
      " -1.34190187e-01  7.28272676e-01  1.61185265e-01 -7.39404038e-02\n",
      "  2.29872808e-01 -8.06810856e-02 -2.62280315e-01  9.04846843e-03\n",
      "  4.36411798e-02 -2.34168440e-01  1.33841515e-01 -4.87366676e-01\n",
      "  9.40187946e-02 -2.92565554e-01 -1.42727911e-01 -6.77573562e-01\n",
      "  9.04370099e-02  5.54577827e-01  3.13430011e-01  2.52544850e-01\n",
      "  6.76773340e-02  1.05100177e-01  1.35782495e-01  3.57523113e-02\n",
      " -3.21551681e-01  3.56524497e-01 -1.82050005e-01 -1.06989294e-01\n",
      "  4.02673900e-01  2.68155158e-01  1.46698073e-01  7.41977453e-01\n",
      "  2.48449698e-01 -6.10867366e-02  3.43116760e-01  9.69204366e-01\n",
      "  2.52374500e-01  1.64143190e-01  4.45282320e-03  9.07698199e-02\n",
      "  7.41428928e-03  6.83543205e-01 -4.49091941e-02  1.90159544e-01\n",
      " -1.08530670e-01  1.74646631e-01 -8.32020998e-01  2.05453739e-01\n",
      " -3.44204992e-01 -3.79467398e-01 -4.70844239e-01 -4.21978295e-01\n",
      "  2.20239863e-01  3.11728436e-02  3.16692084e-01 -2.31599614e-01\n",
      "  7.08728373e-01  1.60696149e-01 -5.44118166e-01 -1.50985762e-01\n",
      " -3.43097299e-01 -1.47267088e-01  1.02178268e-01  2.66330987e-01\n",
      "  1.27743021e-01 -1.34556755e-01  1.63021788e-01 -4.23554033e-01\n",
      " -2.29439691e-01 -8.33189011e-01 -6.39231363e-03  1.32580101e-01\n",
      "  3.38443041e-01  8.64432082e-02  5.24421930e-02 -3.93161088e-01\n",
      " -5.01638114e-01  1.36397734e-01  2.10766137e-01  4.49232787e-01\n",
      " -4.39736366e-01 -1.33903593e-01 -2.49062538e-01 -3.17483693e-01\n",
      " -9.86562729e-01  2.51162816e-02  4.11117375e-01 -9.46079269e-02\n",
      "  7.82202408e-02  2.19533592e-01  2.54221946e-01 -5.55053473e-01\n",
      " -7.24167049e-01 -1.80166766e-01  8.06767941e-02 -3.80235106e-01\n",
      " -4.52134311e-02  1.30804926e-01  5.95639795e-02 -4.49925721e-01\n",
      "  1.16272859e-01 -2.70594954e-01 -9.54328254e-02  5.42314947e-01\n",
      " -2.39731789e-01  3.81362051e-01 -1.27521977e-01  5.67524210e-02\n",
      " -2.97713637e-01 -2.10127160e-01  4.43590671e-01 -9.17517394e-02\n",
      " -5.58518350e-01  1.44271478e-01 -6.51284635e-01  4.12470609e-01\n",
      "  9.17718112e-01 -2.11993173e-01 -2.68445283e-01  6.13672853e-01\n",
      "  7.03846216e-01  3.51380199e-01  1.67661726e-01  4.31342512e-01\n",
      "  1.69049636e-01 -1.27338037e-01 -4.87378627e-01 -1.62444077e-02\n",
      "  3.86898592e-02  4.04595464e-01  5.28637059e-02 -5.20553172e-01\n",
      "  1.49998546e-01 -5.72479665e-01  1.15426421e-01 -1.62772164e-01\n",
      "  2.03778073e-01  4.00180697e-01  1.56156600e-01  2.65952229e-01\n",
      "  4.72948521e-01 -1.38975888e-01 -2.84612447e-01  2.19336644e-01\n",
      "  2.73059547e-01 -7.83561319e-02 -3.14102143e-01  6.28074825e-01\n",
      " -4.55607891e-01 -1.60528719e-03 -2.26280645e-01 -6.75200857e-03\n",
      "  4.54709798e-01  1.74850360e-01  5.00869602e-02 -3.58166657e-02\n",
      "  1.19468018e-01  6.40266359e-01  1.33192480e-01  7.60062737e-03\n",
      " -2.24515676e-01  1.39466658e-01  2.93824494e-01 -1.03835016e-01\n",
      "  1.22974515e-01  5.11467874e-01 -3.46491337e-01 -4.12545234e-01\n",
      "  6.12567186e-01  5.78426421e-01 -5.34381509e-01 -3.81392479e-01\n",
      "  2.71824360e-01 -3.63139361e-01  9.42737982e-02  4.88521084e-02\n",
      "  5.27676148e-03 -1.66862309e-01  4.02379155e-01  1.04968168e-01\n",
      " -6.17148757e-01  5.67100942e-01 -3.55313599e-01 -3.81077468e-01\n",
      " -1.34508312e-01  1.84286445e-01  6.01793043e-02  6.47933900e-01\n",
      " -1.65984407e-01  3.96734327e-01 -5.28452516e-01 -5.17195642e-01\n",
      "  2.09688231e-01  4.37361211e-01  2.78446853e-01 -4.86959904e-01\n",
      "  5.02451837e-01  3.17413211e-01 -9.98046175e-02  2.79825181e-01\n",
      " -4.87911664e-02 -5.28197736e-02 -5.96285202e-02  1.16708223e-02\n",
      "  3.37624997e-01  8.59709501e-01  2.46558324e-01  2.73967236e-01\n",
      "  1.58479229e-01 -3.26929614e-02  1.94980398e-01  2.56678611e-01\n",
      "  4.90221381e-01  4.35896128e-01  2.94141829e-01  1.39319569e-01\n",
      "  2.06975996e-01  1.16326831e-01  1.93720296e-01 -4.96255606e-01\n",
      " -3.53079736e-01  4.21195209e-01  7.80160904e-01  1.71035990e-01\n",
      " -1.58888519e-01  3.60118388e-03 -1.55743942e-01  2.41549581e-01\n",
      "  7.65400454e-02 -2.09494054e-01 -1.55497342e-01  3.89145106e-01\n",
      "  2.18627974e-01  6.59086183e-02 -6.04412556e-01 -2.73640126e-01\n",
      "  9.35675297e-03  4.19967733e-02 -2.13664725e-01 -2.24155843e-01\n",
      " -2.09685549e-01 -5.09338737e-01  2.33814232e-02 -1.18733212e-01\n",
      " -1.46852434e-01 -3.02303672e-01 -2.37115532e-01 -4.44256127e-01\n",
      "  3.45178246e-01 -5.36383092e-01  4.42761868e-01 -3.02120954e-01\n",
      " -1.50089592e-01  4.75949973e-01  7.08679140e-01 -1.60310581e-01\n",
      "  1.02260463e-01 -3.39663982e-01 -2.36622810e-01 -3.80150974e-02\n",
      " -1.17973546e-02  2.41448626e-01  1.87617969e-02  4.31512333e-02\n",
      " -5.72245345e-02 -5.13237119e-01  3.45932931e-01  2.40959421e-01\n",
      " -6.25297725e-01 -1.68495495e-02 -4.38436508e-01  4.03073877e-01\n",
      " -1.51193753e-01 -2.22317696e-01  6.25932515e-02  5.10176301e-01\n",
      " -6.36595070e-01 -6.74313828e-02  4.30710793e-01  3.21490973e-01\n",
      " -1.18900202e-01  9.16589946e-02  1.32829294e-01 -9.90768429e-03\n",
      " -2.55487680e-01 -2.74422765e-01  6.82758152e-01  4.71755236e-01\n",
      "  2.80199200e-01  3.81901383e-01  3.33388060e-01 -4.54117298e-01\n",
      " -1.70497671e-01  3.34014505e-01  5.29027125e-03 -1.50554478e-01\n",
      " -4.23703194e-01  3.21384519e-01  2.06245750e-01 -1.41856492e-01\n",
      " -5.71753025e-01  1.95348009e-01  1.27584562e-01 -4.44040745e-01\n",
      " -1.57328889e-01 -3.29751521e-01  3.62259269e-01  4.16534096e-02\n",
      " -1.91411644e-01 -3.24715883e-01 -1.93683490e-01  1.36172459e-01\n",
      "  3.80617976e-01 -1.27539635e-01 -2.56648391e-01  3.69657487e-01]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME_1 = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_1)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME_1)\n",
    "\n",
    "# Encode some text\n",
    "text = \"This is a sample text to encode.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Move model to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Get the embedding from the last hidden state\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "print(\"Embedding:\", embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the pickle file\n",
    "import pickle \n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from shared.sparse_autoencoder import SparseAutoencoder, SparseAutoencoderConfig\n",
    "import json\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# load the dataset\n",
    "pre_trained_sentences_file = \"pre-trained/data/asap_sentences_3_bert-base-uncased.npy\"\n",
    "pre_trained_embeddings_file = \"pre-trained/data/asap_embeddings_3_bert-base-uncased.npy\"\n",
    "pre_trained_mini_pile_dataset = MiniPileDataset(pre_trained_sentences_file, pre_trained_embeddings_file)\n",
    "\n",
    "# Load the configuration from the JSON file\n",
    "pre_trained_config_path = \"pre-trained/sae/config.json\"\n",
    "with open(pre_trained_config_path, \"r\") as pre_trained_config_file:\n",
    "    pre_trained_config = json.load(pre_trained_config_file)\n",
    "\n",
    "# Load the pre-trained model from the pickle file\n",
    "pre_trained_sae_config = SparseAutoencoderConfig(d_model=pre_trained_config[\"dimensions\"], d_sparse=8 * pre_trained_config[\"dimensions\"], sparsity_alpha=pre_trained_config[\"sparsity_alpha\"])\n",
    "pre_trained_model = SparseAutoencoder(pre_trained_sae_config)\n",
    "pre_trained_model_path = \"pre-trained/sae/sae.pkl\"\n",
    "with open(pre_trained_model_path, \"rb\") as pre_trained_f:\n",
    "    pre_trained_model_state_dict = pickle.load(pre_trained_f)\n",
    "    pre_trained_model.load_state_dict(pre_trained_model_state_dict)\n",
    "\n",
    "# Load the log feature densities from the JSON file\n",
    "pre_trained_log_feature_densities_path = \"pre-trained/sae/log_feature_densities.json\"\n",
    "with open(pre_trained_log_feature_densities_path, \"r\") as pre_trained_json_file:\n",
    "    pre_trained_log_feature_densities = json.load(pre_trained_json_file)\n",
    "\n",
    "# Load features\n",
    "pre_trained_features_folder = \"pre-trained/features\"\n",
    "pre_trained_features = []\n",
    "pre_trained_filtered_out_count = 0\n",
    "\n",
    "for pre_trained_filename in os.listdir(pre_trained_features_folder):\n",
    "    if pre_trained_filename.startswith(\"feature_\") and pre_trained_filename.endswith(\".json\"):\n",
    "        with open(os.path.join(pre_trained_features_folder, pre_trained_filename), \"r\") as pre_trained_json_file:\n",
    "            pre_trained_feature_data = json.load(pre_trained_json_file)\n",
    "            pre_trained_feature = Feature(**pre_trained_feature_data)\n",
    "            pre_trained_features.append(pre_trained_feature)\n",
    "            # if len(pre_trained_feature.high_act_samples) >= 10:\n",
    "            #     pre_trained_features.append(pre_trained_feature)\n",
    "            # else:\n",
    "            #     pre_trained_filtered_out_count += 1\n",
    "\n",
    "# Load the scores from the JSON file\n",
    "with open(\"asap_sentences_scores_3.json\", \"r\") as score_file:\n",
    "    score_dict = json.load(score_file)\n",
    "\n",
    "# Add scores to high acting samples in pre_trained_features\n",
    "for feature in pre_trained_features:\n",
    "    for sample in feature.high_act_samples:\n",
    "        sample_text = sample.text\n",
    "        if sample_text in score_dict:\n",
    "            sample.score = score_dict[sample_text]\n",
    "        else:\n",
    "            sample.score = None  # or some default value if the text is not found\n",
    "\n",
    "print(\"Scores added to high acting samples in pre_trained_features.\")\n",
    "\n",
    "# pre_trained_features.sort(key=lambda x: x.confidence, reverse=True)\n",
    "\n",
    "# print(f\"Number of filtered out samples: {pre_trained_filtered_out_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class BottleneckT5Autoencoder:\n",
    "    def __init__(self, model_path: str, device='cpu'):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=512)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, text: str) -> torch.FloatTensor:\n",
    "        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        decoder_inputs = self.tokenizer('', return_tensors='pt').to(self.device)\n",
    "        return self.model(\n",
    "            **inputs,\n",
    "            decoder_input_ids=decoder_inputs['input_ids'],\n",
    "            encode_only=True,\n",
    "        )[0]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_latent(self, latent: torch.FloatTensor, max_length=512, temperature=1.0) -> str:\n",
    "        dummy_text = '.'\n",
    "        dummy = self.embed(dummy_text)\n",
    "        perturb_vector = latent - dummy\n",
    "        self.model.perturb_vector = perturb_vector\n",
    "        input_ids = self.tokenizer(dummy_text, return_tensors='pt').to(self.device).input_ids\n",
    "        output = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        return self.tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/josephtey/Projects/auto-ed-coder/venv/lib/python3.9/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/josephtey/Projects/auto-ed-coder/venv/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mBottleneckT5Autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthesephist/contra-bottleneck-t5-base-wikipedia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mBottleneckT5Autoencoder.__init__\u001b[0;34m(self, model_path, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: \u001b[38;5;28mstr\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:889\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    888\u001b[0m         )\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2163\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2161\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2397\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2397\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2402\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:119\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n",
      "File \u001b[0;32m~/Projects/auto-ed-coder/venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:134\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "autoencoder = BottleneckT5Autoencoder(model_path='thesephist/contra-bottleneck-t5-base-wikipedia', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
